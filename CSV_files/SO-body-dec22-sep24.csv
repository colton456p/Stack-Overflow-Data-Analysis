Body
"<p>I am trying to get Latex symbols in titles and labels of a Plotly figure.
I am using VSCode and I run the code in Interactive Window.
Latex usage looks really simple in Jupyter Notebook, from what I saw in other posts, but I can't get it to work within this environment.</p>
<p>My env:</p>
<p>python 3.10.4</p>
<p>plotly 5.9.0</p>
<p>vscode 1.62.3</p>
<p>What I tried:</p>
<ul>
<li>use r&quot;$$&quot; formatting,</li>
<li>change the font family</li>
<li>change plotly.io.renderers.default</li>
<li>install mathjax in my conda env and try to adapt plotly.offline mode (see <a href=""https://github.com/plotly/plotly.py/issues/515"" rel=""noreferrer"">https://github.com/plotly/plotly.py/issues/515</a>)</li>
</ul>
<p>This basic code snippet should work according to most posts I have seen but does not do the Latexrendering in the Interactive Window.
It has been taken from <a href=""https://plotly.com/python/LaTeX/"" rel=""noreferrer"">https://plotly.com/python/LaTeX/</a>, where everything looks so easy. That's why I am guessing the issue is related to VSCode.</p>
<pre><code>import plotly.graph_objs as go

fig = go.Figure()
fig.add_trace(go.Scatter(
    x=[1, 2, 3, 4],
    y=[1, 4, 9, 16],
    name=r'$\alpha_{1c} = 352 \pm 11 \text{ km s}^{-1}$'
))
fig.add_trace(go.Scatter(
    x=[1, 2, 3, 4],
    y=[0.5, 2, 4.5, 8],
    name=r'$\beta_{1c} = 25 \pm 11 \text{ km s}^{-1}$'
))
fig.update_layout(
    xaxis_title=r'$\sqrt{(n_\text{c}(t|{T_\text{early}}))}$',
    yaxis_title=r'$d, r \text{ (solar radius)}$'
)
fig.show()
</code></pre>
<p>What I have
<a href=""https://i.sstatic.net/qtUfd.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/qtUfd.png"" alt=""What I have"" /></a></p>
<p>What I should have
<a href=""https://i.sstatic.net/tCpvm.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/tCpvm.png"" alt=""Expected"" /></a></p>
"
"<p>I'm getting below error, while pip installing ta-lib.
I used command :</p>
<pre><code>!pip install ta-lib
</code></pre>
<p>Please provide me solution.</p>
<pre><code>    Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/
Collecting ta-lib
  Using cached TA-Lib-0.4.25.tar.gz (271 kB)
  Installing build dependencies ... done
  Getting requirements to build wheel ... done
  Installing backend dependencies ... done
  Preparing metadata (pyproject.toml) ... done
Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from ta-lib) (1.21.6)
Building wheels for collected packages: ta-lib
  error: subprocess-exited-with-error
  
  × Building wheel for ta-lib (pyproject.toml) did not run successfully.
  │ exit code: 1
  ╰─&gt; See above for output.
  
  note: This error originates from a subprocess, and is likely not a problem with pip.
  Building wheel for ta-lib (pyproject.toml) ... error
  ERROR: Failed building wheel for ta-lib
Failed to build ta-lib
ERROR: Could not build wheels for ta-lib, which is required to install pyproject.toml-based projects
</code></pre>
<p>I tried following commands :</p>
<pre><code>pip install --upgrade pip setuptools wheel
</code></pre>
<pre><code>pip install pep517
</code></pre>
<pre><code>!pip3 install --upgrade pip
</code></pre>
<pre><code>!pip install pyproject-toml
</code></pre>
<pre><code>pip install TA_Lib‑0.4.10‑cp35‑cp35m‑win_amd64.whl
</code></pre>
<pre><code>!pip install ta-lib
</code></pre>
"
"<p>For a captcha solver I need to use FFmpeg on Windows 10. Warning when running the code for the first time:</p>
<pre><code>C:\Users\user\AppData\Roaming\Python\Python310\site-packages\pydub\utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
  warn(&quot;Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work&quot;, RuntimeWarning)
</code></pre>
<p>Running the script anyway while it required ffprobe I got:</p>
<pre><code>C:\Users\user\AppData\Roaming\Python\Python310\site-packages\pydub\utils.py:198: RuntimeWarning: Couldn't find ffprobe or avprobe - defaulting to ffprobe, but may not work
  warn(&quot;Couldn't find ffprobe or avprobe - defaulting to ffprobe, but may not work&quot;, RuntimeWarning)
Traceback (most recent call last):
  File &quot;D:\Scripts\captcha\main.py&quot;, line 164, in &lt;module&gt;
    main()
  File &quot;D:\Scripts\captcha\main.py&quot;, line 155, in main
    captchaSolver()
  File &quot;D:\Scripts\captcha\main.py&quot;, line 106, in captchaSolver
    sound = pydub.AudioSegment.from_mp3(
  File &quot;C:\Users\user\AppData\Roaming\Python\Python310\site-packages\pydub\audio_segment.py&quot;, line 796, in from_mp3
    return cls.from_file(file, 'mp3', parameters=parameters)
  File &quot;C:\Users\user\AppData\Roaming\Python\Python310\site-packages\pydub\audio_segment.py&quot;, line 728, in from_file
    info = mediainfo_json(orig_file, read_ahead_limit=read_ahead_limit)
  File &quot;C:\Users\user\AppData\Roaming\Python\Python310\site-packages\pydub\utils.py&quot;, line 274, in mediainfo_json
    res = Popen(command, stdin=stdin_parameter, stdout=PIPE, stderr=PIPE)
  File &quot;C:\Program Files\Python310\lib\subprocess.py&quot;, line 966, in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
  File &quot;C:\Program Files\Python310\lib\subprocess.py&quot;, line 1435, in _execute_child
    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,
FileNotFoundError: [WinError 2] The system cannot find the file specified
</code></pre>
<p>I tried downloading it manually, editing environment variables, pasting them in the same folder as the script and installing with pip. FFmpeg works however my script doesn't.</p>
"
"<p>I've been looking all over the web for the current standards for Python Docstrings and I've come across different answers for different scenarios. What is the currently most-accepted and wide-spread docstring format that I should use?
These are the ones that I've found so far:</p>
<p>Sphinx format (1): <code>:param type name: description</code></p>
<p>Sphinx format (2):
<code>:py:param type name: description</code></p>
<p>NumPy format: <code>Parameters: __________  param: description</code></p>
<p>Other formats:</p>
<p><code>Args: param (type): description</code></p>
<p><code>Parameters: param (type): description</code></p>
<p>I just want to document my code in a <code>standard</code> way that is accepted by almost every IDE (including VS Code and PyCharm) that also conforms to PEP and readthedocs, so I can also enable hover-over with mouse over the code to see description of the arguments.</p>
<p>I'm looking for current standards that are at least backwards compatible with Python 3.6 a since that's the base of the projects I work on.</p>
"
"<p>Is there a way to assess whether a case statement variable is inside a particular list? Consider the following scenario. We have three lists:</p>
<pre><code>a  = [1, 2, 3]
b  = [4, 5, 6]
c  = [7, 8, 9]
</code></pre>
<p>Then I want to check whether x is in each list. Something like this (of course this is a Syntax Error but I hope you get the point):</p>
<pre><code>match x:
    case in a:
       return &quot;132&quot;
    case in b:
       return &quot;564&quot;
    case in c:
       return &quot;798&quot;
</code></pre>
<p>This can be easy with an if-else scenario. Nonetheless, focusing on the match-case, if one has many lists. And big lists, it would be a mundane task to write them like that:</p>
<pre><code>match x:
    case 1 | 2 | 3:
       return &quot;132&quot;
    case 4 | 5 | 6:
       return &quot;564&quot;
    case 7 | 8 | 9:
       return &quot;762&quot;
</code></pre>
<p>Is there an easy way to check for multiple conditions for each case, without having to write them down?</p>
<p>I checked for duplicates, but I couldn't find them, I hope I don't miss something. Please be kind and let me know if there is a duplicate question.</p>
"
"<p>I edited a .qmd file in VS code but failed to render it to html. The error information was as followed:</p>
<pre><code>Starting python3 kernel...Traceback (most recent call last):
  File &quot;D:\Program Files\Quarto\share\jupyter\jupyter.py&quot;, line 21, in &lt;module&gt; 
    from notebook import notebook_execute, RestartKernel
  File &quot;D:\Program Files\Quarto\share\jupyter\notebook.py&quot;, line 16, in &lt;module&gt;
    import nbformat
ModuleNotFoundError: No module named 'nbformat'
</code></pre>
<p>I wonder how to resolve this problem.</p>
<p>I guess the problem was with the python interpreter but I don't know how to switch to another python interpreter under a .qmd file.</p>
<p>Also, I tried <code>conda install nbformat</code> in the command line and it was successfully installed. Next I rendered the .qmd file again the same error appeared again.</p>
<p>So how can I solve this problem?</p>
"
"<p>When running ipynbs in VS Code, I've started noticing Pylance warnings on standard library imports. I am using a conda virtual environment, and I believe the warning is related to that. An example using the glob library reads:</p>
<p><code> &quot;env\Lib\glob.py&quot; is overriding the stdlib &quot;glob&quot; modulePylance(reportShadowedImports)</code></p>
<p>So far my notebooks run as expected, but I am curious if this warning is indicative of poor layout or is just stating the obvious more of an &quot;FYI you are not using the base install of python&quot;.</p>
<p>I have turned off linting and the problem stills persists. And almost nothing returns from my searches of the error &quot;reportShadowedImports&quot;.</p>
"
"<p>When computing <code>A @ a</code> where <code>A</code> is a random N by N matrix and a is a vector with N random elements using numpy the computation time jumps by an order of magnitude at N=100. Is there any particular reason for this? As a comparison the same operation using torch on the cpu has a more gradual increase<a href=""https://i.sstatic.net/aWBp2.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/aWBp2.png"" alt=""matrix-vector multiply computation time"" /></a></p>
<p>Tried it with python3.10 and 3.9 and 3.7 with the same behavior</p>
<p>Code used for generating numpy part of the plot:</p>
<pre class=""lang-py prettyprint-override""><code>import numpy as np
from tqdm.notebook import tqdm
import pandas as pd
import time
import sys

def sym(A):
    return .5 * (A + A.T)

results = []
for n in tqdm(range(2, 500)):
    for trial_idx in range(10):
        A = sym(np.random.randn(n, n))
        a = np.random.randn(n)        
        
        t = time.time()
        for i in range(1000):
            A @ a
        t = time.time() - t
        results.append({
            'n': n,
            'time': t,
            'method': 'numpy',
        })
results = pd.DataFrame(results)

from matplotlib import pyplot as plt
fig, ax = plt.subplots(1, 1)
ax.semilogy(results.n.unique(), results.groupby('n').time.mean(), label=&quot;numpy&quot;)
ax.set_title(f'A @ a timimgs (1000 times)\nPython {sys.version.split(&quot; &quot;)[0]}')
ax.legend()
ax.set_xlabel('n')
ax.set_ylabel('avg. time')
</code></pre>
<h3>Update</h3>
<p>Adding</p>
<pre><code>import os
os.environ[&quot;MKL_NUM_THREADS&quot;] = &quot;1&quot; 
os.environ[&quot;NUMEXPR_NUM_THREADS&quot;] = &quot;1&quot; 
os.environ[&quot;OMP_NUM_THREADS&quot;] = &quot;1&quot; 
</code></pre>
<p>before <code>ìmport numpy</code> gives a more expected output, see this answer for details: <a href=""https://stackoverflow.com/a/74662135/5043576"">https://stackoverflow.com/a/74662135/5043576</a> <a href=""https://i.sstatic.net/szGbq.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/szGbq.png"" alt=""update"" /></a></p>
"
"<p>I'm using FastAPI with non-<code>async</code> endpoints running over gunicorn with multiple workers, from the <code>uvicorn.workers.UvicornWorker</code> class as suggested <a href=""https://fastapi.tiangolo.com/deployment/server-workers/"" rel=""nofollow noreferrer"">here</a>. Latley, I noticed high latency in some of our endpoints during times in the day our application is busier than usual. I started to investigate it and I figured out that concurrency in our app doesn't work as we expect.</p>
<p>Let's say I have this FastAPI application (main.py) with the following endpoint</p>
<pre class=""lang-py prettyprint-override""><code>app = FastAPI()
logger = logging.getLogger()

@app.get(&quot;/&quot;)
def root():
    logger.info(f&quot;Running on {os.getpid()}&quot;)
    time.sleep(3600)
    return {&quot;message&quot;: &quot;Hello World&quot;}
</code></pre>
<p>and I run <code>gunicorn</code> with the following cmd:</p>
<pre><code>gunicorn main:app --workers 4 --worker-class uvicorn.workers.UvicornWorker --bind 0.0.0.0:8000
</code></pre>
<p>When I send to the server five requests, they all get to the same worker except the last one, instead of running in parallel over all workers:</p>
<pre><code>INFO:root:Running on 643
INFO:root:Running on 643
INFO:root:Running on 643
INFO:root:Running on 643
INFO:root:Running on 642
</code></pre>
<p>If I turn the endpoint into <code>async</code>, every request will be handled on a different worker (the last one will be holded).
I know that when using non-async endpoint, FastAPI uses AnyIO threads to handle the requests, the default value for maximum threads is 40. When I try to lower this limit to 2 threads for example using the suggestion <a href=""https://github.com/tiangolo/fastapi/issues/4221#issuecomment-982260467"" rel=""nofollow noreferrer"">here</a>, only the first two requests are being handled while the rest are waiting (even though I still have 4 workers!)</p>
<p>That's bad because both not using all our resources and suffering from python threading problems due to GIL on the same worker.</p>
<p>Is there a way to overcome those problems without turning to <code>async</code> endpoints?</p>
"
"<p>I'm trying to test my FastAPI endpoints by overriding the injected database using the officially recommended method in the <a href=""https://fastapi.tiangolo.com/advanced/testing-dependencies/"" rel=""noreferrer"">FastAPI documentation</a>.</p>
<p>The function I'm injecting the db with is a closure that allows me to build any desired database from a MongoClient by giving it the database name whilst (I assume) still working with FastAPI depends as it returns a closure function's signature. No error is thrown so I think this method is correct:</p>
<pre class=""lang-python prettyprint-override""><code># app
def build_db(name: str):
    def close():
          return build_singleton_whatever(MongoClient, args....)
     return close
</code></pre>
<p>Adding it to the endpoint:</p>
<pre class=""lang-python prettyprint-override""><code># endpoint
@app.post(&quot;/notification/feed&quot;)
async def route_receive_notifications(db: Database = Depends(build_db(&quot;someDB&quot;))):
   ...
</code></pre>
<p>And finally, attempting to override it in the tests:</p>
<pre class=""lang-python prettyprint-override""><code># pytest
# test_endpoint.py
fastapi_app.dependency_overrides[app.build_db] = lambda x: lambda: x
</code></pre>
<p>However, the dependency doesn't seem to override at all and the test ends up creating a MongoClient with the IP of the production database as in normal execution.</p>
<p><strong>So</strong>, any ideas on overriding FastAPI dependencies that are given parameters in their endpoints?</p>
<p>I have tried creating a mock closure function with no success:</p>
<pre class=""lang-python prettyprint-override""><code>def mock_closure(*args):
    def close():
        return args
    return close

app.dependency_overrides[app.build_db] = mock_closure('otherDB')
</code></pre>
<p>And I have also tried providing the same signature, including the parameter, with still no success:</p>
<pre class=""lang-python prettyprint-override""><code>app.dependency_overrides[app.build_db('someDB')] = mock_closure('otherDB')
</code></pre>
<p><em>Edit note</em> I'm also aware I can create a separate function that creates my desired database and use that as the dependency, but I would much prefer to use this dynamic version as it's more scalable to using more databases in my apps and avoids me writing essentially repeated functions just so they can be cleanly injected.</p>
"
"<p>I have tried installing ansible through <strong>pip install ansible</strong> but whenever i get the following error trying to use it:</p>
<pre><code>Traceback (most recent call last):
  File &quot;&lt;frozen runpy&gt;&quot;, line 198, in _run_module_as_main
  File &quot;&lt;frozen runpy&gt;&quot;, line 88, in _run_code
  File &quot;C:\Users\ruan.greyling\AppData\Local\Programs\Python\Python311\Scripts\ansible.exe\__main__.py&quot;, line 4, in &lt;module&gt;
  File &quot;C:\Users\ruan.greyling\AppData\Local\Programs\Python\Python311\Lib\site-packages\ansible\cli\__init__.py&quot;, line 42, in &lt;module&gt;
    check_blocking_io()
  File &quot;C:\Users\ruan.greyling\AppData\Local\Programs\Python\Python311\Lib\site-packages\ansible\cli\__init__.py&quot;, line 34, in check_blocking_io
    if not os.get_blocking(fd):
           ^^^^^^^^^^^^^^^
AttributeError: module 'os' has no attribute 'get_blocking'
</code></pre>
<p>I have python and pip installed on the machine.</p>
"
"<p>I have a asyncio running loop, and from the coroutine I'm calling a sync function, is there any way we can call and get result from an async function in a sync function
tried below code, it is not working
want to print output of hel() in i() without changing i() to async function
is it possible, if yes how?</p>
<pre><code>import asyncio

async def hel():
    return 4

def i():
    loop = asyncio.get_running_loop()
    x = asyncio.run_coroutine_threadsafe(hel(), loop)   ## need to change
    y = x.result()                                      ## this lines
    print(y)

async def h():
    i()

asyncio.run(h())
</code></pre>
"
"<p>I need to delete duplicated rows based on combination of two columns (person1 and person2 columns) which have strings.
For example person1: ryan and person2: delta or person 1: delta and person2: ryan is same and provides the same value in messages column. Need to drop one of these two rows. Return the non duplicated rows as well.</p>
<pre><code>Code to recreate df 
df = pd.DataFrame({&quot;&quot;: [0,1,2,3,4,5,6],
                     &quot;person1&quot;: [&quot;ryan&quot;, &quot;delta&quot;, &quot;delta&quot;, &quot;delta&quot;,&quot;bravo&quot;,&quot;alpha&quot;,&quot;ryan&quot;], 
                     &quot;person2&quot;: [&quot;delta&quot;, &quot;ryan&quot;, &quot;alpha&quot;, &quot;bravo&quot;,&quot;delta&quot;,&quot;ryan&quot;,&quot;alpha&quot;], 
                     &quot;messages&quot;: [1, 1, 2, 3,3,9,9]})
</code></pre>
<pre><code> df
        person1 person2 messages
0   0   ryan    delta   1
1   1   delta   ryan    1
2   2   delta   alpha   2
3   3   delta   bravo   3
4   4   bravo   delta   3
5   5   alpha   ryan    9
6   6   ryan    alpha   9
</code></pre>
<p>Answer df should be:</p>
<pre><code> finaldf
        person1 person2 messages
0   0   ryan    delta   1
1   2   delta   alpha   2
2   3   delta   bravo   3
3   5   alpha   ryan    9
</code></pre>
"
"<p><code>Ubuntu 22.04.1 LTS</code></p>
<p><code>pyodbc 4.0.35</code></p>
<p><code>OpenSSL 3.0.2 15 Mar 2022 (Library: OpenSSL 3.0.2 15 Mar 2022)</code></p>
<p>Followed steps on <a href=""https://learn.microsoft.com/en-us/sql/connect/odbc/linux-mac/installing-the-microsoft-odbc-driver-for-sql-server?view=sql-server-ver16"" rel=""noreferrer"">Install the Microsoft ODBC driver for SQL Server (Linux)</a></p>
<p>Installation successful. When I run this snippet</p>
<pre><code>def select_driver():
    &quot;&quot;&quot;Find least version of: ODBC Driver for SQL Server.&quot;&quot;&quot;
    drv = sorted([drv for drv in pyodbc.drivers() if &quot;ODBC Driver &quot; in drv and &quot; for SQL Server&quot; in drv])
    if len(drv) == 0:
        raise Exception(&quot;No 'ODBC Driver XX for SQL Server' found.&quot;)
    return drv[-1]

print(select_driver()) 
</code></pre>
<p>Output is : <code>ODBC Driver 18 for SQL Server</code></p>
<p>My connection string .</p>
<pre><code>cnxn_str = (&quot;Driver={SQL Server Native Client 18.0};&quot;
            &quot;Server=xx;&quot;
            &quot;Database=xx;&quot;
            &quot;UID=xx;&quot;
            &quot;PWD=xx&quot;)
myCon = pyodbc.connect(cnxn_str)
</code></pre>
<p>Edit: With new connection &quot;Driver={ODBC Driver 18 for SQL Server};&quot;</p>
<p><code>[Microsoft][ODBC Driver 18 for SQL Server]TCP Provider: Error code 0x2746 (10054) (SQLDriverConnect)')</code></p>
<p>EDIT: root@vps:~# openssl version -a</p>
<blockquote>
<p>OpenSSL 3.0.2 15 Mar 2022 (Library: OpenSSL 3.0.2 15 Mar 2022) built
on: Thu Oct 27 17:06:56 2022 UTC platform: debian-amd64 options:
bn(64,64) compiler: gcc -fPIC -pthread -m64 -Wa,--noexecstack -Wall
-Wa,--noexecstack -g -O2                                                                                  -ffile-prefix-map=/build/openssl-WsPfAX/openssl-3.0.2=. -flto=auto -ffat-lto-object                                                                                 s -flto=auto -ffat-lto-objects -fstack-protector-strong -Wformat
-Werror=format-sec                                                                                 urity -DOPENSSL_TLS_SECURITY_LEVEL=2 -DOPENSSL_USE_NODELETE -DL_ENDIAN
-DOPENSSL_PI                                                                                 C -DOPENSSL_BUILDING_OPENSSL -DNDEBUG -Wdate-time -D_FORTIFY_SOURCE=2
OPENSSLDIR: &quot;/usr/lib/ssl&quot; ENGINESDIR:
&quot;/usr/lib/x86_64-linux-gnu/engines-3&quot; MODULESDIR:
&quot;/usr/lib/x86_64-linux-gnu/ossl-modules&quot; Seeding source: os-specific
CPUINFO: OPENSSL_ia32cap=0xffbaa2234f8bffff:0x400000283</p>
</blockquote>
"
"<p>I am playing around with the openAI API and I am trying to continue a conversation. For example:</p>
<pre class=""lang-py prettyprint-override""><code>import openai
openai.api_key = mykey
    
prompt= &quot;write me a haiku&quot;
    
response = openai.Completion.create(engine=&quot;text-davinci-001&quot;,
                                    prompt=prompt,
                                    max_tokens=50)
print(response)
</code></pre>
<p>This produces a Haiku in the following format:</p>
<pre><code>{
  &quot;choices&quot;: [
    {
      &quot;finish_reason&quot;: &quot;stop&quot;,
      &quot;index&quot;: 0,
      &quot;logprobs&quot;: null,
      &quot;text&quot;: &quot;\n\n\n\nThis world is\nfull of wonders\nSo much to see and do&quot;
    }
  ],
  &quot;created&quot;: 1670379922,
  &quot;id&quot;: &quot;cmpl-6KePalYQFhm1cXmwOOJdyKiygSMUq&quot;,
  &quot;model&quot;: &quot;text-davinci-001&quot;,
  &quot;object&quot;: &quot;text_completion&quot;,
  &quot;usage&quot;: {
    &quot;completion_tokens&quot;: 17,
    &quot;prompt_tokens&quot;: 5,
    &quot;total_tokens&quot;: 22
  }
}
</code></pre>
<p>Which is great. However, what if I now want to ask to &quot;write me another&quot;? If I use the the openAI playground chat or chatGPT, I am able to continue the conversation. I would like to do this via my python script. I notice I receive an <code>id</code> in response. Can I use this somehow to continue my conversation?</p>
"
"<pre><code> File &quot;f:\drug-traceability-blockchain-maddy\src\app.py&quot;, line 2, in &lt;module&gt;
    from web3 import Web3,HTTPProvider
  File &quot;C:\Users\Swapn\AppData\Local\Programs\Python\Python311\Lib\site-packages\web3\__init__.py&quot;, line 6, in &lt;module&gt;
    from eth_account import (
  File &quot;C:\Users\Swapn\AppData\Local\Programs\Python\Python311\Lib\site-packages\eth_account\__init__.py&quot;, line 1, in &lt;module&gt;
    from eth_account.account import (
  File &quot;C:\Users\Swapn\AppData\Local\Programs\Python\Python311\Lib\site-packages\eth_account\account.py&quot;, line 59, in &lt;module&gt;
    from eth_account.messages import (
  File &quot;C:\Users\Swapn\AppData\Local\Programs\Python\Python311\Lib\site-packages\eth_account\messages.py&quot;, line 26, in &lt;module&gt;
    from eth_account._utils.structured_data.hashing import (
  File &quot;C:\Users\Swapn\AppData\Local\Programs\Python\Python311\Lib\site-packages\eth_account\_utils\structured_data\hashing.py&quot;, line 9, in &lt;module&gt;
    from eth_abi import (
  File &quot;C:\Users\Swapn\AppData\Local\Programs\Python\Python311\Lib\site-packages\eth_abi\__init__.py&quot;, line 6, in &lt;module&gt;
    from eth_abi.abi import (  # NOQA
  File &quot;C:\Users\Swapn\AppData\Local\Programs\Python\Python311\Lib\site-packages\eth_abi\abi.py&quot;, line 1, in &lt;module&gt;
    from eth_abi.codec import (
  File &quot;C:\Users\Swapn\AppData\Local\Programs\Python\Python311\Lib\site-packages\eth_abi\codec.py&quot;, line 16, in &lt;module&gt;
    from eth_abi.decoding import (
  File &quot;C:\Users\Swapn\AppData\Local\Programs\Python\Python311\Lib\site-packages\eth_abi\decoding.py&quot;, line 14, in &lt;module&gt;
    from eth_abi.base import (
  File &quot;C:\Users\Swapn\AppData\Local\Programs\Python\Python311\Lib\site-packages\eth_abi\base.py&quot;, line 7, in &lt;module&gt;
    from .grammar import (
  File &quot;C:\Users\Swapn\AppData\Local\Programs\Python\Python311\Lib\site-packages\eth_abi\grammar.py&quot;, line 4, in &lt;module&gt;
    import parsimonious
  File &quot;C:\Users\Swapn\AppData\Local\Programs\Python\Python311\Lib\site-packages\parsimonious\__init__.py&quot;, line 9, in &lt;module&gt;
    from parsimonious.grammar import Grammar, TokenGrammar
  File &quot;C:\Users\Swapn\AppData\Local\Programs\Python\Python311\Lib\site-packages\parsimonious\grammar.py&quot;, line 14, in &lt;module&gt;
    from parsimonious.expressions import (Literal, Regex, Sequence, OneOf,
  File &quot;C:\Users\Swapn\AppData\Local\Programs\Python\Python311\Lib\site-packages\parsimonious\expressions.py&quot;, line 9, in &lt;module&gt;
    from inspect import getargspec
ImportError: cannot import name 'getargspec' from 'inspect' (C:\Users\Swapn\AppData\Local\Programs\Python\Python311\Lib\inspect.py)   
</code></pre>
<p>please help me .. how to solve this error.
this is blockchain project wgich i downloaded from github.
after that i was
npm install
npm start
truffle complile
truffle migrate
and for homepage of project
run app.py then i was receive this error.<a href=""https://i.sstatic.net/9WffU.png"" rel=""noreferrer"">Image of running errors</a></p>
"
"<p>I want to write a wrapper function for a known function, like</p>
<pre class=""lang-py prettyprint-override""><code>    def wrapper(*args, **kwargs)
         foo()
         return known_function(*args, **kwargs)
</code></pre>
<p>How can i add type-annotations to <code>wrapper</code>, such that it exactly follows the type annotations of <code>known_function</code></p>
<hr />
<p>I have looked at <code>ParamSpec</code>, but it appears to only work when the wrapper-function is generic and takes the inner function as argument.</p>
<pre class=""lang-py prettyprint-override""><code>    P = ParamSpec(&quot;P&quot;)
    T = TypeVar('T')
    def wrapper(func_arg_that_i_dont_want: Callable[P,T], *args: P.args, **kwargs: P.kwargs)
         foo()
         return known_function(*args, **kwargs)
</code></pre>
<p>Can i force the <code>P</code> to only be valid for <code>known_function</code>, without linking it to a <code>Callable</code>-argument?</p>
"
"<blockquote>
<p>The seaborn styles shipped by Matplotlib are deprecated since 3.6,
as they no longer correspond to the styles shipped by seaborn.
However, they will remain available as 'seaborn-v0_8-&lt;style&gt;'. Alternatively, directly use the seaborn API instead.</p>
</blockquote>
<p>I have tried this:</p>
<pre><code># use seaborn style
plt.style.use(&quot;seaborn&quot;)
</code></pre>
<p>but it is deprecated, and I want to remove this warning when I use the cmd in windows</p>
"
"<p>I am running this code using the <em>healpy</em> package. I am not using multiprocessing and I need it to run on a single core. It worked for a certain amount of time, but, when I run it now, the function <code>healpy.projector.GnomonicProj.projmap</code> takes all the available cores.</p>
<p>This is the incriminated code block:</p>
<pre><code>def Stacking () :

    f = lambda x,y,z: pixelfunc.vec2pix(xsize,x,y,z,nest=False)
    map_array = pixelfunc.ma_to_array(data)
    im = np.zeros((xsize, xsize))
    plt.figure()

    for i in range (nvoids) :
        sys.stdout.write(&quot;\r&quot; + str(i+1) + &quot;/&quot; + str(nvoids))
        sys.stdout.flush()
        proj = hp.projector.GnomonicProj(rot=[rav[i],decv[i]], xsize=xsize, reso=2*nRad*rad_deg[i]*60/(xsize))
        im += proj.projmap(map_array, f)

    im/=nvoids
    plt.imshow(im)
    plt.colorbar()
    plt.title(title + &quot; (Map)&quot;)
    plt.savefig(&quot;../Plots/stackedMap_&quot;+name+&quot;.png&quot;)

    return im
</code></pre>
<p>Does someone know why this function is running in parallel? And most important, does someone know a way to run it in a single core?</p>
<p>Thank you!</p>
"
"<p>Let me try to explain my issue with an example, I have a large corpus and a substring like below,</p>
<pre><code>corpus = &quot;&quot;&quot;very quick service, polite workers(cory, i think that's his name), i basically just drove there and got a quote(which seems to be very fair priced), then dropped off my car 4 days later(because they were fully booked until then), then i dropped off my car on my appointment day, then the same day the shop called me and notified me that the the job is done i can go pickup my car. when i go checked out my car i was amazed by the job they've done to it, and they even gave that dirty car a wash( prob even waxed it or coated it, cuz it was shiny as hell), tires shine, mats were vacuumed too. i gave them a dirty, broken car, they gave me back a what seems like a brand new car. i'm happy with the result, and i will def have all my car's work done by this place from now.&quot;&quot;&quot;

substring = &quot;&quot;&quot;until then then i dropped off my car on my appointment day then the same day the shop called me and notified me that the the job is done i can go pickup my car when i go checked out my car i was amazed by the job they ve done to it and they even gave that dirty car a wash prob even waxed it or coated it cuz it was shiny as hell tires shine mats were vacuumed too i gave them a dirty broken car they gave me back a what seems like a brand new car i m happy with the result and i will def have all my car s work done by this place from now&quot;&quot;&quot;
</code></pre>
<p>Both the substring and corpus are very similar but it not exact,</p>
<p>If I do something like,</p>
<pre><code>import re
re.search(substring, corpus, flags=re.I) # this will fail substring is not exact but rather very similar
</code></pre>
<p>In the corpus the substring is like below which is bit different from the substring I have because of that regular expression search is failing, can someone suggest a really good alternative for similar substring lookup,</p>
<pre><code>until then), then i dropped off my car on my appointment day, then the same day the shop called me and notified me that the the job is done i can go pickup my car. when i go checked out my car i was amazed by the job they've done to it, and they even gave that dirty car a wash( prob even waxed it or coated it, cuz it was shiny as hell), tires shine, mats were vacuumed too. i gave them a dirty, broken car, they gave me back a what seems like a brand new car. i'm happy with the result, and i will def have all my car's work done by this place from now
</code></pre>
<p>I did try difflib library but it was not satisfying my use-case.</p>
<p>Some background information,</p>
<p>The substring I have right now, is obtained some time ago from pre-processed corpus using this regex <code>re.sub(&quot;[^a-zA-Z]&quot;, &quot; &quot;, corpus)</code>.</p>
<p>But now I need to use that substring I have to do the reverse lookup in the corpus text and find the start and ending index in the corpus.</p>
"
"<p>I've have dataframe with column <strong>b</strong> with list elements, I need to create column <strong>c</strong> that counts number elements in list for every row. Here is toy example in Pandas:</p>
<pre><code>import pandas as pd

df = pd.DataFrame({'a': [1,2,3], 'b':[[1,2,3], [2], [5,0]]})

    a   b
0   1   [1, 2, 3]
1   2   [2]
2   3   [5, 0]

df.assign(c=df['b'].str.len())

    a   b           c
0   1   [1, 2, 3]   3
1   2   [2]         1
2   3   [5, 0]      2

</code></pre>
<p>Here is my equivalent in Polars:</p>
<pre><code>import polars as pl

dfp = pl.DataFrame({'a': [1,2,3], 'b':[[1,2,3], [2], [5,0]]})

dfp.with_columns(pl.col('b').map_elements(lambda x: len(x)).alias('c'))

</code></pre>
<p>I've a feeling that <code>.map_elements(lambda x: len(x))</code> is not optimal.</p>
<p>Is a better way to do it in Polars?</p>
"
"<p>I am using OpenAI's new Whisper model for STT, and I get <code>RuntimeError: &quot;slow_conv2d_cpu&quot; not implemented for 'Half'</code> when I try to run it.</p>
<p>Not sure</p>
<p>Here is the full error:</p>
<pre><code>Traceback (most recent call last):
  File &quot;/Users/reallymemorable/git/fp-stt/2-stt.py&quot;, line 20, in &lt;module&gt;
    result = whisper.decode(model, mel, options)
  File &quot;/opt/homebrew/lib/python3.10/site-packages/torch/autograd/grad_mode.py&quot;, line 27, in decorate_context
    return func(*args, **kwargs)
  File &quot;/opt/homebrew/lib/python3.10/site-packages/whisper/decoding.py&quot;, line 705, in decode
    result = DecodingTask(model, options).run(mel)
  File &quot;/opt/homebrew/lib/python3.10/site-packages/torch/autograd/grad_mode.py&quot;, line 27, in decorate_context
    return func(*args, **kwargs)
  File &quot;/opt/homebrew/lib/python3.10/site-packages/whisper/decoding.py&quot;, line 621, in run
    audio_features: Tensor = self._get_audio_features(mel)  # encoder forward pass
  File &quot;/opt/homebrew/lib/python3.10/site-packages/whisper/decoding.py&quot;, line 565, in _get_audio_features
    audio_features = self.model.encoder(mel)
  File &quot;/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py&quot;, line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File &quot;/opt/homebrew/lib/python3.10/site-packages/whisper/model.py&quot;, line 148, in forward
    x = F.gelu(self.conv1(x))
  File &quot;/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py&quot;, line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File &quot;/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/conv.py&quot;, line 313, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File &quot;/opt/homebrew/lib/python3.10/site-packages/whisper/model.py&quot;, line 43, in _conv_forward
    return super()._conv_forward(
  File &quot;/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/conv.py&quot;, line 309, in _conv_forward
    return F.conv1d(input, weight, bias, self.stride,
RuntimeError: &quot;slow_conv2d_cpu&quot; not implemented for 'Half'
</code></pre>
<p>Here is my code, though I don't think the issue is here:</p>
<pre><code>import whisper

model = whisper.load_model(&quot;base&quot;)

# load audio and pad/trim it to fit 30 seconds
audio = whisper.load_audio(&quot;speech-to-text-sample.wav&quot;)
audio = whisper.pad_or_trim(audio)

# make log-Mel spectrogram and move to the same device as the model
mel = whisper.log_mel_spectrogram(audio).to(model.device)

# detect the spoken language
_, probs = model.detect_language(mel)
print(f&quot;Detected language: {max(probs, key=probs.get)}&quot;)

# decode the audio
options = whisper.DecodingOptions()
result = whisper.decode(model, mel, options)

# print the recognized text
print(result.text)
</code></pre>
<p>How am I supposed to handle a wrong datatype error in a dependency?</p>
"
"<p>I am using pandas to read from gcs buckets and after making the required transformation and want to save it as parquet i got this error <code>No module named fsspec</code>, I solved it by just installing fsspec,gcsfs libraries but I just want to know why this error appeared and before it was running well.</p>
<p><code>I installed two libraries (fsspec,gcsfs) and works well but I want to know what the reason behind this error</code></p>
"
"<p>I often run into the mistake of using <code>gettext</code> in a context where the actual language is not set yet - instead of using the appropriate <code>gettext_lazy</code>.</p>
<p>Those errors can be difficult to catch - I would like to make them more visible appropriate logging, or possibly even throwing exceptions.</p>
<ul>
<li>Would it be correct, to write a wrapper function that logs an error in the case of <code>get_language() is None</code>?</li>
<li>Then of course, one would need to enforce that this wrapper is always used - introducing another subtle easy to make mistake. In general, I suppose monkey-patching could be used, but I'd like to avoid that if possible. The whole dispatch behind <code>django.util.translation</code> seems already relatively complex. Is there a clean way too hook this functionality in there?</li>
</ul>
"
"<p>Supposed I have a dataset:</p>
<pre><code>datasets = [0,1,2,3,4]
</code></pre>
<p>In scenario I, the code is:</p>
<pre><code>torch.manual_seed(1)

ran_sampler = RandomSampler(data_source=datasets)
for data in ran_sampler:
  print(data)
</code></pre>
<p>The result is <code>1,3,4,0,2</code>.</p>
<p>In scenario II, the code is:</p>
<pre><code>torch.manual_seed(1)

seed=1234
G = torch.Generator()
G.manual_seed(seed)

ran_sampler = RandomSampler(data_source=datasets)
dataloader = DataLoader(dataset=datasets, 
                        sampler=ran_sampler,
                        generator=G)
for data in ran_sampler:
  print(data)
</code></pre>
<p>The result is <code>1,3,4,0,2</code>. In fact, give any value to the variable <code>seed</code>, the result is still <code>1,3,4,0,2</code>.</p>
<p>In scenario III, the code is:</p>
<pre><code>torch.manual_seed(1)

ran_sampler = RandomSampler(data_source=datasets)
dataloader = DataLoader(dataset=datasets, 
                        sampler=ran_sampler)
for data in dataloader:
  print(data)
</code></pre>
<p>The result is <code>4,1,3,0,2</code>.</p>
<p>I check the <a href=""https://pytorch.org/docs/stable/_modules/torch/utils/data/sampler.html#RandomSampler"" rel=""noreferrer"">source code</a> of <code>RandomSampler</code> and find:</p>
<pre><code>seed = int(torch.empty((), dtype=torch.int64).random_().item())
generator = torch.Generator()
generator.manual_seed(seed)
</code></pre>
<p>It shows that <code>RandomSampler</code> would create a generator itself if no generator is given. Therefore in theory, my scenario I, II and III would output the same results but scenario III outputs a different results. Why would this happen? I am lost in the source code of <code>Dataloader</code> and I am confused about the relationship between Dataloader, sampler and generator.</p>
<p>I have already asked a question about <a href=""https://stackoverflow.com/questions/74580942/the-shuffling-order-of-dataloader-in-pytorch"">The shuffling order of DataLoader in pytorch</a>. I understand that <code>Dataloader</code> would pass the generator to sampler in certain environments but in my scenario III, the <code>RandomSampler</code> has a generator already.</p>
"
"<pre><code>from pydantic import BaseModel


class Request(BaseModel):
    num: int

    @validator(&quot;num&quot;)
    @classmethod
    def validate_num(cls, num: int) -&gt; int:
        return num
</code></pre>
<p>PyCharm gives the warning &quot;This decorator will not receive a callable it may expect; the built-in decorator returns a special object&quot; for the above code. I don't think the warning is clear so I'd appreciate some help.</p>
<p>When I change the above code to this:</p>
<pre><code>from fastapi.exceptions import RequestValidationError

from pydantic import BaseModel, validator
from pydantic.error_wrappers import ErrorWrapper


class Request(BaseModel):
    num: int

    @classmethod
    @validator(&quot;num&quot;)
    def validate_num(cls, num: int) -&gt; int:
        if num &lt; 0:
            raise RequestValidationError([ErrorWrapper(ValueError(&quot;error&quot;), ())])
        return num


request = Request(num=-2)
</code></pre>
<p>The warning goes away, but the code executes without any problem when it's not supposed to, meaning that the validation has been ignored for some reason.</p>
"
"<p>I am learning and playing around with Python and I came up with the following test code (please be aware that <em>I would not write productive code like that</em>, but when learning new languages I like to play around with the language's corner cases):</p>
<pre><code>a = None    
print(None == a) # I expected True, I got True

b = 1
print(None == b) # I expected False, I got False

class MyNone:
    # Called if I compare some myMyNone == somethingElse
    def __eq__(self, __o: object) -&gt; bool:
        return True

c = MyNone()
print (None == c) # !!! I expected False, I got True !!!
</code></pre>
<p><strong>Please see the very last line of the code example.</strong></p>
<p>How can it be that <code>None == something</code>, where something is clearly not <code>None</code>, return <code>True</code>? I would have expected that result for <code>something == None</code>, but not for <code>None == something</code>.</p>
<p>I expected that it would call <code>None is something</code> behind the scenes.</p>
<p>So I think the question boils down to: <strong>How does the <code>__eq__</code> method of the <code>None</code> singleton object look like and how could I have found that out?</strong></p>
<hr />
<p>PS: I am aware of <a href=""https://peps.python.org/pep-0008/"" rel=""nofollow noreferrer"">PEP-0008</a> and its quote</p>
<blockquote>
<p>Comparisons to singletons like None should always be done with is or is not, never the equality operators.</p>
</blockquote>
<p>but I <em>still</em> would like to know why <code>print (None == c)</code> in the above example returns <code>True</code>.</p>
"
"<p>I'm using
<code>AutoModelForCausalLM</code> and <code>AutoTokenizer</code> to generate text output with <code>DialoGPT</code>.</p>
<p>For whatever reason, even when using the provided examples from huggingface I get this warning:</p>
<blockquote>
<p>A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set <code>padding_side='left'</code> when initializing the tokenizer.</p>
</blockquote>
<pre><code>from transformers import AutoModelForCausalLM, AutoTokenizer
import torch


tokenizer = AutoTokenizer.from_pretrained(&quot;microsoft/DialoGPT-medium&quot;)
model = AutoModelForCausalLM.from_pretrained(&quot;microsoft/DialoGPT-medium&quot;)

# Let's chat for 5 lines
for step in range(5):
    # encode the new user input, add the eos_token and return a tensor in Pytorch
    new_user_input_ids = tokenizer.encode(input(&quot;&gt;&gt; User:&quot;) + tokenizer.eos_token, return_tensors='pt')

    # append the new user input tokens to the chat history
    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step &gt; 0 else new_user_input_ids

    # generated a response while limiting the total chat history to 1000 tokens, 
    chat_history_ids = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)

    # pretty print last ouput tokens from bot
    print(&quot;DialoGPT: {}&quot;.format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))
</code></pre>
<p>Code provided by <a href=""https://huggingface.co/microsoft/DialoGPT-medium?text=Hey+my+name+is+Thomas%21+How+are+you%3F"" rel=""noreferrer"">microsoft on the model card at huggingface</a></p>
<p>I've tried adding padding_side='left' to the tokenizer but that doesn't change anything.
Apparently (from some reading) DialoGPT wants the padding on the right side anyways?
I can't figure this out, there are few results when I tried googling it.</p>
<p>I was able to suppress the warnings like this:</p>
<pre><code>from transformers.utils import logging

logging.set_verbosity_info()
</code></pre>
<p>But this doesn't seem like the best answer?</p>
"
"<p>I am trying to iterate through a folder and delete any file that is a duplicate image (but different name). After running this script all files get deleted except for one. There are at least a dozen unique ones out of about 5,000. Any help understanding why this is happening would be appreciated.</p>
<pre><code>import os
import cv2 

directory = r'C:\Users\Grid\scratch'
 
for filename in os.listdir(directory):
    a=directory+'\\'+filename
    n=(cv2.imread(a))
    q=0
    for filename in os.listdir(directory):
        b=directory+'\\'+filename
        m=(cv2.imread(b))
        comparison = n == m
        equal_arrays = comparison.all()
        if equal_arrays==True and q==1:
            os.remove(b)
        q=1
</code></pre>
"
"<p>I'm trying to figure out how to use properly builtin <a href=""https://docs.python.org/3/library/argparse.html"" rel=""noreferrer"">argparse</a> module to get a similar output than tools
such as git where I can display a nice help with all &quot;root commands&quot; nicely grouped, ie:</p>
<pre><code>$ git --help
usage: git [--version] [--help] [-C &lt;path&gt;] [-c &lt;name&gt;=&lt;value&gt;]
           [--exec-path[=&lt;path&gt;]] [--html-path] [--man-path] [--info-path]
           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]
           [--git-dir=&lt;path&gt;] [--work-tree=&lt;path&gt;] [--namespace=&lt;name&gt;]
           [--super-prefix=&lt;path&gt;] [--config-env=&lt;name&gt;=&lt;envvar&gt;]
           &lt;command&gt; [&lt;args&gt;]

These are common Git commands used in various situations:

start a working area (see also: git help tutorial)
   clone     Clone a repository into a new directory
   init      Create an empty Git repository or reinitialize an existing one

work on the current change (see also: git help everyday)
   add       Add file contents to the index
   mv        Move or rename a file, a directory, or a symlink
   restore   Restore working tree files
   rm        Remove files from the working tree and from the index

examine the history and state (see also: git help revisions)
   bisect    Use binary search to find the commit that introduced a bug
   diff      Show changes between commits, commit and working tree, etc
   grep      Print lines matching a pattern
   log       Show commit logs
   show      Show various types of objects
   status    Show the working tree status

grow, mark and tweak your common history
   branch    List, create, or delete branches
   commit    Record changes to the repository
   merge     Join two or more development histories together
   rebase    Reapply commits on top of another base tip
   reset     Reset current HEAD to the specified state
   switch    Switch branches
   tag       Create, list, delete or verify a tag object signed with GPG

collaborate (see also: git help workflows)
   fetch     Download objects and refs from another repository
   pull      Fetch from and integrate with another repository or a local branch
   push      Update remote refs along with associated objects

'git help -a' and 'git help -g' list available subcommands and some
concept guides. See 'git help &lt;command&gt;' or 'git help &lt;concept&gt;'
to read about a specific subcommand or concept.
See 'git help git' for an overview of the system.
</code></pre>
<p>Here's my attempt:</p>
<pre><code>from argparse import ArgumentParser


class FooCommand:
    def __init__(self, subparser):
        self.name = &quot;Foo&quot;
        self.help = &quot;Foo help&quot;
        subparser.add_parser(self.name, help=self.help)


class BarCommand:
    def __init__(self, subparser):
        self.name = &quot;Bar&quot;
        self.help = &quot;Bar help&quot;
        subparser.add_parser(self.name, help=self.help)


class BazCommand:
    def __init__(self, subparser):
        self.name = &quot;Baz&quot;
        self.help = &quot;Baz help&quot;
        subparser.add_parser(self.name, help=self.help)


def test1():
    parser = ArgumentParser(description=&quot;Test1 ArgumentParser&quot;)
    root = parser.add_subparsers(dest=&quot;command&quot;, description=&quot;All Commands:&quot;)

    # Group1
    FooCommand(root)
    BarCommand(root)

    # Group2
    BazCommand(root)

    args = parser.parse_args()
    print(args)


def test2():
    parser = ArgumentParser(description=&quot;Test2 ArgumentParser&quot;)

    # Group1
    cat1 = parser.add_subparsers(dest=&quot;command&quot;, description=&quot;Category1 Commands:&quot;)
    FooCommand(cat1)
    BarCommand(cat1)

    # Group2
    cat2 = parser.add_subparsers(dest=&quot;command&quot;, description=&quot;Category2 Commands:&quot;)
    BazCommand(cat2)

    args = parser.parse_args()
    print(args)
</code></pre>
<p>If you run <code>test1</code> you'd get:</p>
<pre><code>$ python mcve.py --help
usage: mcve.py [-h] {Foo,Bar,Baz} ...

Test1 ArgumentParser

options:
  -h, --help     show this help message and exit

subcommands:
  All Commands:

  {Foo,Bar,Baz}
    Foo          Foo help
    Bar          Bar help
    Baz          Baz help
</code></pre>
<p>Obviously this is not what I want, in there I just see all commands in a flat list, no groups or whatsoever... so the next logical attempt would be trying to group them. But if I run <code>test2</code> I'll get:</p>
<pre><code>$ python mcve.py --help
usage: mcve.py [-h] {Foo,Bar} ...
mcve.py: error: cannot have multiple subparser arguments
</code></pre>
<p>Which obviously means I'm not using properly argparse to accomplish the task at hand. So, is it possible to use argparse to achieve a similar behaviour than git? In the past I've relied on &quot;hacks&quot; so I thought the best practice here would be using the concept of <code>add_subparsers</code> but it seems I didn't understand properly that concept.</p>
"
"<p>I have the following dataframe:</p>
<pre><code>d_test = {
    'name' : ['South Beach', 'Dog', 'Bird', 'Ant', 'Big Dog', 'Beach', 'Dear', 'Cat'],
    'cluster_number' : [1, 2, 3, 3, 2, 1, 4, 2]
}
df_test = pd.DataFrame(d_test)
</code></pre>
<p>I want to identify similar names in <code>name</code> column if those names belong to one cluster number and create unique id for them. For example <code>South Beach</code> and <code>Beach</code> belong to cluster number <code>1</code> and their similarity score is pretty high. So we associate it with unique id, say <code>1</code>. Next cluster is number <code>2</code> and three entities from <code>name</code> column belong to this cluster: <code>Dog</code>, <code>Big Dog</code> and <code>Cat</code>. <code>Dog</code> and <code>Big Dog</code> have high similarity score and their unique id will be, say <code>2</code>. For <code>Cat</code> unique id will be, say <code>3</code>. And so on.</p>
<p>I created a code for the logic above:</p>
<pre><code># pip install thefuzz
from thefuzz import fuzz

d_test = {
    'name' : ['South Beach', 'Dog', 'Bird', 'Ant', 'Big Dog', 'Beach', 'Dear', 'Cat'],
    'cluster_number' : [1, 2, 3, 3, 2, 1, 4, 2]
}

df_test = pd.DataFrame(d_test)

df_test['id'] = 0

i = 1
for index, row in df_test.iterrows():
    for index_, row_ in df_test.iterrows():
        if row['cluster_number'] == row_['cluster_number'] and row_['id'] == 0:
            if fuzz.ratio(row['name'], row_['name']) &gt; 50:
                df_test.loc[index_,'id'] = int(i)
                is_i_used = True
    if is_i_used == True:
        i += 1
        is_i_used = False
                           
</code></pre>
<p>Code generates expected result:</p>
<pre><code>    name        cluster_number id
0   South Beach 1              1
1   Dog         2              2
2   Bird        3              3
3   Ant         3              4
4   Big Dog     2              2
5   Beach       1              1
6   Dear        4              5
7   Cat         2              6
</code></pre>
<p>Note, for <code>Cat</code> we got <code>id</code> as <code>6</code> but it is fine because it is unique anyway.</p>
<p>While algorithm above works for test data I am not able to use it for real data that I have (about 1 million rows) and I am trying to understand how to vectorize the code and get rid of two for-loops.</p>
<p>Also <code>thefuzz</code> module has <code>process</code> function and it allows to process data at once:</p>
<pre><code>from thefuzz import process
out = process.extract(&quot;Beach&quot;, df_test['name'], limit=len(df_test))
</code></pre>
<p>But I don't see if it can help with speeding up the code.</p>
"
"<p>I wanted to run external programs using python but I receive an error saying I don't have the file</p>
<p>the code I wrote:</p>
<pre><code>import subprocess

subprocess.run([&quot;ls&quot;, &quot;-l&quot;])
</code></pre>
<p>Output:</p>
<pre><code>Traceback (most recent call last):
  File &quot;C:\Users\hahan\desktop\Pythonp\main.py&quot;, line 3, in &lt;module&gt;
    subprocess.run([&quot;ls&quot;, &quot;-l&quot;])
  File &quot;C:\Users\hahan\AppData\Local\Programs\Python\Python310\lib\subprocess.py&quot;, line 501, in run
    with Popen(*popenargs, **kwargs) as process:
  File &quot;C:\Users\hahan\AppData\Local\Programs\Python\Python310\lib\subprocess.py&quot;, line 969, in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
  File &quot;C:\Users\hahan\AppData\Local\Programs\Python\Python310\lib\subprocess.py&quot;, line 1438, in _execute_child
    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,
FileNotFoundError: [WinError 2] The system cannot find the file specified
</code></pre>
<p>I expected it to return the files in that directory</p>
"
"<p>I have a generator <code>gen</code>, with the following properties:</p>
<ul>
<li>it's quite expensive to make it yield (more expensive than creating the generator)</li>
<li>the elements take up a fair amount of memory</li>
<li>sometimes all of the <code>__next__</code> calls will throw an exception, but creating the generator doesn't tell you when that will happen</li>
</ul>
<p>I didn't implement the generator myself.</p>
<p>Is there a way to make the generator yield its first element (I will do this in a try/except), without having the generator subsequently start on the second element if I loop through it afterwards?</p>
<p>I thought of creating some code like this:</p>
<pre><code>try:
    first = next(gen)
except StopIterator:
    return None
except Exception:
    print(&quot;Generator throws exception on a yield&quot;)

# looping also over the first element which we yielded already
for thing in (first, *gen):
    do_something_complicated(thing)
</code></pre>
<p>Solutions I can see which are not very nice:</p>
<ol>
<li>Create generator, test first element, create a new generator, loop through the second one.</li>
<li>Put the entire for loop in a try/except; not so nice because the exception thrown by the yield is very general and it would potentially catch other things.</li>
<li>Yield first element, test it, then reform a new generator from the first element and the rest of <code>gen</code> (ideally without extracting all of <code>gen</code>'s elements into a list, since this could take a lot of memory).</li>
</ol>
<p>For 3, which seems like the best solution, a nearly-there example would be the example I gave above, but I believe that would just extract all the elements of <code>gen</code> into a tuple before we start iterating, which I would like to avoid.</p>
"
"<p>there have been explanations about the different between <code>miniforge</code> and <code>miniconda</code></p>
<blockquote>
<p><code>miniforge</code> is the community (conda-forge) driven minimalistic conda installer. Subsequent package installations come thus from conda-forge channel.
<code>miniconda</code> is the Anaconda (company) driven minimalistic conda installer. Subsequent package installations come from the anaconda channels (default or otherwise).</p>
</blockquote>
<p>as for <a href=""https://conda-forge.org/miniforge/"" rel=""noreferrer"">mambaforge, mambaforge-pypy3, miniforge, miniforge-pypy3</a>, how do we choose which package to install?</p>
"
"<p>I have tried to install new package in conda for windows using the following command:</p>
<p><code>conda install -c conda-forge python-pdfkit</code></p>
<p>but got the following error:</p>
<blockquote>
<p>Collecting package metadata (current_repodata.json): done
Solving environment: failed with initial frozen solve. Retrying with flexible solve.
Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.</p>
</blockquote>
<p>I have tried the following workarounds but no use, still getting the same error:</p>
<p>Workaround 1:</p>
<pre><code>$conda create --name myenv
$conda activate myenv
</code></pre>
<p>Workaround 2:</p>
<pre><code>conda config --set ssl_verify false
</code></pre>
"
"<p>There's a common error in the code where people write something like:</p>
<pre><code>if (id):
    query.filter(row.id == id)
</code></pre>
<p>instead of</p>
<pre><code>if (id):
    query = query.filter(row.id == id)
</code></pre>
<p>The code looks &quot;valid&quot; and it's very hard to spot these by hand. In C++ there's the <code>[[nodiscard]]</code> function attribute that effectively prevents this mistake by enforcing the usage of the return value. I wasn't able to find anything similar in Python, does it exist?</p>
<p><strong>UPD</strong> A similar question: <a href=""https://stackoverflow.com/questions/51858215/how-to-make-pylint-report-unused-return-values"">How to make pylint report unused return values</a></p>
<p>I've checked pylint and mypy, apparently neither of them catch this as of today.</p>
<p><strong>UPD2</strong> A pylint feature request: <a href=""https://github.com/PyCQA/pylint/issues/7935"" rel=""noreferrer"">https://github.com/PyCQA/pylint/issues/7935</a></p>
"
"<p>I've been trying to get into ML and I wanted to follow a course on it but it requires Tensorflow and I've been trying to get that working on my system. I have the 2021 14&quot; 16GB Macbook Pro with the M1 Pro Chip and I am running Ventura 13.1. I have been following <a href=""https://caffeinedev.medium.com/how-to-install-tensorflow-on-m1-mac-8e9b91d93706"" rel=""noreferrer"">this article</a> as well as digging around about getting Tensorflow working on M1 but to no avail. I managed to get tensorflow-macos installed in my environment as well as tensorflow-metal but when I try to run some sample code in Juyter, I'm getting an error that I do not understand. In Jupyter, when I run:</p>
<pre><code>import tensorflow as tf print(&quot;Num GPUs Available: &quot;, len(tf.config.experimental.list_physical_devices('GPU')))
</code></pre>
<p>I get</p>
<p><code>Num GPUs Available: 1</code></p>
<p>So it does seem like I have tensorflow and metal installed, but when I try to run the rest of the code, I get:</p>
<pre><code>TensorFlow version: 2.11.0
Num GPUs Available:  1
Metal device set to: Apple M1 Pro
WARNING:tensorflow:AutoGraph could not transform &lt;function normalize_img at 0x14a4cec10&gt; and will run it as-is.
Cause: Unable to locate the source code of &lt;function normalize_img at 0x14a4cec10&gt;. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
2022-12-13 13:54:33.658225: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.
2022-12-13 13:54:33.658309: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -&gt; physical PluggableDevice (device: 0, name: METAL, pci bus id: &lt;undefined&gt;)
WARNING:tensorflow:AutoGraph could not transform &lt;function normalize_img at 0x14a4cec10&gt; and will run it as-is.
Cause: Unable to locate the source code of &lt;function normalize_img at 0x14a4cec10&gt;. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING: AutoGraph could not transform &lt;function normalize_img at 0x14a4cec10&gt; and will run it as-is.
Cause: Unable to locate the source code of &lt;function normalize_img at 0x14a4cec10&gt;. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
Epoch 1/12
2022-12-13 13:54:34.162300: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz
2022-12-13 13:54:34.163015: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.
2022-12-13 13:54:35.383325: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:418 : NOT_FOUND: could not find registered platform with id: 0x14a345660
2022-12-13 13:54:35.383350: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:418 : NOT_FOUND: could not find registered platform with id: 0x14a345660
2022-12-13 13:54:35.389028: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:418 : NOT_FOUND: could not find registered platform with id: 0x14a345660
2022-12-13 13:54:35.389049: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:418 : NOT_FOUND: could not find registered platform with id: 0x14a345660
2022-12-13 13:54:35.401250: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:418 : NOT_FOUND: could not find registered platform with id: 0x14a345660
2022-12-13 13:54:35.401274: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:418 : NOT_FOUND: could not find registered platform with id: 0x14a345660
2022-12-13 13:54:35.405004: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:418 : NOT_FOUND: could not find registered platform with id: 0x14a345660
2022-12-13 13:54:35.405025: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:418 : NOT_FOUND: could not find registered platform with id: 0x14a345660
---------------------------------------------------------------------------
NotFoundError                             Traceback (most recent call last)
File &lt;timed exec&gt;:45

File ~/conda/envs/mlp3/lib/python3.8/site-packages/keras/utils/traceback_utils.py:70, in filter_traceback.&lt;locals&gt;.error_handler(*args, **kwargs)
     67     filtered_tb = _process_traceback_frames(e.__traceback__)
     68     # To get the full stack trace, call:
     69     # `tf.debugging.disable_traceback_filtering()`
---&gt; 70     raise e.with_traceback(filtered_tb) from None
     71 finally:
     72     del filtered_tb

File ~/conda/envs/mlp3/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:52, in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     50 try:
     51   ctx.ensure_initialized()
---&gt; 52   tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
     53                                       inputs, attrs, num_outputs)
     54 except core._NotOkStatusException as e:
     55   if name is not None:

NotFoundError: Graph execution error:

Detected at node 'StatefulPartitionedCall_6' defined at (most recent call last):
    File &quot;/Users/imigh/conda/envs/mlp3/lib/python3.8/runpy.py&quot;, line 194, in _run_module_as_main
      return _run_code(code, main_globals, None,
    File &quot;/Users/imigh/conda/envs/mlp3/lib/python3.8/runpy.py&quot;, line 87, in _run_code
      exec(code, run_globals)
    File &quot;/Users/imigh/conda/envs/mlp3/lib/python3.8/site-packages/ipykernel_launcher.py&quot;, line 17, in &lt;module&gt;
      app.launch_new_instance()
    File &quot;/Users/imigh/conda/envs/mlp3/lib/python3.8/site-packages/traitlets/config/application.py&quot;, line 992, in launch_instance
      app.start()
    File &quot;/Users/imigh/conda/envs/mlp3/lib/python3.8/site-packages/ipykernel/kernelapp.py&quot;, line 711, in start
      self.io_loop.start()
    File &quot;/Users/imigh/conda/envs/mlp3/lib/python3.8/site-packages/tornado/platform/asyncio.py&quot;, line 215, in start
      self.asyncio_loop.run_forever()
    File &quot;/Users/imigh/conda/envs/mlp3/lib/python3.8/asyncio/base_events.py&quot;, line 570, in run_forever
      self._run_once()
    File &quot;/Users/imigh/conda/envs/mlp3/lib/python3.8/asyncio/base_events.py&quot;, line 1859, in _run_once
      handle._run()
    File &quot;/Users/imigh/conda/envs/mlp3/lib/python3.8/asyncio/events.py&quot;, line 81, in _run
      self._context.run(self._callback, *self._args)
    File &quot;/Users/imigh/conda/envs/mlp3/lib/python3.8/site-packages/ipykernel/kernelbase.py&quot;, line 510, in dispatch_queue
      await self.process_one()
    File &quot;/Users/imigh/conda/envs/mlp3/lib/python3.8/site-packages/ipykernel/kernelbase.py&quot;, line 499, in process_one
      await dispatch(*args)
    File &quot;/Users/imigh/conda/envs/mlp3/lib/python3.8/site-packages/ipykernel/kernelbase.py&quot;, line 406, in dispatch_shell
      await result
    File &quot;/Users/imigh/conda/envs/mlp3/lib/python3.8/site-packages/ipykernel/kernelbase.py&quot;, line 729, in execute_request
      reply_content = await reply_content
    File &quot;/Users/imigh/conda/envs/mlp3/lib/python3.8/site-packages/ipykernel/ipkernel.py&quot;, line 411, in do_execute
      res = shell.run_cell(
    File &quot;/Users/imigh/conda/envs/mlp3/lib/python3.8/site-packages/ipykernel/zmqshell.py&quot;, line 531, in run_cell
      return super().run_cell(*args, **kwargs)
    File &quot;/Users/imigh/conda/envs/mlp3/lib/python3.8/site-packages/IPython/core/interactiveshell.py&quot;, line 2940, in run_cell
      result = self._run_cell(
    File &quot;/Users/imigh/conda/envs/mlp3/lib/python3.8/site-packages/IPython/core/interactiveshell.py&quot;, line 2995, in _run_cell
      return runner(coro)
    File &quot;/Users/imigh/conda/envs/mlp3/lib/python3.8/site-packages/IPython/core/async_helpers.py&quot;, line 129, in _pseudo_sync_runner
      coro.send(None)
    File &quot;/Users/imigh/conda/envs/mlp3/lib/python3.8/site-packages/IPython/core/interactiveshell.py&quot;, line 3194, in run_cell_async
      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,
    File &quot;/Users/imigh/conda/envs/mlp3/lib/python3.8/site-packages/IPython/core/interactiveshell.py&quot;, line 3373, in run_ast_nodes
      if await self.run_code(code, result, async_=asy):
    File &quot;/Users/imigh/conda/envs/mlp3/lib/python3.8/site-packages/IPython/core/interactiveshell.py&quot;, line 3433, in run_code
      exec(code_obj, self.user_global_ns, self.user_ns)
    File &quot;/var/folders/k4/vgd34_w913ndkfkmvgssqgjr0000gn/T/ipykernel_16072/1016625245.py&quot;, line 1, in &lt;module&gt;
      get_ipython().run_cell_magic('time', '', 'import tensorflow as tf\nimport tensorflow_datasets as tfds\nprint(&quot;TensorFlow version:&quot;, tf.__version__)\nprint(&quot;Num GPUs Available: &quot;, len(tf.config.experimental.list_physical_devices(\'GPU\')))\ntf.config.list_physical_devices(\'GPU\')\n(ds_train, ds_test), ds_info = tfds.load(\n    \'mnist\',\n    split=[\'train\', \'test\'],\n    shuffle_files=True,\n    as_supervised=True,\n    with_info=True,\n)\ndef normalize_img(image, label):\n  &quot;&quot;&quot;Normalizes images: `uint8` -&gt; `float32`.&quot;&quot;&quot;\n  return tf.cast(image, tf.float32) / 255., label\nbatch_size = 128\nds_train = ds_train.map(\n    normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)\nds_train = ds_train.cache()\nds_train = ds_train.shuffle(ds_info.splits[\'train\'].num_examples)\nds_train = ds_train.batch(batch_size)\nds_train = ds_train.prefetch(tf.data.experimental.AUTOTUNE)\nds_test = ds_test.map(\n    normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)\nds_test = ds_test.batch(batch_size)\nds_test = ds_test.cache()\nds_test = ds_test.prefetch(tf.data.experimental.AUTOTUNE)\nmodel = tf.keras.models.Sequential([\n  tf.keras.layers.Conv2D(32, kernel_size=(3, 3),\n                 activation=\'relu\'),\n  tf.keras.layers.Conv2D(64, kernel_size=(3, 3),\n                 activation=\'relu\'),\n  tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n#   tf.keras.layers.Dropout(0.25),\n  tf.keras.layers.Flatten(),\n  tf.keras.layers.Dense(128, activation=\'relu\'),\n#   tf.keras.layers.Dropout(0.5),\n  tf.keras.layers.Dense(10, activation=\'softmax\')\n])\nmodel.compile(\n    loss=\'sparse_categorical_crossentropy\',\n    optimizer=tf.keras.optimizers.Adam(0.001),\n    metrics=[\'accuracy\'],\n)\nmodel.fit(\n    ds_train,\n    epochs=12,\n    validation_data=ds_test,\n)\n')
    File &quot;/Users/imigh/conda/envs/mlp3/lib/python3.8/site-packages/IPython/core/interactiveshell.py&quot;, line 2417, in run_cell_magic
      result = fn(*args, **kwargs)
    File &quot;/Users/imigh/conda/envs/mlp3/lib/python3.8/site-packages/IPython/core/magics/execution.py&quot;, line 1321, in time
      out = eval(code_2, glob, local_ns)
    File &quot;&lt;timed exec&gt;&quot;, line 45, in &lt;module&gt;
    File &quot;/Users/imigh/conda/envs/mlp3/lib/python3.8/site-packages/keras/utils/traceback_utils.py&quot;, line 65, in error_handler
      return fn(*args, **kwargs)
    File &quot;/Users/imigh/conda/envs/mlp3/lib/python3.8/site-packages/keras/engine/training.py&quot;, line 1650, in fit
      tmp_logs = self.train_function(iterator)
    File &quot;/Users/imigh/conda/envs/mlp3/lib/python3.8/site-packages/keras/engine/training.py&quot;, line 1249, in train_function
      return step_function(self, iterator)
    File &quot;/Users/imigh/conda/envs/mlp3/lib/python3.8/site-packages/keras/engine/training.py&quot;, line 1233, in step_function
      outputs = model.distribute_strategy.run(run_step, args=(data,))
    File &quot;/Users/imigh/conda/envs/mlp3/lib/python3.8/site-packages/keras/engine/training.py&quot;, line 1222, in run_step
      outputs = model.train_step(data)
    File &quot;/Users/imigh/conda/envs/mlp3/lib/python3.8/site-packages/keras/engine/training.py&quot;, line 1027, in train_step
      self.optimizer.minimize(loss, self.trainable_variables, tape=tape)
    File &quot;/Users/imigh/conda/envs/mlp3/lib/python3.8/site-packages/keras/optimizers/optimizer_experimental/optimizer.py&quot;, line 527, in minimize
      self.apply_gradients(grads_and_vars)
    File &quot;/Users/imigh/conda/envs/mlp3/lib/python3.8/site-packages/keras/optimizers/optimizer_experimental/optimizer.py&quot;, line 1140, in apply_gradients
      return super().apply_gradients(grads_and_vars, name=name)
    File &quot;/Users/imigh/conda/envs/mlp3/lib/python3.8/site-packages/keras/optimizers/optimizer_experimental/optimizer.py&quot;, line 634, in apply_gradients
      iteration = self._internal_apply_gradients(grads_and_vars)
    File &quot;/Users/imigh/conda/envs/mlp3/lib/python3.8/site-packages/keras/optimizers/optimizer_experimental/optimizer.py&quot;, line 1166, in _internal_apply_gradients
      return tf.__internal__.distribute.interim.maybe_merge_call(
    File &quot;/Users/imigh/conda/envs/mlp3/lib/python3.8/site-packages/keras/optimizers/optimizer_experimental/optimizer.py&quot;, line 1216, in _distributed_apply_gradients_fn
      distribution.extended.update(
    File &quot;/Users/imigh/conda/envs/mlp3/lib/python3.8/site-packages/keras/optimizers/optimizer_experimental/optimizer.py&quot;, line 1211, in apply_grad_to_update_var
      return self._update_step_xla(grad, var, id(self._var_key(var)))
Node: 'StatefulPartitionedCall_6'
could not find registered platform with id: 0x14a345660
     [[{{node StatefulPartitionedCall_6}}]] [Op:__inference_train_function_1261]
</code></pre>
<p>Sorry for just dumping the entire error code but as you can see something's gone awry. It only seems to run the first Epoch and I'm not sure what's going wrong. I've followed everything in that guide as well as the instructions from <a href=""https://developer.apple.com/metal/tensorflow-plugin/"" rel=""noreferrer"">tensor flow-metal</a>. I've looked around seeminly everywhere but this is as far as I've gotten after hours of battling. I just updated my Mac today so the Xcode command line tools should be up to date. Any and all advice or helping me decipher the error code would be greatly appreciated. I just want to learn Machine Learning but I can't even follow my course without this working.</p>
<p>I've uninstalled and reinstalled Conda Miniforge for M1 several times. I've created and tried the steps in a blank environment. I've followed the steps listed in the guides I've linked above and went through them multiple times. I was originally getting some issues with numpy, h5py, grcio, and protobuf but after tinkering with the versions I no longer get error codes for them, so I'm not sure if that's all good but I don't see any explicit mentions. I've also ran</p>
<pre><code>conda install -c conda-forge openblas
</code></pre>
<p>after looking at <a href=""https://stackoverflow.com/questions/68996176/tensorflow-2-5-mac-m1-installing-problem-compatibility-with-numpy-library-co"">this page from StackOverflow</a> from someone with a similar issue, but I'm still getting this error.</p>
"
"<p>I have the following time:</p>
<pre class=""lang-py prettyprint-override""><code>time = datetime.timedelta(days=1, hours=4, minutes=5, seconds=33, milliseconds=623)
</code></pre>
<p>Is it possible, to convert the time in milliseconds? <br>
Like this:</p>
<pre class=""lang-py prettyprint-override""><code>101133623.0
</code></pre>
"
"<p>I've been finding a strange behaviour of <code>log</code> functions in C++ and numpy about the behaviour of <code>log</code> function handling complex infinite numbers. Specifically, <code>log(inf + inf * 1j)</code> equals <code>(inf + 0.785398j)</code> when I expect it to be <code>(inf + nan * 1j)</code>.</p>
<p>When taking the log of a complex number, the real part is the log of the absolute value of the input and the imaginary part is the phase of the input. Returning 0.785398 as the imaginary part of <code>log(inf + inf * 1j)</code> means it assumes the <code>inf</code>s in the real and the imaginary part have the same length.
This assumption does not seem to be consistent with other calculation, for example, <code>inf - inf == nan</code>, <code>inf / inf == nan</code> which assumes 2 <code>inf</code>s do not necessarily have the same values.</p>
<p>Why is the assumption for <code>log(inf + inf * 1j)</code> different?</p>
<p>Reproducing C++ code:</p>
<pre><code>#include &lt;complex&gt;
#include &lt;limits&gt;
#include &lt;iostream&gt;
int main() {
    double inf = std::numeric_limits&lt;double&gt;::infinity();
    std::complex&lt;double&gt; b(inf, inf);
    std::complex&lt;double&gt; c = std::log(b);
    std::cout &lt;&lt; c &lt;&lt; &quot;\n&quot;;
}
</code></pre>
<p>Reproducing Python code (numpy):</p>
<pre><code>import numpy as np

a = complex(float('inf'), float('inf'))
print(np.log(a))
</code></pre>
<p>EDIT: Thank you for everyone who's involved in the discussion about the historical reason and the mathematical reason. All of you turn this naive question into a really interesting discussion. The provided answers are all of high quality and I wish I can accept more than 1 answers. However, I've decided to accept @simon's answer as it explains in more detail the mathematical reason and provided a link to the document explaining the logic (although I can't fully understand it).</p>
"
"<p>Find a polynomial time algorithm or prove np-hardness for the following problem:</p>
<p>Given two strings <code>s1=a1, a2,...,ak</code> and <code>s2=b1,...,bk,</code> where <code>s2</code> is a random permutation of <code>s1</code>.</p>
<p>We now want to build <code>s1</code> out of <code>s2</code>. A construction works as follows:</p>
<p>Pick a letter from <code>s2</code>, that is equal to <code>a1</code> and remove it.</p>
<p>Proceed, with letter <code>a2</code> and remove it and so on, until <code>s2</code> is empty.</p>
<p>Note, that the same letter can occur multiple times in <code>s2</code>. Let <code>C = c1, c2,...,ck</code> be the constructed sequence, so that <code>C = s1</code>. We define <code>l</code> to be the number of times we need to jump back in <code>s2</code> to pick the next letter.</p>
<p>For example, if we chose <code>c3</code> and <code>c4</code>, but the index of <code>c4</code> in the original <code>s2</code> is smaller than the index of <code>c3</code> in the original <code>s2</code>, we increment <code>l</code> by one.</p>
<p>Our task is, to find the optimum sequence, so that <code>l</code> is minimal. If we are given <code>s1=abac</code> and <code>s2=acab</code> for example, the output has to be 1, since we can pick the second &quot;a&quot; from <code>s2</code>, then choose &quot;b&quot;, then jump back and pick the first &quot;a&quot;, then add &quot;c&quot;.</p>
<p>I don't know how to solve this problem in polynomial time. I thought maybe there is some way to calculate a perfect matching and read of the optimal sequence, but I am not sure. So far, I have only an exponential algorithm.</p>
<p>The exponential algorithm looks as follows (not sure if it is correct, don't know how to test it):</p>
<pre><code>def solvenaive(s1, s2, curr_ind):
    if len(s1) == 0:
        return 0
    first_s1 = s1[0]
    
    vorkommen = findOccurrences(s2, first_s1)
    results = []

    for i in vorkommen:
        new_s1 = s1[1:]
        new_s2 = stringPop(s2, i)
        res = solvenaive(new_s1, new_s2, i)
        
        if curr_ind &gt; i:
            results.append(res+1)
        else:
            results.append(res)
    
    return min(results)
</code></pre>
"
"<p>I have an old project created with poetry. The <strong>pyproject.toml</strong> create by poetry is the following:</p>
<pre><code>[tool.poetry]
name = &quot;Dota2Learning&quot;
version = &quot;0.3.0&quot;
description = &quot;Statistics and Machine Learning for your Dota2 Games.&quot;
license = &quot;MIT&quot;
readme = &quot;README.md&quot;
homepage = &quot;Coming soon...&quot;
repository = &quot;https://github.com/drigols/dota2learning/&quot;
documentation = &quot;Coming soon...&quot;
include = [&quot;CHANGELOG.md&quot;]
authors = [
    &quot;drigols &lt;drigols.creative@gmail.com&gt;&quot;,
]
maintainers = [
    &quot;drigols &lt;drigols.creative@gmail.com&gt;&quot;,
]
keywords = [
    &quot;dota2&quot;,
    &quot;statistics&quot;,
    &quot;machine Learning&quot;,
    &quot;deep learning&quot;,
]

[tool.poetry.scripts]
dota2learning = &quot;dota2learning.cli.main:app&quot;

[tool.poetry.dependencies]
python = &quot;^3.10&quot;
requests = &quot;^2.27.1&quot;
typer = {extras = [&quot;all&quot;], version = &quot;^0.4.1&quot;}
install = &quot;^1.3.5&quot;
SQLAlchemy = &quot;^1.4.39&quot;
PyMySQL = &quot;^1.0.2&quot;
cryptography = &quot;^37.0.4&quot;
pydantic = &quot;^1.9.1&quot;
rich = &quot;^12.5.1&quot;
fastapi = &quot;^0.79.0&quot;
uvicorn = &quot;^0.18.2&quot;

[tool.poetry.dev-dependencies]
black = {extras = [&quot;jupyter&quot;], version = &quot;^22.3.0&quot;}
pre-commit = &quot;^2.19.0&quot;
flake8 = &quot;^4.0.1&quot;
reorder-python-imports = &quot;^3.1.0&quot;
pyupgrade = &quot;^2.34.0&quot;
coverage = &quot;^6.4.1&quot;

[tool.black]
line-length = 79
include = '\.pyi?$' # All Python files
exclude = '''
/(
    \.git
  | \.hg
  | \.mypy_cache
  | \.tox
  | \.venv
  | _build
  | buck-out
  | build
  | dist
)/
'''

[build-system]
requires = [&quot;poetry-core&gt;=1.0.0&quot;]
build-backend = &quot;poetry.core.masonry.api&quot;

[tool.poetry.urls]
&quot;Bug Tracker&quot; = &quot;https://github.com/drigols/dota2learning/issues&quot;
</code></pre>
<p>If a run <strong>&quot;pip install .&quot;</strong> it's worked for me, however, now I want to follow the new approach without poetry to manage the project and dependencies. Then I created a new pyproject.toml (manually):</p>
<pre><code>[project]
name = &quot;Dota2Learning&quot;
version = &quot;2.0.0&quot;
description = &quot;Statistics and Machine Learning for your Dota2 Games.&quot;
license = &quot;MIT&quot;
readme = &quot;README.md&quot;
homepage = &quot;&quot;
requires-python = &quot;&gt;=3.10&quot;
repository = &quot;https://github.com/drigols/dota2learning/&quot;
documentation = &quot;&quot;
include = [&quot;CHANGELOG.md&quot;]
authors = [
    &quot;drigols &lt;drigols.creative@gmail.com&gt;&quot;,
]
maintainers = [
    &quot;drigols &lt;drigols.creative@gmail.com&gt;&quot;,
]
keywords = [
    &quot;dota2&quot;,
    &quot;statistics&quot;,
    &quot;machine Learning&quot;,
    &quot;deep learning&quot;,
]

dependencies = [
    &quot;requests&gt;=2.27.1&quot;,
    &quot;typer&gt;=0.4.1&quot;,
    &quot;SQLAlchemy&gt;=1.4.39&quot;,
    &quot;PyMySQL&gt;=1.0.2&quot;,
    &quot;cryptography&gt;=37.0.4&quot;,
    &quot;pydantic&gt;=1.9.1&quot;,
    &quot;rich&gt;=12.5.1&quot;,
    &quot;fastapi&gt;=0.79.0&quot;,
    &quot;uvicorn&gt;=0.18.2&quot;,
]

[project.optional-dependencies]
# Dev dependencies.
dev = [
    &quot;black&gt;=22.3.0&quot;,
    &quot;pre-commit&gt;=2.19.0&quot;,
    &quot;flake8&gt;=4.0.1&quot;,
    &quot;reorder-python-imports&gt;=3.1.0&quot;,
    &quot;pyupgrade&gt;=2.34.0&quot;,
]
# Testing dependencies.
test = [
    &quot;coverage&gt;=6.4.1&quot;,
]
# Docs dependencies.
doc = []

[project.scripts]
dota2learning = &quot;dota2learning.cli.main:app&quot;

[tool.black]
line-length = 79
include = '\.pyi?$' # All Python files
exclude = '''
/(
    \.git
  | \.hg
  | \.mypy_cache
  | \.tox
  | \.venv
  | _build
  | buck-out
  | build
  | dist
)/
'''
</code></pre>
<p>The problem now is that the pip command <strong>&quot;pip install .&quot;</strong> don't work:</p>
<pre><code>  Installing build dependencies ... done
  Getting requirements to build wheel ... error
  error: subprocess-exited-with-error
  
  × Getting requirements to build wheel did not run successfully.
  │ exit code: 1
  ╰─&gt; [89 lines of output]
      configuration error: `project.license` must be valid exactly by one definition (2 matches found):
      
          - keys:
              'file': {type: string}
            required: ['file']
          - keys:
              'text': {type: string}
            required: ['text']
      
      DESCRIPTION:
          `Project license &lt;https://www.python.org/dev/peps/pep-0621/#license&gt;`_.
      
      GIVEN VALUE:
          &quot;MIT&quot;
      
      OFFENDING RULE: 'oneOf'
      
      DEFINITION:
          {
              &quot;oneOf&quot;: [
                  {
                      &quot;properties&quot;: {
                          &quot;file&quot;: {
                              &quot;type&quot;: &quot;string&quot;,
                              &quot;$$description&quot;: [
                                  &quot;Relative path to the file (UTF-8) which contains the license for the&quot;,
                                  &quot;project.&quot;
                              ]
                          }
                      },
                      &quot;required&quot;: [
                          &quot;file&quot;
                      ]
                  },
                  {
                      &quot;properties&quot;: {
                          &quot;text&quot;: {
                              &quot;type&quot;: &quot;string&quot;,
                              &quot;$$description&quot;: [
                                  &quot;The license of the project whose meaning is that of the&quot;,
                                  &quot;`License field from the core metadata&quot;,
                                  &quot;&lt;https://packaging.python.org/specifications/core-metadata/#license&gt;`_.&quot;
                              ]
                          }
                      },
                      &quot;required&quot;: [
                          &quot;text&quot;
                      ]
                  }
              ]
          }
      Traceback (most recent call last):
        File &quot;/home/drigols/Workspace/dota2learning/environment/lib/python3.10/site-packages/pip/_vendor/pep517/in_process/_in_process.py&quot;, line 351, in &lt;module&gt;
          main()
        File &quot;/home/drigols/Workspace/dota2learning/environment/lib/python3.10/site-packages/pip/_vendor/pep517/in_process/_in_process.py&quot;, line 333, in main
          json_out['return_val'] = hook(**hook_input['kwargs'])
        File &quot;/home/drigols/Workspace/dota2learning/environment/lib/python3.10/site-packages/pip/_vendor/pep517/in_process/_in_process.py&quot;, line 118, in get_requires_for_build_wheel
          return hook(config_settings)
        File &quot;/tmp/pip-build-env-up7_m__d/overlay/lib/python3.10/site-packages/setuptools/build_meta.py&quot;, line 338, in get_requires_for_build_wheel
          return self._get_build_requires(config_settings, requirements=['wheel'])
        File &quot;/tmp/pip-build-env-up7_m__d/overlay/lib/python3.10/site-packages/setuptools/build_meta.py&quot;, line 320, in _get_build_requires
          self.run_setup()
        File &quot;/tmp/pip-build-env-up7_m__d/overlay/lib/python3.10/site-packages/setuptools/build_meta.py&quot;, line 484, in run_setup
          super(_BuildMetaLegacyBackend,
        File &quot;/tmp/pip-build-env-up7_m__d/overlay/lib/python3.10/site-packages/setuptools/build_meta.py&quot;, line 335, in run_setup
          exec(code, locals())
        File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt;
        File &quot;/tmp/pip-build-env-up7_m__d/overlay/lib/python3.10/site-packages/setuptools/__init__.py&quot;, line 87, in setup
          return distutils.core.setup(**attrs)
        File &quot;/tmp/pip-build-env-up7_m__d/overlay/lib/python3.10/site-packages/setuptools/_distutils/core.py&quot;, line 159, in setup
          dist.parse_config_files()
        File &quot;/tmp/pip-build-env-up7_m__d/overlay/lib/python3.10/site-packages/setuptools/dist.py&quot;, line 867, in parse_config_files
          pyprojecttoml.apply_configuration(self, filename, ignore_option_errors)
        File &quot;/tmp/pip-build-env-up7_m__d/overlay/lib/python3.10/site-packages/setuptools/config/pyprojecttoml.py&quot;, line 62, in apply_configuration
          config = read_configuration(filepath, True, ignore_option_errors, dist)
        File &quot;/tmp/pip-build-env-up7_m__d/overlay/lib/python3.10/site-packages/setuptools/config/pyprojecttoml.py&quot;, line 126, in read_configuration
          validate(subset, filepath)
        File &quot;/tmp/pip-build-env-up7_m__d/overlay/lib/python3.10/site-packages/setuptools/config/pyprojecttoml.py&quot;, line 51, in validate
          raise ValueError(f&quot;{error}\n{summary}&quot;) from None
      ValueError: invalid pyproject.toml config: `project.license`.
      configuration error: `project.license` must be valid exactly by one definition (2 matches found):
      
          - keys:
              'file': {type: string}
            required: ['file']
          - keys:
              'text': {type: string}
            required: ['text']
      
      [end of output]
  
  note: This error originates from a subprocess, and is likely not a problem with pip.
error: subprocess-exited-with-error

× Getting requirements to build wheel did not run successfully.
│ exit code: 1
╰─&gt; See above for output.

note: This error originates from a subprocess, and is likely not a problem with pip.
</code></pre>
<p>It's like pip doesn't know how to install the dependencies from pyproject.toml unlike poetry's approach and I don't understand why.</p>
<p><strong>THE PROBLEM WAS SOLVED!</strong>
A edited the pyproject.toml to:</p>
<pre><code>[build-system]
# Minimum requirements for the build system to execute.
requires = [&quot;setuptools&quot;, &quot;wheel&quot;]  # PEP 508 specifications.
build-backend = &quot;setuptools.build_meta&quot;

# Ignore flat-layout.
[tool.setuptools]
py-modules = []

[project]
name = &quot;Dota2Learning&quot;
version = &quot;2.0.0&quot;
description = &quot;Statistics and Machine Learning for your Dota2 Games.&quot;
license = {file = &quot;LICENSE.md&quot;}
readme = &quot;README.md&quot;
requires-python = &quot;&gt;=3.10.0&quot;
authors = [
    { name = &quot;Rodrigo Leite&quot;, email = &quot;drigols.creative@gmail.com&quot; },
]
maintainers = [
    { name = &quot;Rodrigo Leite&quot;, email = &quot;drigols.creative@gmail.com&quot; },
]
keywords = [
    &quot;dota2&quot;,
    &quot;statistics&quot;,
    &quot;machine Learning&quot;,
    &quot;deep learning&quot;,
]

dependencies = [
    &quot;requests&gt;=2.27.1&quot;,
    &quot;typer&gt;=0.4.1&quot;,
    &quot;SQLAlchemy&gt;=1.4.39&quot;,
    &quot;PyMySQL&gt;=1.0.2&quot;,
    &quot;cryptography&gt;=37.0.4&quot;,
    &quot;pydantic&gt;=1.9.1&quot;,
    &quot;rich&gt;=12.5.1&quot;,
    &quot;fastapi&gt;=0.79.0&quot;,
    &quot;uvicorn&gt;=0.18.2&quot;,
]

[project.optional-dependencies]
# Dev dependencies.
dev = [
    &quot;black&gt;=22.3.0&quot;,
    &quot;pre-commit&gt;=2.19.0&quot;,
    &quot;flake8&gt;=4.0.1&quot;,
    &quot;reorder-python-imports&gt;=3.1.0&quot;,
    &quot;pyupgrade&gt;=2.34.0&quot;,
]
# Test dependencies.
test = [
    &quot;coverage&gt;=6.4.1&quot;,
]
# Doc dependencies.
doc = []

[project.scripts]
# dota2learning = &quot;dota2learning.cli.main:app&quot;

[tool.black]
line-length = 79
include = '\.pyi?$' # All Python files
exclude = '''
/(
    \.git
  | \.hg
  | \.mypy_cache
  | \.tox
  | \.venv
  | _build
  | buck-out
  | build
  | dist
)/
'''
</code></pre>
"
"<p>Seeing kind of contradictory results:</p>
<pre class=""lang-py prettyprint-override""><code>class A:
    def __init__(self, a: int):
        pass
</code></pre>
<p>The snippet above passes a <code>mypy</code> test, but the one below doesn't.</p>
<pre class=""lang-py prettyprint-override""><code>class A:
    def __init__(self):
        pass
</code></pre>
<p>Any idea why?</p>
"
"<p>I'm looking at a <code>setup.py</code> with this syntax:</p>
<pre><code>from setuptools import setup

setup(
...
    tests_require=[&quot;h5py&gt;=2.9=mpi*&quot;,
                   &quot;mpi4py&quot;]
)
</code></pre>
<p>I understand the idea of the &quot;&gt;=&quot; where <code>h5py</code> should be at least version 2.9, but I cannot for the life of me understand the <code>=mpi*</code> afterwards. Is it saying the version should somehow match the mpi version, while also being at least 2.9?</p>
<p>I can't find anything that explains specifying python package versions that also explains the use of a single <code>=</code>.</p>
<p>The only other place I've found it used is some obscure blog post that seemed to imply it was sort of like importing the package with an alias, which doesn't make much sense to me; and also the <a href=""https://mpi4py-fft.readthedocs.io/en/latest/installation.html"" rel=""nofollow noreferrer"">mpi4py docs</a> that include a command line snippet <code>conda install -c conda-forge h5py=*=mpi* netcdf4=*=mpi*</code> but don't really explain it.</p>
"
"<p>I'm trying to use the python package <code>aitextgen</code> in google Colab so I can fine-tune GPT.</p>
<p>First, when I installed the last version of this package I had this error when importing it.</p>
<pre><code>Unable to import name '_TPU_AVAILABLE' from 'pytorch_lightning.utilities'
</code></pre>
<p>Though with the help of the solutions given in <a href=""https://stackoverflow.com/questions/74319873/unable-to-import-name-tpu-available-from-pytorch-lightning-utilities"">this question</a> I could pass this error by downgrading my packages like this:</p>
<pre><code>!pip3 install -q aitextgen==0.5.2
!pip3 install -q torchtext==0.10.0
!pip3 install -q torchmetrics==0.6.0
!pip3 install -q pytorch-lightning==1.4.0rc0
</code></pre>
<p>But now I'm facing this error when importing the <code>aitextgen</code> package and colab will crash!</p>
<pre><code>/usr/local/lib/python3.8/dist-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory
  warn(f&quot;Failed to load image Python extension: {e}&quot;)
</code></pre>
<p>Keep in mind that the error is in importing the package and there is not a bug in my code. To be more clear I have this error when I just import <code>aitextgen</code> like this:</p>
<pre><code>import aitextgen
</code></pre>
<p>How can I deal with this error?</p>
"
"<p>When I run this code on google colab.</p>
<p><code>from google.cloud import aiplatform</code></p>
<p>The following error occurred</p>
<p><code>ImportError: cannot import name 'WKBWriter' from 'shapely.geos' (/usr/local/lib/python3.8/dist-packages/shapely/geos.py)</code></p>
<p>Does anyone know how to solve this problem?</p>
<p>I was working fine on 2022/12/16, but today it is not working.</p>
"
"<pre><code>import pandas_datareader

end = &quot;2022-12-15&quot;
start = &quot;2022-12-15&quot;
stock_list = [&quot;TATAELXSI.NS&quot;]

data = pandas_datareader.get_data_yahoo(symbols=stock_list, start=start, end=end)

print(data)
</code></pre>
<p>When I run this code, I get error <code>&quot;TypeError: string indices must be integers&quot;</code>.</p>
<p>Edit : I have updated the code and passed list as symbol parameter but it still shows the same error</p>
<p>Error :</p>
<pre><code>Traceback (most recent call last):
  File &quot;C:\Users\Deepak Shetter\PycharmProjects\100DAYSOFPYTHON\mp3downloader.py&quot;, line 7, in &lt;module&gt;
    data = pandas_datareader.get_data_yahoo(symbols=[TATAELXSI], start=start, end=end)
  File &quot;C:\Users\Deepak Shetter\PycharmProjects\100DAYSOFPYTHON\venv\lib\site-packages\pandas_datareader\data.py&quot;, line 80, in get_data_yahoo
    return YahooDailyReader(*args, **kwargs).read()
  File &quot;C:\Users\Deepak Shetter\PycharmProjects\100DAYSOFPYTHON\venv\lib\site-packages\pandas_datareader\base.py&quot;, line 258, in read
    df = self._dl_mult_symbols(self.symbols)
  File &quot;C:\Users\Deepak Shetter\PycharmProjects\100DAYSOFPYTHON\venv\lib\site-packages\pandas_datareader\base.py&quot;, line 268, in _dl_mult_symbols
    stocks[sym] = self._read_one_data(self.url, self._get_params(sym))
  File &quot;C:\Users\Deepak Shetter\PycharmProjects\100DAYSOFPYTHON\venv\lib\site-packages\pandas_datareader\yahoo\daily.py&quot;, line 153, in _read_one_data
    data = j[&quot;context&quot;][&quot;dispatcher&quot;][&quot;stores&quot;][&quot;HistoricalPriceStore&quot;]
TypeError: string indices must be integers
</code></pre>
"
"<p>I'm getting the error:</p>
<p><code>SyntaxError: invalid non-printable character U+00A0</code></p>
<p>When I'm running the code below:</p>
<pre><code># coding=utf-8
from PIL import Image
 
img = Image.open(&quot;img.png&quot;)
</code></pre>
<p>I have tried to load different images with different formats (png, jpg, jpeg). I have tried using different versions of the Pillow library. I have also tried running it using python 2 and 3.</p>
"
"<p>I recently updated the Python version of my base conda environment from 3.8 to 3.9, using <code>mamba update python=3.9</code>, but I can no longer run IPython, because the sqlite3 package appears to be broken.</p>
<pre><code>python
Python 3.9.15 | packaged by conda-forge | (main, Nov 22 2022, 08:55:37) 
[Clang 14.0.6 ] on darwin
Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.
&gt;&gt;&gt; import sqlite3
Traceback (most recent call last):
  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;
  File &quot;/Users/rosborn/opt/miniconda3/lib/python3.9/sqlite3/__init__.py&quot;, line 57, in &lt;module&gt;
    from sqlite3.dbapi2 import *
  File &quot;/Users/rosborn/opt/miniconda3/lib/python3.9/sqlite3/dbapi2.py&quot;, line 27, in &lt;module&gt;
    from _sqlite3 import *
ImportError: dlopen(/Users/rosborn/opt/miniconda3/lib/python3.9/lib-dynload/_sqlite3.cpython-39-darwin.so, 0x0002): Symbol not found: (_sqlite3_enable_load_extension)
  Referenced from: '/Users/rosborn/opt/miniconda3/lib/python3.9/lib-dynload/_sqlite3.cpython-39-darwin.so'
  Expected in: '/usr/lib/libsqlite3.dylib'
</code></pre>
<p>Since I had another Python 3.9 environment that is still functional, I tried copying over the <code>envs/py39/lib/sqlite3.36.0</code> and <code>envs/py39/lib/python3.9/sqlite3</code> directories, as well as <code>envs/py39/lib/python3.9/lib-dynload/_sqlite3.cpython-39-darwin.so</code> because I assumed the sqlite3 libraries had been incorrectly compiled, but that doesn't fix the problem.</p>
<p>On the Homebrew Github, there was a <a href=""https://github.com/Homebrew/legacy-homebrew/issues/49731"" rel=""noreferrer"">related issue</a>, where someone suggested checking whether the missing symbol was there. It seems to be all present and correct.</p>
<pre><code>$ nm -gj /Users/rosborn/opt/miniconda3/lib/python3.9/lib-dynload/_sqlite3.cpython-39-darwin.so | grep enable_load_extension 
_sqlite3_enable_load_extension
</code></pre>
<p>I don't know how Homebrew installs sqlite3, but the remaining fixes seemed to require checking the system libsqlite, which I don't have administrative access to. In case it's relevant, I am on an Intel Mac, so it's not related to the M1 chip, as some related issues appear to be.</p>
<p>Does the conda distribution attempt to link to the system libsqlite? If so, why does this problem not affect the <code>py39</code> environment?</p>
<p>Any tips will be welcome. If it were not the base environment, I would just delete the one with the problem and start again. I attempted a forced reinstall of sqlite3, but it appeared not to be installable as a separate package.</p>
"
"<p>I am wondering what the fastest way for a mean computation is in numpy. I used the following code to experiment with it:</p>
<pre class=""lang-py prettyprint-override""><code>import time
n = 10000
p = np.array([1] * 1000000)

t1 = time.time()
for x in range(n):
    np.divide(np.sum(p), p.size)
t2 = time.time()

print(t2-t1)
</code></pre>
<p>3.9222593307495117</p>
<pre class=""lang-py prettyprint-override""><code>t3 = time.time()
for x in range(n):
    np.mean(p)
t4 = time.time()

print(t4-t3)
</code></pre>
<p>5.271147012710571</p>
<p>I would assume that np.mean would be faster or at least equivalent in speed, however it looks like the combination of numpy functions is faster than np.mean. Why is the combination of numpy functions faster?</p>
"
"<p>I am using NumPy 1.24.0.</p>
<p>On running this sample code line,</p>
<pre class=""lang-py prettyprint-override""><code>import numpy as np
num = np.float(3)
</code></pre>
<p>I am getting this error:</p>
<pre class=""lang-none prettyprint-override""><code>Traceback (most recent call last):   File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;   File &quot;/home/ubuntu/.local/lib/python3.8/site-packages/numpy/__init__.py&quot;, line 284, in __getattr__
    raise AttributeError(&quot;module {!r} has no attribute &quot; AttributeError: module 'numpy' has no attribute 'float'
</code></pre>
<p>How can I fix it?</p>
"
"<p>I am trying to install TensorFlow in Python. I am getting the following error message, I tried uninstalling NumPy and re-installing NumPy but still getting the same error message. How to resolve this issue?</p>
<pre><code>AttributeError: module 'numpy' has no attribute 'typeDict'
</code></pre>
"
"<p>How can I pass a list to an <strong>IN</strong> statement in a query using psycopg's named arguments?</p>
<p>Example:</p>
<pre class=""lang-py prettyprint-override""><code>cur.execute(&quot;&quot;&quot;
        SELECT name
        FROM users
        WHERE id IN (%(ids)s)
        &quot;&quot;&quot;,
        {&quot;ids&quot;: [1, 2, 3]})
</code></pre>
<p>When I do that, I get the following error message:</p>
<pre><code>psycopg.errors.UndefinedFunction: operator does not exist: integer = smallint[]

HINT:  No operator matches the given name and argument type(s). You might need to add explicit type casts.
</code></pre>
"
"<p>Does it make sense to specify range of allowed Python versions in environment.yml file? I got this idea while reading the <a href=""https://cloud.google.com/python/docs/reference/bigquery/latest"" rel=""nofollow noreferrer"">Google's Biq Query documentation</a></p>
<pre><code>Supported Python Versions
Python &gt;= 3.7, &lt; 3.11
</code></pre>
<p>If this makes sense then what is the right syntax to specify the range in the environment.yml file?</p>
"
"<p>I need to downgrade my version of <code>poetry</code> to version <code>1.2.1</code>.</p>
<p>Currently, it's <code>1.2.2</code>.</p>
<pre><code>&gt;&gt;&gt; poetry --version
Poetry (version 1.2.2)
</code></pre>
<p>I use the following command:</p>
<pre><code>&gt;&gt;&gt; curl -sSL https://install.python-poetry.org | POETRY_VERSION=1.2.1 python3 -
Retrieving Poetry metadata

The latest version (1.2.1) is already installed.
</code></pre>
<p>But I'm told that <code>1.2.1</code> is already installed. Yet the poetry version is still stuck on the original.</p>
<pre><code>&gt;&gt;&gt; poetry --version
Poetry (version 1.2.2)
</code></pre>
<p>The answer given <a href=""https://stackoverflow.com/questions/74153270/how-to-downgrade-poetry"">here</a> doesn't work (<code>poetry self update@1.2.1</code>) =&gt; <code>The command &quot;self&quot; does not exist.</code></p>
<p>What am I doing wrong here?</p>
"
"<p>I'm following along with a Udemy course for Python &amp; Finance, unfortunately hit a wall whilst trying to call stock data from yahoo using <code>pandas_dataReader</code>.</p>
<p>Here's my code copied directly from jupyter notebook</p>
<pre><code>import numpy as np
import pandas as pd

from pandas_datareader import data as wb
PG = wb.DataReader('PG', data_source='yahoo', start='1995-1-1')
</code></pre>
<p>this returns the following - rather lengthy - error message:</p>
<pre><code>---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
~\AppData\Local\Temp\ipykernel_16532\693660725.py in &lt;module&gt;
----&gt; 1 PG = wb.DataReader('PG', data_source='yahoo', start='1995-1-1')

~\anaconda3\lib\site-packages\pandas\util\_decorators.py in wrapper(*args, **kwargs)
    205                 else:
    206                     kwargs[new_arg_name] = new_arg_value
--&gt; 207             return func(*args, **kwargs)
    208 
    209         return cast(F, wrapper)

~\anaconda3\lib\site-packages\pandas_datareader\data.py in DataReader(name, data_source, start, end, retry_count, pause, session, api_key)
    368 
    369     if data_source == &quot;yahoo&quot;:
--&gt; 370         return YahooDailyReader(
    371             symbols=name,
    372             start=start,

~\anaconda3\lib\site-packages\pandas_datareader\base.py in read(self)
    251         # If a single symbol, (e.g., 'GOOG')
    252         if isinstance(self.symbols, (string_types, int)):
--&gt; 253             df = self._read_one_data(self.url, params=self._get_params(self.symbols))
    254         # Or multiple symbols, (e.g., ['GOOG', 'AAPL', 'MSFT'])
    255         elif isinstance(self.symbols, DataFrame):

~\anaconda3\lib\site-packages\pandas_datareader\yahoo\daily.py in _read_one_data(self, url, params)
    151         try:
    152             j = json.loads(re.search(ptrn, resp.text, re.DOTALL).group(1))
--&gt; 153             data = j[&quot;context&quot;][&quot;dispatcher&quot;][&quot;stores&quot;][&quot;HistoricalPriceStore&quot;]
    154         except KeyError:
    155             msg = &quot;No data fetched for symbol {} using {}&quot;

TypeError: string indices must be integers
</code></pre>
"
"<p>I am trying to run an exe file on <a href=""https://github.com/microsoftgraph/msgraph-sdk-python-core"" rel=""noreferrer"">msgraph-sdk-python-core</a> from windows machine.
The exe is created from a simple python file by PyInstaller.</p>
<p>main.py</p>
<pre><code>from azure.identity import ClientSecretCredential
from msgraph.core import GraphClient

def getGraphClient():

    client_secret_credential = ClientSecretCredential(
        tenant_id=&quot;tenant_id&quot;,
        client_id=&quot;client_id&quot;,
        client_secret=&quot;client_id&quot;)

    gRaphClient = GraphClient(credential=client_secret_credential)

    return gRaphClient

print(getGraphClient())
</code></pre>
<pre><code>azure-core==1.26.1
azure-identity==1.12.0
certifi==2022.12.7
cffi==1.15.1
charset-normalizer==2.1.1
cryptography==38.0.4
idna==3.4
msal==1.20.0
msal-extensions==1.0.0
msgraph-core==0.2.2
portalocker==2.6.0
pycparser==2.21
pyinstall==0.1.4
PyJWT==2.6.0
pywin32==305
requests==2.28.1
six==1.16.0
typing_extensions==4.4.0
urllib3==1.26.13
</code></pre>
<p>In the development environment, it works fine, but when I convert it to Exe, I get the following error.</p>
<pre><code>&gt;main.exe
Traceback (most recent call last):
  File &quot;main.py&quot;, line 1, in &lt;module&gt;
ModuleNotFoundError: No module named 'azure'
[19104] Failed to execute script 'main' due to unhandled exception!
</code></pre>
<p>I also tried --hidden-import 'azure' but it doesn't work.
'azure' has already been discontinued, do you know how to do this with Exe?</p>
"
"<p>I'm trying to add a new package using <code>poetry add</code>, but it always comes with this error:</p>
<p><code>HTTPSConnectionPool(host='10.140.240.64', port=443): Max retries exceeded with url: /api/v4/projects/118/packages/pypi/files/47f05b39ebe470235b70724fb049985ea75fad6c1a5007ad3462f3d430da338b/tg_client-0.1.10-py3-none-any.whl (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self signed certificate (_ssl.c:1129)')))</code></p>
<p>Who knows how to skip this verification?</p>
<p><strong>Updated:</strong></p>
<p>I try to add a package from private repository:</p>
<pre><code>[[tool.poetry.source]]
name = &quot;my_package&quot;
url = &quot;https://...&quot;
secondary = true
</code></pre>
<p>Maybe that is why the solution <code>poetry config certificates.my_package.cert false</code> doesn't work.</p>
"
"<p>On Windows, when we try to import a <code>.pyd</code> file, and a DLL that the <code>.pyd</code> depends on cannot be found, we get this traceback:</p>
<pre><code>Traceback (most recent call last):
  ...
ImportError: DLL load failed: The specified module could not be found.
</code></pre>
<p>When this happens, often one has to resort to a graphical tool like <a href=""https://github.com/lucasg/Dependencies"" rel=""nofollow noreferrer"">Dependencies</a> to figure out what is the name of the missing module.</p>
<p>How can I obtain the missing module name <strong>via the command-line</strong>?</p>
<p>Context: often we get this error in CI, and it would be easier to login via SSH to find out the missing module name, rather than having to log via GUI.</p>
"
"<p>I am having trouble understanding the usage of the <code>inputs</code> keyword in the <code>.backward()</code> call.</p>
<p>The Documentation says the following:</p>
<blockquote>
<p><strong>inputs</strong> (sequence of Tensor) – Inputs w.r.t. which the gradient will be accumulated into .grad. All other Tensors will be ignored. If not provided, the gradient is accumulated into all the leaf Tensors that were used to compute the attr::tensors.</p>
</blockquote>
<p>From what I understand this allows us to specify the inputs against which we'll look at gradients.</p>
<p>Isn't the already specified if <code>.backward()</code> is called some tensor like a loss, <code>loss.backward()</code>?
wouldn't the computation graph ensure that gradients are calculated with respect to the relevant parameters.</p>
<p>I haven't found sources that explain this better. I'd appreciate if I could be directed to an explanation.</p>
"
"<p>I have a dataset with type dictionary which I converted to <code>Dataset</code>:</p>
<p>ds = datasets.Dataset.from_dict(bio_dict)</p>
<p>The shape now is:</p>
<pre><code>Dataset({
    features: ['id', 'text', 'ner_tags', 'input_ids', 'attention_mask', 'label'],
    num_rows: 8805
})
</code></pre>
<p>When I use the <code>train_test_split</code> function of <code>Datasets</code> I receive the following error:</p>
<pre><code>train_testvalid = ds.train_test_split(test_size=0.5, shuffle=True, stratify_by_column=&quot;label&quot;)
</code></pre>
<blockquote>
<p>ValueError: Stratifying by column is only supported for ClassLabel
column, and column label is Sequence.</p>
</blockquote>
<p>How can I change the type to ClassLabel so that stratify works?</p>
"
"<h2>TL;DR - This is a PyCharm remote interpreter question.</h2>
<p>Remote libraries are not properly synced, and PyCharm is unable to index properly when using remote interpreter. Everything runs fine.</p>
<p>Following is the entire <s>(currently unsuccessful)</s> debug process</p>
<p><strong>See update section for a narrowing down of the problem</strong></p>
<hr />
<p>I am using a virtual environment created with <code>python -m venv venv</code>, then pointing to it as I always have using ssh interpreter. The exact same happens with conda as well.</p>
<p><a href=""https://i.sstatic.net/CCWEu.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/CCWEu.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.sstatic.net/O18vK.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/O18vK.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.sstatic.net/3oGbx.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/3oGbx.png"" alt=""enter image description here"" /></a></p>
<p>After configuring the interpreter, many of the installed packages are marked red by PyCharm, not giving auto complete, and not knowing these packages.</p>
<p>Here is the requirements.txt file, which is used with <code>pip install -r requirements.txt</code></p>
<pre><code>--index https:&lt;our_internal_pypi_server&gt;
--extra-index-url &lt;some_external_pypi_server&gt;
algo_api&gt;=2.5.0
algo_flows&gt;=2.4.0
DateTime==4.7
fastapi==0.88.0
imagesize==1.4.1
numpy==1.23.1
opencv_python==4.6.0.66
overrides==6.1.0
pydantic==1.9.0
pymongo==4.1.1
pytest==7.1.2
pytorch_lightning==1.6.4
PyYAML==6.0
scikit_learn==1.1.3
setuptools==59.5.0
tinytree==0.2.1
#torch==1.10.2+cu113
#torchvision==0.11.3+cu113
tqdm==4.64.0
uv_build_utils==1.4.0
uv_python_utils&gt;=1.11.1
allegroai
pymongo[srv]
</code></pre>
<p>Here is <code>pip freeze</code></p>
<pre><code>absl-py==1.3.0
aggdraw==1.3.15
aiohttp==3.8.3
aiosignal==1.3.1
albumentations==1.3.0
algo-api==2.5.0
algo-flows==2.4.0
allegroai==3.6.1
altair==4.2.0
amqp==5.1.1
anomalib==0.3.2
antlr4-python3-runtime==4.9.3
anyio==3.6.2
astunparse==1.6.3
async-timeout==4.0.2
attrs==20.3.0
bcrypt==4.0.1
bleach==5.0.1
boto3==1.26.34
botocore==1.29.34
cachetools==5.2.0
certifi==2022.12.7
cffi==1.15.1
charset-normalizer==2.1.1
clearml==1.8.3
click==8.1.3
commonmark==0.9.1
contourpy==1.0.6
cpu-cores==0.1.3
cryptography==38.0.4
cycler==0.11.0
DateTime==4.7
decorator==5.1.1
deepmerge==1.1.0
dnspython==2.2.1
docker-pycreds==0.4.0
docopt==0.6.2
docutils==0.19
dotsi==0.0.3
efficientnet==1.0.0
einops==0.6.0
entrypoints==0.4
fastapi==0.88.0
ffmpy==0.3.0
fire==0.5.0
Flask==2.2.2
flatbuffers==1.12
focal-loss==0.0.7
fonttools==4.38.0
frozenlist==1.3.3
fsspec==2022.11.0
furl==2.1.3
future==0.18.2
gast==0.4.0
gitdb==4.0.10
GitPython==3.1.29
google-auth==2.15.0
google-auth-oauthlib==0.4.6
google-pasta==0.2.0
gradio==3.15.0
grpcio==1.51.1
gunicorn==20.1.0
h11==0.14.0
h5py==3.7.0
httpcore==0.16.3
httpx==0.23.1
humanfriendly==9.2
idna==3.4
image-classifiers==1.0.0
imageio==2.23.0
imagesize==1.4.1
imgaug==0.4.0
importlib-metadata==5.2.0
importlib-resources==5.10.1
imutils==0.5.4
inflection==0.5.1
iniconfig==1.1.1
itsdangerous==2.1.2
jaraco.classes==3.2.3
jeepney==0.8.0
Jinja2==3.1.2
jmespath==1.0.1
joblib==1.2.0
jsonschema==3.2.0
keras==2.9.0
Keras-Applications==1.0.8
Keras-Preprocessing==1.1.2
keyring==23.13.1
kiwisolver==1.4.4
kmeans1d==0.3.1
kornia==0.6.8
libclang==14.0.6
linkify-it-py==1.0.3
luqum==0.11.0
Markdown==3.4.1
markdown-it-py==2.1.0
MarkupSafe==2.1.1
maskrcnn-benchmark==1.1.2+cu113
matplotlib==3.6.2
mdit-py-plugins==0.3.3
mdurl==0.1.2
ml-distillery==1.0.1
more-itertools==9.0.0
multidict==6.0.3
networkx==2.8.8
numpy==1.23.1
oauthlib==3.2.2
omegaconf==2.3.0
opencv-python==4.6.0.66
opencv-python-headless==4.6.0.66
opt-einsum==3.3.0
orderedmultidict==1.0.1
orjson==3.8.3
overrides==6.1.0
packaging==22.0
pandas==1.5.2
paramiko==2.12.0
pathlib==1.0.1
pathlib2==2.3.7.post1
pathtools==0.1.2
pika==1.3.1
Pillow==9.3.0
pkginfo==1.9.2
pluggy==1.0.0
ply==3.11
promise==2.3
protobuf==3.19.6
psd-tools==1.9.23
psutil==5.9.4
py==1.11.0
pyasn1==0.4.8
pyasn1-modules==0.2.8
pyclipper==1.3.0.post4
pycocotools==2.0.6
pycparser==2.21
pycpd==2.0.0
pycryptodome==3.16.0
pydantic==1.9.0
pyDeprecate==0.3.2
pydub==0.25.1
pygit2==1.11.1
Pygments==2.13.0
pyhumps==3.8.0
PyJWT==2.4.0
pymongo==4.1.1
PyNaCl==1.5.0
pyparsing==2.4.7
pyrsistent==0.19.2
pytest==7.1.2
python-dateutil==2.8.2
python-multipart==0.0.5
pytorch-lightning==1.6.4
pytz==2022.7
PyWavelets==1.4.1
PyYAML==6.0
qudida==0.0.4
readme-renderer==37.3
requests==2.28.1
requests-oauthlib==1.3.1
requests-toolbelt==0.10.1
rfc3986==1.5.0
rich==12.6.0
rsa==4.9
s3transfer==0.6.0
scikit-image==0.19.3
scikit-learn==1.1.3
scipy==1.9.3
SecretStorage==3.3.3
segmentation-models==1.0.1
sentry-sdk==1.12.1
setproctitle==1.3.2
shapely==2.0.0
shortuuid==1.0.11
six==1.16.0
sklearn==0.0.post1
smmap==5.0.0
sniffio==1.3.0
starlette==0.22.0
tensorboard==2.9.1
tensorboard-data-server==0.6.1
tensorboard-plugin-wit==1.8.1
tensorflow==2.9.1
tensorflow-estimator==2.9.0
tensorflow-io-gcs-filesystem==0.29.0
termcolor==2.1.1
threadpoolctl==3.1.0
tifffile==2022.10.10
timm==0.5.4
tinytree==0.2.1
tomli==2.0.1
toolz==0.12.0
torch==1.10.2+cu113
torchmetrics==0.9.0
torchtext==0.11.2
torchvision==0.11.3+cu113
tqdm==4.64.0
twine==4.0.2
typing-utils==0.1.0
typing_extensions==4.4.0
uc-micro-py==1.0.1
urllib3==1.26.13
uv-build-utils==1.4.0
uv-envyaml==2.0.1
uv-python-serving==2.0.1
uv-python-utils==1.12.0
uvicorn==0.20.0
uvrabbit==1.4.1
validators==0.20.0
vine==5.0.0
wandb==0.12.17
webencodings==0.5.1
websockets==10.4
Werkzeug==2.2.2
windshield-grid-localisation==1.0.0.dev5
wrapt==1.14.1
yacs==0.1.8
yarl==1.8.2
zipp==3.11.0
zope.interface==5.5.2
</code></pre>
<p>The following minimal test program</p>
<pre><code>import pytest
import uv_python_utils
from importlib_metadata import version as version_query
from pkg_resources import parse_version
import requests


installed_pytest_version = parse_version(version_query('pytest'))
installed_uv_python_utils_version = parse_version(version_query('uv_python_utils'))
installed_importlib_metadata_version = parse_version(version_query('importlib_metadata'))
print(installed_pytest_version)
print(installed_uv_python_utils_version)
print(installed_importlib_metadata_version)
</code></pre>
<p>runs with output</p>
<pre><code>7.1.2
1.12.0
5.2.0
</code></pre>
<p>but in the IDE, it looks like this:</p>
<p><a href=""https://i.sstatic.net/yWHTc.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/yWHTc.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://intellij-support.jetbrains.com/hc/en-us/requests/4567907"" rel=""nofollow noreferrer"">Here</a> is the support ticket for JetBrains (not sure if visible for everyone or not). They were not able to help yet.</p>
<p>They offered, and I have done all of the following which did not help:</p>
<ol>
<li>Delete <code>~/.pycharm_helpers</code> on remote</li>
<li>Go to Help | Find Action... and search for &quot;Registry...&quot;.
In the registry, search for python.use.targets.api and disable it.
Reconfigure your project interpreter.</li>
</ol>
<p>They looked in &quot;the logs&quot; (not sure which log), coming from Help --&gt; &quot;Collect Logs and Diagnostic Data&quot;, and saw the following</p>
<pre><code>at java.desktop/java.awt.EventDispatchThread.run(EventDispatchThread.java:92)
2022-12-15 11:14:42,932 [ 478638] WARN - net.schmizz.sshj.xfer.FileSystemFile - Could not set permissions for C:\Users\noam.s\AppData\Local\JetBrains\PyCharm2022.3\remote_sources\-2115534621\.\site-packages__1.zip to 1a4
2022-12-15 11:14:42,986 [ 478692] WARN - net.schmizz.sshj.xfer.FileSystemFile - Could not set permissions for C:\Users\noam.s\AppData\Local\JetBrains\PyCharm2022.3\remote_sources\-2115534621\.\.state.json to 1a4
2022-12-15 11:14:43,077 [ 478783] WARN - net.schmizz.sshj.xfer.FileSystemFile - Could not set permissions for C:\Users\noam.s\AppData\Local\JetBrains\PyCharm2022.3\remote_sources\-2115534621\.\python3.8.zip to 1a4
</code></pre>
<p>I could not find any permission irregularities though.</p>
<p>I also tried to purge everything from Pycharm from both local and remote, and reinstall, and this persists.</p>
<ol>
<li>Uninstall PyCharm, resinstall an older version that works for a colleague (works on the same remote in the same directory for the colleague, so the problem is local)</li>
<li>Delete .idea</li>
<li>Delete <code>C:\Users\noam.s\AppData\Roaming\JetBrains</code></li>
<li>Obviously I tried invalidate caches &amp; restart.</li>
</ol>
<p><strong><s>The libraries just don't get downloaded to the External Libraries</s> [See update below], as shown in the Project menu, which doesn't agree with <code>pip freeze</code></strong></p>
<p>In the venv case:</p>
<p><a href=""https://i.sstatic.net/jbPPO.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/jbPPO.png"" alt=""enter image description here"" /></a></p>
<p>In the conda case, the downloaded remote libraries don't even agree with the Pycharm interpreter screen!</p>
<p><a href=""https://i.sstatic.net/MoeV6.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/MoeV6.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.sstatic.net/KHKhy.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/KHKhy.png"" alt=""enter image description here"" /></a></p>
<p>This really makes it hard for me to work and I am not able to find any workaround.
Any ideas?</p>
<hr />
<h2>Update - The problem occurs when Pycharm tries to unpack from <code>skeletons.zip</code>.</h2>
<p>I found a workaround to avoid the &quot;reds&quot;:</p>
<ol>
<li>Open the Remote Libraries in explorer</li>
</ol>
<p><a href=""https://i.sstatic.net/9vhF4.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/9vhF4.png"" alt=""enter image description here"" /></a></p>
<ol start=""2"">
<li>Delete that folder.</li>
<li>Manually extract the folder from skeletons.zip</li>
<li>Reindex pycharm</li>
</ol>
<p>This gave the folowing warnings:</p>
<pre><code>! Attempting to correct the invalid file or folder name
! Renaming C:\Users\noam.s\AppData\Local\Temp\Rar$DRa30340.29792\756417188\uvrabbit\aux.py to C:\Users\noam.s\AppData\Local\Temp\Rar$DRa30340.29792\756417188\uvrabbit\_aux.py
</code></pre>
<p><a href=""https://i.sstatic.net/ZLDMk.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/ZLDMk.png"" alt=""enter image description here"" /></a></p>
<p>but allowed me to start working. This is not a valid solution in my opinion though, as it required manual handling, rather then let the IDE do it's one job.</p>
<hr />
<ol>
<li>Why does this happen?</li>
<li>How to fix it?</li>
<li>How to avoid it?</li>
</ol>
"
"<p>I am getting the following error when trying to plot a lineplot with seaborn.</p>
<pre><code>TypeError: ufunc 'isfinite' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''
</code></pre>
<p>Minimal example reproducing error:</p>
<pre><code>import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

dataset = {
    &quot;x&quot;: [1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3],
    &quot;y&quot;: [1.0, 2.3, 4.5, 1.2, 3.4, 5.3, 1.1, 2.4, 3.6, 1.1, 3.3, 5.3],
    &quot;id&quot;: [&quot;a&quot;, &quot;a&quot;, &quot;a&quot;, &quot;b&quot;, &quot;b&quot;, &quot;b&quot;, &quot;a&quot;, &quot;a&quot;, &quot;a&quot;, &quot;b&quot;, &quot;b&quot;, &quot;b&quot;],
    &quot;seed&quot;: [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1],
}

df = pd.DataFrame(data=dataset)
print(df.dtypes)

g = sns.lineplot(data=df, x=&quot;x&quot;, y=&quot;y&quot;, hue=&quot;id&quot;, errorbar=&quot;sd&quot;)
plt.show()
plt.close()
</code></pre>
<p>I have tried checking the datatypes of all inputs and Dataframe columns, and changing &quot;id&quot; to be an integer type (even though that is not my goal) and the error persists.</p>
"
"<p>I have pandas <code>DataFrame</code> A. I am struggling transforming this into my desired format, see <code>DataFrame</code> B. I tried <code>pivot</code> or <code>melt</code> but I am not sure how I could make it conditional (<code>string</code> values to <code>FIELD_STR_VALUE</code>, <code>numeric</code> values to <code>FIELD_NUM_VALUE</code>). I was hoping you could point me the right direction.</p>
<p>A: Input DataFrame</p>
<pre><code>|FIELD_A |FIELD_B |FIELD_C |FIELD_D |
|--------|--------|--------|--------|
|123123  |8       |a       |23423   |
|123124  |7       |c       |6464    |
|123144  |99      |x       |234     |
</code></pre>
<p>B: Desired output DataFrame</p>
<pre><code>|ID |FIELD_A |FIELD_NAME |FIELD_STR_VALUE |FIELD_NUM_VALUE |
|---|--------|-----------|----------------|----------------|
|1  |123123  |B          |                |8               |
|2  |123123  |C          |a               |                |
|3  |123123  |D          |                |23423           |
|4  |123124  |B          |                |7               |
|5  |123124  |C          |c               |                |
|6  |123124  |D          |                |6464            |
|7  |123144  |B          |                |99              |
|8  |123144  |C          |x               |                |
|9  |123144  |D          |                |234             |
</code></pre>
"
"<p>I'm using a conda environment with Python version 3.9.7,
pip 22.3.1, numpy 1.24.0, gluoncv 0.10.5.post0,
mxnet 1.7.0.post2</p>
<p><code>from gluoncv import data, utils</code>  gives the error:</p>
<pre><code>C:\Users\std\anaconda3\envs\myenv\lib\site-packages\mxnet\numpy\utils.py:37: FutureWarning: In the future `np.bool` will be defined as the corresponding NumPy scalar.  (This may have returned Python scalars in past versions
  bool = onp.bool

---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
Cell In[1], line 3
      1 #import cv2
      2 #import os
----&gt; 3 from gluoncv import data, utils #does not work

File ~\anaconda3\envs\myenv\lib\site-packages\gluoncv\__init__.py:16
     14 _found_mxnet = _found_pytorch = False
     15 try:
---&gt; 16     _require_mxnet_version('1.4.0', '2.0.0')
     17     from . import data
     18     from . import model_zoo

File ~\anaconda3\envs\myenv\lib\site-packages\gluoncv\check.py:6, in _require_mxnet_version(mx_version, max_mx_version)
      4 def _require_mxnet_version(mx_version, max_mx_version='2.0.0'):
      5     try:
----&gt; 6         import mxnet as mx
      7         from distutils.version import LooseVersion
      8         if LooseVersion(mx.__version__) &lt; LooseVersion(mx_version) or \
      9             LooseVersion(mx.__version__) &gt;= LooseVersion(max_mx_version):

File ~\anaconda3\envs\myenv\lib\site-packages\mxnet\__init__.py:33
     30 # version info
     31 __version__ = base.__version__
---&gt; 33 from . import contrib
     34 from . import ndarray
     35 from . import ndarray as nd

File ~\anaconda3\envs\myenv\lib\site-packages\mxnet\contrib\__init__.py:30
     27 from . import autograd
     28 from . import tensorboard
---&gt; 30 from . import text
     31 from . import onnx
     32 from . import io

File ~\anaconda3\envs\myenv\lib\site-packages\mxnet\contrib\text\__init__.py:23
     21 from . import utils
     22 from . import vocab
---&gt; 23 from . import embedding

File ~\anaconda3\envs\myenv\lib\site-packages\mxnet\contrib\text\embedding.py:36
     34 from ... import base
     35 from ...util import is_np_array
---&gt; 36 from ... import numpy as _mx_np
     37 from ... import numpy_extension as _mx_npx
     40 def register(embedding_cls):

File ~\anaconda3\envs\myenv\lib\site-packages\mxnet\numpy\__init__.py:23
     21 from . import random
     22 from . import linalg
---&gt; 23 from .multiarray import *  # pylint: disable=wildcard-import
     24 from . import _op
     25 from . import _register

File ~\anaconda3\envs\myenv\lib\site-packages\mxnet\numpy\multiarray.py:47
     45 from ..ndarray.numpy import _internal as _npi
     46 from ..ndarray.ndarray import _storage_type, from_numpy
---&gt; 47 from .utils import _get_np_op
     48 from .fallback import *  # pylint: disable=wildcard-import,unused-wildcard-import
     49 from . import fallback

File ~\anaconda3\envs\myenv\lib\site-packages\mxnet\numpy\utils.py:37
     35 int64 = onp.int64
     36 bool_ = onp.bool_
---&gt; 37 bool = onp.bool
     39 pi = onp.pi
     40 inf = onp.inf

File ~\anaconda3\envs\myenv\lib\site-packages\numpy\__init__.py:284, in __getattr__(attr)
    281     from .testing import Tester
    282     return Tester
--&gt; 284 raise AttributeError(&quot;module {!r} has no attribute &quot;
    285                      &quot;{!r}&quot;.format(__name__, attr))

AttributeError: module 'numpy' has no attribute 'bool'
</code></pre>
"
"<p>I'm creating an application that uses manual calibration to align two images. I'm trying to align them almost pixel perfectly, so I'm not relying on automatic calibration as it did not work the best for this scenario. I'm doing it manually by choosing pixels. However, the result is not what I hoped for, and I do not know where I'm making a mistake. I feel like the computed points should place the image precisely on top of the other one, but for some reason, it does not. What am I doing wrong?</p>
<p>Results of homography:</p>
<pre><code>[[ 7.43200521e-01 -1.79170744e-02 -1.76782990e+02]
 [ 1.00046389e-02  7.84106136e-01 -3.22549155e+01]
 [ 5.10695284e-05 -8.48641135e-05  1.00000000e+00]]
</code></pre>
<p>Manually picked points:
RGB:</p>
<pre><code>[[ 277  708]
 [1108  654]
 [ 632  545]
 [ 922  439]
 [ 874  403]
 [ 398  376]
 [ 409  645]
 [ 445  593]
 [ 693  342]
 [ 739  244]
 [ 505  234]
 [ 408  275]
 [ 915  162]
 [1094  126]
 [ 483  115]
 [ 951  366]
 [ 517  355]]
</code></pre>
<p>Thermal:</p>
<pre><code>[[  8 549]
 [634 491]
 [282 397]
 [496 318]
 [461 289]
 [113 269]
 [122 479]
 [148 438]
 [325 236]
 [360 162]
 [194 156]
 [121 188]
 [484 106]
 [621  67]
 [178  62]
 [515 261]
 [203 253]]
</code></pre>
<pre class=""lang-py prettyprint-override""><code>    def manual_calibration(self, rgb: cv2.UMat, thermal: cv2.UMat) -&gt; Tuple[cv2.UMat, Tuple[int, int, int, int]]:
               
        rgb_gray = cv2.cvtColor(rgb, cv2.COLOR_BGR2GRAY) 
        thermal_gray = cv2.cvtColor(thermal, cv2.COLOR_BGR2GRAY) 
        
        h_rgb, w_rgb = rgb_gray.shape
        h_th, w_th = thermal_gray.shape
        
        thermal_gray = cv2.copyMakeBorder(thermal_gray, 0, h_rgb - h_th, 0, 0, cv2.BORDER_CONSTANT, value=[0, 0, 0])
        
        merged = cv2.hconcat((rgb_gray, thermal_gray))
        
        self.merged = cv2.cvtColor(merged, cv2.COLOR_GRAY2RGB)
        
        def point_validation(ix, iy):
            if ix &gt; w_rgb:
                ix -= w_rgb
            return ix, iy

        self.points_left = np.array([])
        self.points_right = np.array([])
        self.label = True

        def select_point(event, x, y, flags, param):
            if event == cv2.EVENT_LBUTTONDOWN: # captures left button double-click
                ix, iy = x, y
                cv2.circle(img=self.merged, center=(x,y), radius=5, color=(0,255,0),thickness=-1)
                
                ix, iy = point_validation(ix, iy)
                
                pt = np.array([ix, iy])
                
                if self.label:
                    # self.points_left = np.vstack((self.points_left, pt))
                    self.points_left = np.vstack((self.points_left, pt)) if self.points_left.size else pt
                    self.label = False
                else:
                    # self.points_right = np.vstack((self.points_right, pt))
                    self.points_right = np.vstack((self.points_right, pt)) if self.points_right.size else pt
                    self.label = True
                    
                    
                print(ix, iy)
            
    
        cv2.namedWindow('calibration')
        cv2.setMouseCallback('calibration', select_point)
        
        while True:
            cv2.imshow(&quot;calibration&quot;, self.merged)
            if cv2.waitKey(20) &amp; 0xFF == 27:
                break
        cv2.destroyAllWindows()
        
        print(self.points_left)
        print(self.points_right)
        
        ### EDIT NEW POINT VALIDATION
        rgb_gray_check = rgb_gray
        thermal_gray_check = thermal_gray
        
        for point in self.points_left:
            cv2.circle(img=rgb_gray_check, center=point, radius=5, color=(0,255,0),thickness=-1)
            
        for point in self.points_right:
            cv2.circle(img=thermal_gray_check, center=point, radius=5, color=(0,255,0),thickness=-1)
        
        cv2.imshow('rgb', rgb_gray_check)
        cv2.imshow('thermal', thermal_gray_check)
        cv2.waitKey(0)
       
        ### EDIT NEW POINT VALIDATION


        
        
        # Compute homography

        # 0 - a regular method using all the points
        # CV_RANSAC - RANSAC-based robust method
        # CV_LMEDS - Least-Median robust method
        matrix, mask = cv2.findHomography(self.points_left, self.points_right, 0)
        
        print(matrix)
                   
        # matrix[0][3] += (w_th/2)
        # matrix[1][3] += (h_th/2)
        
        warp_src = cv2.warpPerspective(thermal, matrix, (rgb.shape[1], rgb.shape[0]))
                
        alpha = 0.5
        beta = (1.0 - alpha)
        dst_warp_blended = cv2.addWeighted(rgb, alpha, warp_src, beta, 0.0)
        
        cv2.imshow('Blended destination and warped image', dst_warp_blended)
        cv2.waitKey(0)
</code></pre>
<p>Source images:
<a href=""https://i.sstatic.net/Zjg9N.jpg"" rel=""noreferrer""><img src=""https://i.sstatic.net/Zjg9N.jpg"" alt=""rgb"" /></a>
<a href=""https://i.sstatic.net/G987k.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/G987k.png"" alt=""thermal"" /></a></p>
<p>My result:
<a href=""https://i.sstatic.net/xlKii.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/xlKii.png"" alt=""result"" /></a></p>
"
"<p>I am using polars in place of pandas. I am quite amazed by the speed and lazy computation/evaluation. Right now, there are a lot of methods on lazy dataframe, but they can only drive me so far.</p>
<p>So, I am wondering what is the best way to use polars in combination with other tools to achieve more complicated operations, such as regression/model fitting.</p>
<p>To be more specific, I will give an example involving linear regression.</p>
<p>Assume I have a polars dataframe with columns day, y, x1 and x2, and I want to generate a series, which is the residual of regressing y on x1 and x2 group by day. I have included the code example as follows and how it can be solved using pandas and statsmodels. How can I get the same result with the most efficiency using idiomatic polars?</p>
<pre><code>import pandas as pd
import statsmodels.api as sm

def regress_resid(df, yvar, xvars):
    result = sm.OLS(df[yvar], sm.add_constant(df[xvars])).fit()
    return result.resid

df = pd.DataFrame(
    {
        &quot;day&quot;: [1, 1, 1, 1, 1, 2, 2, 2, 2, 2],
        &quot;y&quot;: [1, 6, 3, 2, 8, 4, 5, 2, 7, 3],
        &quot;x1&quot;: [1, 8, 2, 3, 5, 2, 1, 2, 7, 3],
        &quot;x2&quot;: [8, 5, 3, 6, 3, 7, 3, 2, 9, 1],
    }
)

df.groupby(&quot;day&quot;).apply(regress_resid, &quot;y&quot;, [&quot;x1&quot;, &quot;x2&quot;])
# day
# 1    0    0.772431
#      1   -0.689233
#      2   -1.167210
#      3   -0.827896
#      4    1.911909
# 2    5   -0.851691
#      6    1.719451
#      7   -1.167727
#      8    0.354871
#      9   -0.054905
</code></pre>
<p>Thanks for your help.</p>
"
"<p>I have a web app built with a framework like FastAPI or Django, and my project uses Poetry to manage the dependencies.</p>
<p>I didn't find any topic similar to this.</p>
<p><strong>The question is:</strong> should I install poetry in my production dockerfile and install the dependencies using the poetry, or should I export the <code>requirements.txt</code> and just use pip inside my docker image?</p>
<p>Actually, I am exporting the <code>requirements.txt</code> to the project's root before deploy the app and just using it inside the docker image.</p>
<p>My motivation is that I don't need the &quot;complexity&quot; of using poetry inside a dockerfile, since the <code>requirements.txt</code> is already generated by the poetry and use it inside the image will generate a new step into docker build that can impact the build speed.</p>
<p>However, I have seen much dockerfiles with poetry installation, what makes me think that I am doing a bad use of the tool.</p>
"
"<p>I am using the OpenAI API to get embeddings for a bunch of sentences. And by a bunch of sentences, I mean a bunch of sentences, like thousands. Is there a way to make it faster or make it do the embeddings concurrently or something?</p>
<p>I tried Looping through and sending a request for each sentence but that was super slow, but so is sending a list of the sentences. For both cases, I used this code: '''</p>
<pre><code>response = requests.post(
    &quot;https://api.openai.com/v1/embeddings&quot;,
    json={
        &quot;model&quot;: &quot;text-embedding-ada-002&quot;,
        &quot;input&quot;: [&quot;text:This is a test&quot;, &quot;text:This is another test&quot;, &quot;text:This is a third test&quot;, &quot;text:This is a fourth test&quot;, &quot;text:This is a fifth test&quot;, &quot;text:This is a sixth test&quot;, &quot;text:This is a seventh test&quot;, &quot;text:This is a eighth test&quot;, &quot;text:This is a ninth test&quot;, &quot;text:This is a tenth test&quot;, &quot;text:This is a eleventh test&quot;, &quot;text:This is a twelfth test&quot;, &quot;text:This is a thirteenth test&quot;, &quot;text:This is a fourteenth test&quot;, &quot;text:This is a fifteenth test&quot;, &quot;text:This is a sixteenth test&quot;, &quot;text:This is a seventeenth test&quot;, &quot;text:This is a eighteenth test&quot;, &quot;text:This is a nineteenth test&quot;, &quot;text:This is a twentieth test&quot;, &quot;text:This is a twenty first test&quot;, &quot;text:This is a twenty second test&quot;, &quot;text:This is a twenty third test&quot;, &quot;text:This is a twenty fourth test&quot;, &quot;text:This is a twenty fifth test&quot;, &quot;text:This is a twenty sixth test&quot;, &quot;text:This is a twenty seventh test&quot;, &quot;text:This is a twenty eighth test&quot;, &quot;text:This is a twenty ninth test&quot;, &quot;text:This is a thirtieth test&quot;, &quot;text:This is a thirty first test&quot;, &quot;text:This is a thirty second test&quot;, &quot;text:This is a thirty third test&quot;, &quot;text:This is a thirty fourth test&quot;, &quot;text:This is a thirty fifth test&quot;, &quot;text:This is a thirty sixth test&quot;, &quot;text:This is a thirty seventh test&quot;, &quot;text:This is a thirty eighth test&quot;, &quot;text:This is a thirty ninth test&quot;, &quot;text:This is a fourtieth test&quot;, &quot;text:This is a forty first test&quot;, &quot;text:This is a forty second test&quot;, &quot;text:This is a forty third test&quot;, &quot;text:This is a forty fourth test&quot;, &quot;text:This is a forty fifth test&quot;, &quot;text:This is a forty sixth test&quot;, &quot;text:This is a forty seventh test&quot;, &quot;text:This is a forty eighth test&quot;, &quot;text:This is a forty ninth test&quot;, &quot;text:This is a fiftieth test&quot;, &quot;text:This is a fifty first test&quot;, &quot;text:This is a fifty second test&quot;, &quot;text:This is a fifty third test&quot;],
    },
    headers={
        &quot;Authorization&quot;: f&quot;Bearer {key}&quot;
    }
    )
</code></pre>
<p>For the first test, I did a bunch of those requests one by one, and for the second one I sent a list. Should I send individual requests in parallel? Would that help? Thanks!</p>
"
"<p>I'm trying to build a custom field in Fastapi-users pydantic schema as follows:</p>
<pre class=""lang-py prettyprint-override""><code>class UserRead(schemas.BaseUser[uuid.UUID]):
    twitter_account: Optional['TwitterAccount']
</code></pre>
<p>On UserRead validation Pydantic returns</p>
<pre class=""lang-none prettyprint-override""><code>ValidationError: 1 validation error for UserRead
twitter_account
  Field required [type=missing, input_value={}, input_type=dict]
</code></pre>
<p>on every field in <code>'TwitterAccount'</code> <code>schema.update_forward_refs()</code> is called at the end.</p>
<p><code>TwitterAccount</code> itself has required fields and making them optional isn't an acceptable workaround. I notices I could make <code>Optional[List['TwitterAccount']]</code> and it will work, but that's a bit silly.</p>
"
"<p>I want to print out SPY's stock data however it keeps shows typeerror: string indices must be integer</p>
<pre><code>import pandas_datareader.data as web

spy = web.get_data_yahoo('SPY',start='2022-12-23',end='2022-10-24')

print(spy)
</code></pre>
"
"<p>In python, we can make an empty list easily by doing <code>a = []</code>. I want to do a similar thing but with Pytorch tensors.</p>
<p>If you want to know why I need that, I want to get all of the data inside a given dataloader (to create another customer dataloader). Having an empty tensor can help me gather all of the data inside a tensor using a for-loop. This is a sudo code for it.</p>
<pre><code>all_data_tensor = # An empty tensor

for data in dataloader:
  all_data_tensor  = torch.cat((all_data_tensor, data), 0)
</code></pre>
<p>Is there any way to do this?</p>
"
"<p>My understanding of <code>yield from</code> is that it is similar to <code>yield</code>ing every item from an iterable. Yet, I observe the different behavior in the following example.</p>
<p>I have <code>Class1</code></p>
<pre><code>class Class1:
    def __init__(self, gen):
        self.gen = gen
        
    def __iter__(self):
        for el in self.gen:
            yield el
</code></pre>
<p>and Class2 that different only in replacing <code>yield</code> in for loop with <code>yield from</code></p>
<pre><code>class Class2:
    def __init__(self, gen):
        self.gen = gen
        
    def __iter__(self):
        yield from self.gen
</code></pre>
<p>The code below reads the first element from an instance of a given class and then reads the rest in a for loop:</p>
<pre><code>a = Class1((i for i in range(3)))
print(next(iter(a)))
for el in iter(a):
    print(el)
</code></pre>
<p>This produces different outputs for <code>Class1</code> and <code>Class2</code>. For <code>Class1</code> the output is</p>
<pre><code>0
1
2
</code></pre>
<p>and for <code>Class2</code> the output is</p>
<pre><code>0
</code></pre>
<p><a href=""https://godbolt.org/z/sjb54zcTx"" rel=""noreferrer"">Live demo</a></p>
<p>What is the mechanism behind <code>yield from</code> that produces different behavior?</p>
"
"<p>I have a <code>pyproject.toml</code> with</p>
<pre><code>[tool.poetry]
name = &quot;my-project&quot;
version = &quot;0.1.0&quot;

[tool.commitizen]
name = &quot;cz_conventional_commits&quot;
version = &quot;0.1.0&quot;
</code></pre>
<p>I add a new feature and commit with commit message</p>
<pre><code>feat: add parameter for new feature
</code></pre>
<p><strong>That's one commit.</strong></p>
<p>Then I call</p>
<pre><code>commitizen bump
</code></pre>
<p>Commitizen will recognize a minor version increase, update my <code>pyproject.toml</code>, and commit again with the updated <code>pyproject.toml</code> and a tag <code>0.2.0</code>.</p>
<p><strong>That's a second commit.</strong></p>
<p>But now my <code>pyproject.toml</code> is &quot;out of whack&quot; (assuming I want my build version in sync with my git tags).</p>
<pre><code>[tool.poetry]
name = &quot;my-project&quot;
version = &quot;0.1.0&quot;

[tool.commitizen]
name = &quot;cz_conventional_commits&quot;
version = &quot;0.2.0&quot;
</code></pre>
<p>I'm two commits in, one tagged, and things still aren't quite right. Is there workflow to keep everything aligned?</p>
"
"<p>I run a CNN model on CIFAR-10 using PyTorch and use <a href=""https://pytorch.org/tutorials/recipes/recipes/saving_and_loading_a_general_checkpoint.html#:%7E:text=4.-,Save%20the%20general%20checkpoint,-Collect%20all%20relevant"" rel=""noreferrer"">the official PyTorch tutorial</a> to save a general checkpoint.</p>
<p>When the training and testing is completed I pass the last epoch to this <code>save_model</code> function.</p>
<pre><code>def save_model(epoch):
    torch.save({
        'epoch': epoch+1,
        'model_state_dict': net.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        }, '/home/cc/research/AdderNet/pretrained/minionn.pt')
</code></pre>
<p>However, I keep getting the following error while trying to save the model:</p>
<pre><code>&gt; Train - Epoch 1, Batch: 1, Loss: 2.302385
&gt; Test Avg. Loss: 0.020081, Accuracy: 0.269100
&gt; Train - Epoch 2, Batch: 1, Loss: 2.019350
&gt; Test Avg. Loss: 0.018918, Accuracy: 0.324800
&gt; Traceback (most recent call last):
&gt; File &quot;/home/cc/research/AdderNet/main.py&quot;, line 119, in &lt;module&gt;
&gt; main()
&gt; File &quot;/home/cc/research/AdderNet/main.py&quot;, line 115, in main
&gt; save_model(epoch)
&gt; File &quot;/home/cc/research/AdderNet/main.py&quot;, line 105, in save_model
&gt; torch.save({
&gt; File &quot;/home/cc/anaconda3/envs/torch/lib/python3.10/site-packages/torch/serialization.py&quot;, line 422, in save
&gt; with _open_zipfile_writer(f) as opened_zipfile:
&gt; File &quot;/home/cc/anaconda3/envs/torch/lib/python3.10/site-packages/torch/serialization.py&quot;, line 309, in _open_zipfile_writer
&gt; return container(name_or_buffer)
&gt; File &quot;/home/cc/anaconda3/envs/torch/lib/python3.10/site-packages/torch/serialization.py&quot;, line 287, in __init__
&gt; super(_open_zipfile_writer_file, self).__init__(torch._C.PyTorchFileWriter(str(name)))
&gt; **RuntimeError: File /home/cc/research/AdderNet/pretrained/minionn.pt cannot be opened.**
</code></pre>
<p>What do you think is the problem? Please, let me know if any other details need to be added. I am running my code on a remote server using VsCode. I am also using a virtual environment that I created with conda. The python version installed in the venv is <code>Python 3.10.8</code> but the conda's base python version is <code>Python 3.9.13</code> and my system's default python version (when I deactivate conda) is <code>Python 3.8.10</code>. The operating system is also <code>Ubuntu20.04</code>.</p>
<p><strong>Updated:</strong></p>
<p>I am able to save the model using the following:</p>
<pre><code>torch.save(model, '/home/cc/research/AdderNet/pretrained/FILE_NAME')
</code></pre>
<p>But since I want to load and continue training the saved model, PyTorch instructs to use this approach which apparently doesn't work for me:</p>
<pre><code>torch.save({
            'epoch': EPOCH,
            'model_state_dict': net.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'loss': LOSS,
            }, '/home/cc/research/AdderNet/pretrained/FILE_NAME.pt')
</code></pre>
"
"<p>I've  loaded this <a href=""http://data.insideairbnb.com/spain/catalonia/barcelona/2022-09-10/data/calendar.csv.gz"" rel=""nofollow noreferrer"">airbnb dataset</a> into a local mongodb(version 4.4.17) database called <code>sample_db</code> and collection called <code>barcelona_cal</code>. There are <code>6175334</code> records in the collection.</p>
<p>I created a small experiment to compare rust, c++ and python performances. The code for each langugage will simply get a cursor for the collection and iterate over it, increment a counter and note the time taken.</p>
<p>here's the python code:</p>
<pre><code>import pymongo
from bson.raw_bson import RawBSONDocument

db = &quot;sample_db&quot;
collection = &quot;barcelona_cal&quot;


def get_docs(db, collection, document_class=RawBSONDocument):
    url = &quot;mongodb://localhost:27017&quot;
    client = pymongo.MongoClient(url, document_class=document_class)
    coll = client.get_database(db).get_collection(collection)

    count = 0
    for doc in coll.find({}):
        count += 1

    print('Doc count: ', count)


def main():
    get_docs(db, collection, dict)


if __name__ == &quot;__main__&quot;:
    main()

</code></pre>
<p>running this code using <code>time python3 test.py</code> returns the following times:</p>
<pre><code>real    0m32.784s
user    0m28.076s
sys     0m1.888s
</code></pre>
<p>considering most of the time is spent in decoding the bson into python objects, I ran it again with the the default argument <code>document_class=RawBSONDocument</code>. It gave the following times:</p>
<pre><code>real    0m12.132s
user    0m8.113s
sys     0m1.266s
</code></pre>
<p>Now, I repeated the same experiment with c++ with the following code:</p>
<pre><code>#include &lt;iostream&gt;
#include &lt;mongocxx/client.hpp&gt;
#include &lt;mongocxx/uri.hpp&gt;
#include &lt;mongocxx/instance.hpp&gt;
#include &lt;bsoncxx/types.hpp&gt;


void mongo_connection_test(){
    mongocxx::instance instance{};
    mongocxx::client client{mongocxx::uri{}};
    mongocxx::database database = client[&quot;sample_db&quot;];
    mongocxx::collection collection = database[&quot;barcelona_cal&quot;];

    mongocxx::cursor cursor = collection.find({});

    bsoncxx::document::element el;
    uint32_t count = 0;

    for(auto doc: cursor) {
        count += 1;
    }
    std::cout &lt;&lt; &quot;Leads: &quot; &lt;&lt; count &lt;&lt; std::endl;
}

int main(){
    mongo_connection_test();
}
</code></pre>
<p>This resulted in the following times:</p>
<pre><code>real    0m3.347s
user    0m0.388s
sys     0m1.004s
</code></pre>
<p>Now, coming to the rust code. On rustc 1.65, with latest mongodb crate version 2.3.1, this is the equivalent code:</p>
<pre><code>use std::io::Read;
use mongodb::{sync::Client, options::ClientOptions, bson::doc, bson::Document};


fn cursor_iterate()-&gt; mongodb::error::Result&lt;()&gt;{
    // setup
    let mongo_url = &quot;mongodb://localhost:27017&quot;;
    let db_name = &quot;sample_db&quot;;
    let collection_name = &quot;barcelona_cal&quot;;

    let client = Client::with_uri_str(mongo_url)?;
    let database = client.database(db_name);
    let collection = database.collection::&lt;Document&gt;(collection_name);
    let mut cursor = collection.find(None, None)?;

    let mut count = 0;
    for result in cursor {
        count += 1;
    }

    println!(&quot;Doc count:  {}&quot;, count);
    Ok(())
}


fn main() {
    cursor_iterate();
}
</code></pre>
<p>The code was built with <code>cargo build --release</code> and ran with <code>time cargo run --release</code>. I got the following times</p>
<pre><code>real    0m18.727s
user    0m15.541s
sys     0m0.790s
</code></pre>
<p>I've run every version multiple times and got identical results. so all these numbers can be considered representative.</p>
<p><strong>Edit: I've run the rust binary separately like <code>time target/release/bbson</code> and that made very little, if no difference cos I did the build and run steps separately.</strong></p>
<p>So the question is why is the mongodb rust library so much slower than c++? One interesting thing that I noticed, which may explain this partly is that in the system monitor (ubuntu 22.04), the Network part shows that the tranfer rate is no higher than 64MB/s. Meanwhile for c++ it goes up to 600MB/s. I see similar thing when running the python with dict and that explains the slow bson decoding in python, so is the deserializer that slow in rust too?</p>
<p><a href=""https://i.sstatic.net/dKmGT.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/dKmGT.png"" alt=""enter image description here"" /></a></p>
"
"<p>I already know <a href=""https://stackoverflow.com/questions/73070845/how-to-import-julia-packages-into-python"">How to import Julia packages into Python</a>.</p>
<p>However, now I have created my own simple Julia package with the following command:
<code>using Pkg;Pkg.generate(&quot;MyPack&quot;);Pkg.activate(&quot;MyPack&quot;);Pkg.add(&quot;StatsBase&quot;)</code>
where the file <code>MyPack/src/MyPack.jl</code> has the following contents:</p>
<pre><code>module MyPack
using StatsBase

function f1(x, y)
   return 3x + y
end
g(x) = StatsBase.std(x)

export f1

end
</code></pre>
<p>Now I would like to load this Julia package in Python via <code>juliacall</code> and call <code>f1</code> and <code>g</code> functions.
I have already run <code>pip3 install juliacall</code> from command line. How do I call the above functions from Python?</p>
"
"<p>I have been using camelot for our project, but since 2 days I got following errorMessage. When trying to run following code snippet:</p>
<pre><code>import camelot
tables = camelot.read_pdf('C:\\Users\\user\\Downloads\\foo.pdf', pages='1')
</code></pre>
<p>I get this error:</p>
<pre><code>DeprecationError: PdfFileReader is deprecated and was removed in PyPDF2 3.0.0. Use PdfReader instead.
</code></pre>
<p>I checked this file and it does use pdfFileReader: c:\ProgramData\Anaconda3\lib\site-packages\camelot\handlers.py</p>
<p>I thought that I can specify the version of PyPDF2, but it will be installed automatically(because the library is used by camelot) when I install camelot. Do you think there is any solution to specify the version of PyPDF2 manually?</p>
"
"<p>I tried to run my code in another computer, while it successfully compiled in the original environment, this error can outta nowhere:</p>
<pre><code>File &quot;c:\vision_hw\hw_3\cv2IP.py&quot;, line 91, in SECOND_ORDER_LOG
    original = np.zeros((5,5),dtype=np.int)
File &quot;C:\Users\brian2lee\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\numpy\__init__.py&quot;, line 284, in __getattr__
    raise AttributeError(&quot;module {!r} has no attribute &quot; AttributeError: module 'numpy' has no attribute 'int'
</code></pre>
<p>I have tried reinstallin numpy but it did not work. Down below is my code:</p>
<pre><code>def SECOND_ORDER_LOG (self,img):
    original = np.zeros((5,5),dtype=np.int)
    original[2,2] = 1
    kernel = np.array([[ 0,  0, -1,  0,  0],
    [ 0, -1, -2, -1,  0],
    [-1, -2, 16, -2, -1],
    [ 0, -1, -2, -1,  0],
    [ 0,  0, -1,  0,  0]])
    result = original + 1 * kernel
    sharpened = cv2.filter2D(img, -1, result)
    return sharpened
</code></pre>
"
"<p>I am trying to import Top2Vec package for nlp topic modelling. But even after upgrading pip, numpy this error is coming.</p>
<p>I tried</p>
<pre><code>pip install --upgrade pip
</code></pre>
<pre><code>pip install --upgrade numpy
</code></pre>
<p>I was expecting to run</p>
<pre><code>from top2vec import Top2Vec

model = Top2Vec(FAQs, speed='learn', workers=8)
</code></pre>
<p>but it is giving the mentioned error</p>
"
"<p>I am using Python to convert Pandas df to .xlsx (in Plotly-Dash app.). All working well so far but with this warning tho:</p>
<p><strong>&quot;FutureWarning:
save is not part of the public API, usage can give unexpected results and will be removed in a future version&quot;</strong></p>
<p>How should I modify the code below in order to keep its functionality and stability in future? Thanks!</p>
<pre><code> writer = pd.ExcelWriter(&quot;File.xlsx&quot;, engine = &quot;xlsxwriter&quot;)

 workbook  = writer.book

 df.to_excel(writer, sheet_name = 'Sheet', index = False)
  
 writer.save()
</code></pre>
"
"<p>I've got a poetry project. My environment is Conda 22.9.0 on a windows machine with poetry version 1.2.2:</p>
<p>This is my pyproject.toml file:</p>
<pre><code>[tool.poetry]
name = &quot;myproject&quot;
version = &quot;0.1.0&quot;
description = &quot;&quot;

[tool.poetry.dependencies]
# REVIEW DEPENDENCIES
python = &quot;&gt;=3.7,&lt;3.11&quot;
numpy = &quot;*&quot;
tensorflow = &quot;^2.8&quot;

[build-system]
requires = [&quot;poetry&gt;=0.12&quot;]
build-backend = &quot;poetry.masonry.api&quot;

[tool.poetry.scripts]
start = &quot;myproject.main:start&quot;
</code></pre>
<p>The myproject\main.py module contains:</p>
<pre><code>import tensorflow as tf

def start():
    if tf.test.is_gpu_available():
        print(&quot;TensorFlow is using a GPU.&quot;)
    else:
        print(&quot;TensorFlow is NOT using a GPU.&quot;)
</code></pre>
<p>If I do <code>poetry install</code>, it seems to work fine:</p>
<pre><code>Creating virtualenv myproject in D:\Projects\myproject\dev\myproject-series-forecast\.venv
Updating dependencies
Resolving dependencies...

Writing lock file

Package operations: 41 installs, 0 updates, 0 removals

• Installing certifi (2022.12.7)
• Installing charset-normalizer (2.1.1)
• Installing idna (3.4)
• Installing pyasn1 (0.4.8)
• Installing urllib3 (1.26.13)
• Installing cachetools (5.2.0)
• Installing oauthlib (3.2.2)
• Installing rsa (4.9)
• Installing six (1.16.0)
• Installing zipp (3.11.0)
• Installing requests (2.28.1)
• Installing pyasn1-modules (0.2.8)
• Installing google-auth (2.15.0)
• Installing importlib-metadata (5.2.0)
• Installing requests-oauthlib (1.3.1)
• Installing markupsafe (2.1.1)
• Installing absl-py (1.3.0)
• Installing grpcio (1.51.1)
• Installing numpy (1.21.6)
• Installing tensorboard-data-server (0.6.1)
• Installing markdown (3.4.1)
• Installing tensorboard-plugin-wit (1.8.1)
• Installing protobuf (3.19.6)
• Installing werkzeug (2.2.2)
• Installing google-auth-oauthlib (0.4.6)
• Installing astunparse (1.6.3)
• Installing flatbuffers (22.12.6)
• Installing gast (0.4.0)
• Installing google-pasta (0.2.0)
• Installing h5py (3.7.0)
• Installing keras (2.11.0)
• Installing tensorflow-estimator (2.11.0)
• Installing packaging (22.0)
• Installing opt-einsum (3.3.0)
• Installing libclang (14.0.6)
• Installing tensorboard (2.11.0)
• Installing tensorflow-io-gcs-filesystem (0.29.0)
• Installing termcolor (2.1.1)
• Installing typing-extensions (4.4.0)
• Installing wrapt (1.14.1)
• Installing tensorflow (2.11.0)

Installing the current project: myproject (0.1.0)
</code></pre>
<p>But when executing <code>poetry run start</code> I got error in import</p>
<pre><code>Traceback (most recent call last):
File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt;
File &quot;D:\Python\Anaconda3\lib\importlib\__init__.py&quot;, line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1030, in _gcd_import
File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1007, in _find_and_load
File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 986, in _find_and_load_unlocked
File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 680, in _load_unlocked
File &quot;&lt;frozen importlib._bootstrap_external&gt;&quot;, line 850, in exec_module
File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 228, in _call_with_frames_removed
File &quot;D:\Projects\myproject\dev\myproject-series-forecast\myproject\main.py&quot;, line 3, in &lt;module&gt;
    import tensorflow as tf
ModuleNotFoundError: No module named 'tensorflow'
</code></pre>
"
"<p>I am trying to use great expectations.<br />
The function I want to use is <code>expect_compound_columns_to_be_unique</code>.
This is the code (main code - template):</p>
<pre><code>import datetime

import pandas as pd

import great_expectations as ge
import great_expectations.jupyter_ux
from great_expectations.core.batch import BatchRequest
from great_expectations.checkpoint import SimpleCheckpoint
from great_expectations.exceptions import DataContextError

context = ge.data_context.DataContext()

# Note that if you modify this batch request, you may save the new version as a .json file
#  to pass in later via the --batch-request option
batch_request = {'datasource_name': 'impala_okh', 'data_connector_name': 'default_inferred_data_connector_name', 'data_asset_name': 'okh.okh_forecast_prod', 'limit': 1000}


# Feel free to change the name of your suite here. Renaming this will not remove the other one.
expectation_suite_name = &quot;okh_forecast_prod&quot;
try:
    suite = context.get_expectation_suite(expectation_suite_name=expectation_suite_name)
    print(f'Loaded ExpectationSuite &quot;{suite.expectation_suite_name}&quot; containing {len(suite.expectations)} expectations.')
except DataContextError:
    suite = context.create_expectation_suite(expectation_suite_name=expectation_suite_name)
    print(f'Created ExpectationSuite &quot;{suite.expectation_suite_name}&quot;.')


validator = context.get_validator(
    batch_request=BatchRequest(**batch_request),
    expectation_suite_name=expectation_suite_name
)
column_names = [f'&quot;{column_name}&quot;' for column_name in validator.columns()]
print(f&quot;Columns: {', '.join(column_names)}.&quot;)
validator.head(n_rows=5, fetch_all=False)
</code></pre>
<p>Using this code calling</p>
<pre><code>validator.expect_compound_columns_to_be_unique(['column1', 'column2'])
</code></pre>
<p>produces the following error:</p>
<blockquote>
<p>MetricResolutionError: Cannot compile Column object until its 'name' is assigned.</p>
</blockquote>
<p>How can i solve this problem?</p>
"
"<p>The <a href=""https://numpy.org/devdocs/reference/typing.html"" rel=""nofollow noreferrer"">documentation</a> for <code>np.typing.NDArray</code> says that it is &quot;a generic version of <code>np.ndarray[Any, np.dtype[+ScalarType]]</code>&quot;. Where is the generalization in &quot;generic&quot; happening?</p>
<p>And in the <a href=""https://numpy.org/devdocs/reference/generated/numpy.ndarray.__class_getitem__.html"" rel=""nofollow noreferrer"">documentation</a> for <code>numpy.ndarray.__class_getitem__</code> we have this example <code>np.ndarray[Any, np.dtype[Any]]</code> with no explanation as to what the two arguments are.</p>
<p>And why can I do <code>np.ndarray[float]</code>, ie just use one argument? What does that mean?</p>
"
"<p>My issue is that when I execute <code>poetry install</code>, <code>poetry update</code> or <code>poetry lock</code> the process keeps running indefinitely.</p>
<p>I tried using the <code>-vvv</code> flag to get output of what's happening and it looks like it gets stuck forever in the first install.</p>
<p>My connection is good and all packages that I tried installing exist.</p>
<p>I use version 1.2.1 but I cannot upgrade to newer versions because the format of the <code>.lock</code> file is different and our pipeline fails.</p>
"
"<p>AttributeError: module 'cv2.aruco' has no attribute 'Dictionary_get'</p>
<p>even after installing</p>
<ul>
<li>opencv-python</li>
<li>opencv-contrib-python</li>
</ul>
<pre><code>import numpy as np
import cv2, PIL
from cv2 import aruco
import matplotlib.pyplot as plt
import matplotlib as mpl
import pandas as pd

vid = cv2.VideoCapture(0)

while (True):

    ret, frame = vid.read()
    #cv2.imshow('frame', frame)

    if cv2.waitKey(1) &amp; 0xFF == ord('q'):
        break
    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
    aruco_dict = aruco.Dictionary_get(aruco.DICT_6X6_250)
    parameters =  aruco.DetectorParameters()
    corners, ids, rejectedImgPoints = aruco.detectMarkers(gray, aruco_dict, parameters=parameters)
    frame_markers = aruco.drawDetectedMarkers(frame.copy(), corners, ids)

    plt.figure()
    plt.imshow(frame_markers)
    for i in range(len(ids)):
        c = corners[i][0]
        plt.plot([c[:, 0].mean()], [c[:, 1].mean()], &quot;o&quot;, label = &quot;id={0}&quot;.format(ids[i]))
    plt.legend()
    plt.show()
vid.release()
# Destroy all the windows
cv2.destroyAllWindows()

</code></pre>
<p>normal example for finding and marking aruco</p>
"
"<p>I have some log data like:</p>
<pre><code>logs = [
 {'id': '1234', 'error': None, 'fruit': 'orange'},
 {'id': '12345', 'error': None, 'fruit': 'apple'}
]
</code></pre>
<p>Each dict has the same keys: <code>'id'</code>, <code>'error'</code> and <code>'fruit'</code> (in this example).</p>
<p>I want to <a href=""https://stackoverflow.com/questions/7961363"">remove duplicates</a> from this list, but straightforward <code>dict</code> and <code>set</code> based approaches do not work because my elements are themselves <code>dict</code>s, which are <a href=""https://stackoverflow.com/questions/1151658"">not hashable</a>:</p>
<pre><code>&gt;&gt;&gt; set(logs)
Traceback (most recent call last):
  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;
TypeError: unhashable type: 'dict'
</code></pre>
<p>Another approach is to <a href=""https://stackoverflow.com/questions/2213923"">sort and use itertools.groupby</a> - but dicts are also not comparable, so this also does not work:</p>
<pre><code>&gt;&gt;&gt; from itertools import groupby
&gt;&gt;&gt; [k for k, _ in groupby(sorted(logs))]
Traceback (most recent call last):
  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;
TypeError: '&lt;' not supported between instances of 'dict' and 'dict'
</code></pre>
<p>I had the idea to calculate a hash value for each log entry, and store it in a <code>set</code> for comparison, like so:</p>
<pre><code>def compute_hash(log_dict: dict):
    return hash(log_dict.values())

def deduplicate(logs):
    already_seen = set()
    for log in logs:
        log_hash = compute_hash(log)
        if log_hash in already_seen:
            continue
        already_seen.add(log_hash)
        yield log
</code></pre>
<p>However, I found that <code>compute_hash</code> would give the same hash for different dictionaries, even ones with completely bogus contents:</p>
<pre><code>&gt;&gt;&gt; logs = [{'id': '123', 'error': None, 'fruit': 'orange'}, {}]
&gt;&gt;&gt; # The empty dict will be removed; every dict seems to get the same hash.
&gt;&gt;&gt; list(deduplicate(logs))
[{'id': '123', 'error': None, 'fruit': 'orange'}]
</code></pre>
<p>After some experimentation, I was seemingly able to fix the problem by modifying <code>compute_hash</code> like so:</p>
<pre><code>def compute_hash(log_dict: dict):
    return hash(frozenset(log_dict.values()))
</code></pre>
<p>However, I cannot understand why this makes a difference. <strong>Why</strong> did the original version seem to give the same hash for every input dict? Why does converting the <code>.values</code> result to a <code>frozenset</code> first fix the problem?
Aside from that: <strong>is this algorithm correct</strong>? Or is there some counterexample where the wrong values will be removed?</p>
<hr />
<p><sub>This question discusses how hashing works in Python, in depth, as well as considering other data structures that might be more appropriate than dictionaries for the list elements. See <a href=""https://stackoverflow.com/questions/11092511"">List of unique dictionaries</a> instead if you simply want to remove duplicates from a list of dictionaries.</sub></p>
"
"<p>I'm building a Streamlit multipage application and am having trouble keeping session state when switching between pages. My main page is called mainpage.py and has something like the following:</p>
<pre><code>import streamlit as st

if &quot;multi_select&quot; not in st.session_state:
    st.session_state[&quot;multi_select&quot;] = [&quot;abc&quot;, &quot;xyz&quot;]
if &quot;select_slider&quot; not in st.session_state:
    st.session_state[&quot;select_slider&quot;] = (&quot;1&quot;, &quot;10&quot;)
if &quot;text_inp&quot; not in st.session_state:
    st.session_state[&quot;text_inp&quot;] = &quot;&quot;

st.sidebar.multiselect(
    &quot;multiselect&quot;,
    [&quot;abc&quot;, &quot;xyz&quot;],
    key=&quot;multi_select&quot;,
    default=st.session_state[&quot;multi_select&quot;],
)

st.sidebar.select_slider(
    &quot;number range&quot;,
    options=[str(n) for n in range(1, 11)],
    key=&quot;select_slider&quot;,
    value=st.session_state[&quot;select_slider&quot;],
)
st.sidebar.text_input(&quot;Text:&quot;, key=&quot;text_inp&quot;)

for v in st.session_state:
    st.write(v, st.session_state[v])
</code></pre>
<p>Next, I have another page called 'anotherpage.py' in a subdirectory called 'pages' with this content:</p>
<pre><code>import streamlit as st

for v in st.session_state:
    st.write(v, st.session_state[v])
</code></pre>
<p>If I run this app, change the values of the controls and switch to the other page, I see the values for the control being retained and printed. However, if I switch back to the main page, everything gets reset to the original values. For some reason <code>st.session_state</code> is cleared.</p>
<p>Anyone have any idea how to keep the values in the session state? I'm using Python <code>3.11.1</code> and Streamlit <code>1.16.0</code></p>
"
"<p>I am trying to migrate my package from <code>setup.py</code> to <code>pyproject.toml</code> and I am not sure how to do the dynamic versioning in the same way as before. Currently I can pass the development version using environment variables when the build is for development.</p>
<p>The <code>setup.py</code> file looks similar to this:</p>
<pre><code>import os

from setuptools import setup

import my_package


if __name__ == &quot;__main__&quot;:
    dev_version = os.environ.get(&quot;DEV_VERSION&quot;)
    version = dev_version if dev_version else f&quot;{my_package.__version__}&quot;
    
    setup(
        name=&quot;my_package&quot;,
        version=version,
        ...
    )
</code></pre>
<p>Is there a way to do similar thing when using <code>pyproject.toml</code> file?</p>
"
"<p>The current version of <code>tflite-runtime</code> is <code>2.11.0</code>:</p>
<p><a href=""https://pypi.org/project/tflite-runtime/"" rel=""noreferrer"">https://pypi.org/project/tflite-runtime/</a></p>
<p>Here is a testing for downloading the <code>tflite-runtime</code> to the <code>tmp</code> folder:</p>
<pre><code>mkdir -p /tmp/test
cd /tmp/test

echo &quot;tflite-runtime == 2.11.0&quot; &gt; ./test.txt

pip3 download -r ./test.txt
</code></pre>
<p><strong>Here is the error:</strong></p>
<pre><code>ERROR: Could not find a version that satisfies the requirement tflite-runtime==2.11.0 (from versions: none)
ERROR: No matching distribution found for tflite-runtime==2.11.0
</code></pre>
<p><strong>Here is the pip3 version:</strong></p>
<pre><code># pip3 --version
pip 22.0.2 from /usr/lib/python3/dist-packages/pip (python 3.10)
</code></pre>
<p>What's wrong in the above <code>pip3 download</code>? Why can't it find the latest version? And how to fix?</p>
"
"<p>Whenever I have any Python code executed via Python v3.10.4 with or without debugging in Visual Studio Code v1.74.2, I get output looking like the following in the Debug Console window in addition to the normal output of the code. Otherwise, all of my Python programs work correctly and as intended at this time.</p>
<pre><code>1   HIToolbox                           0x00007ff81116c0c2 _ZN15MenuBarInstance22RemoveAutoShowObserverEv + 30
2   HIToolbox                           0x00007ff8111837e3 SetMenuBarObscured + 115
3   HIToolbox                           0x00007ff81118a29e _ZN13HIApplication11FrontUILostEv + 34
4   HIToolbox                           0x00007ff811183622 _ZN13HIApplication15HandleActivatedEP14OpaqueEventRefhP15OpaqueWindowPtrh + 508
5   HIToolbox                           0x00007ff81117d950 _ZN13HIApplication13EventObserverEjP14OpaqueEventRefPv + 182
6   HIToolbox                           0x00007ff811145bd2 _NotifyEventLoopObservers + 153
7   HIToolbox                           0x00007ff81117d3e6 AcquireEventFromQueue + 494
8   HIToolbox                           0x00007ff81116c5a4 ReceiveNextEventCommon + 725
9   HIToolbox                           0x00007ff81116c2b3 _BlockUntilNextEventMatchingListInModeWithFilter + 70
10  AppKit                              0x00007ff80a973f33 _DPSNextEvent + 909
11  AppKit                              0x00007ff80a972db4 -[NSApplication(NSEvent) _nextEventMatchingEventMask:untilDate:inMode:dequeue:] + 1219
12  AppKit                              0x00007ff80a9653f7 -[NSApplication run] + 586
13  _macosx.cpython-310-darwin.so       0x0000000110407e22 show + 162
14  Python                              0x0000000100bb7595 cfunction_vectorcall_NOARGS + 101
15  Python                              0x0000000100c9101f call_function + 175
16  Python                              0x0000000100c8a2c4 _PyEval_EvalFrameDefault + 34676
17  Python                              0x0000000100c801df _PyEval_Vector + 383
18  Python                              0x0000000100c9101f call_function + 175
19  Python                              0x0000000100c8a2c4 _PyEval_EvalFrameDefault + 34676
20  Python                              0x0000000100c801df _PyEval_Vector + 383
21  Python                              0x0000000100b53f61 method_vectorcall + 481
22  Python                              0x0000000100c8a4f2 _PyEval_EvalFrameDefault + 35234
23  Python                              0x0000000100c801df _PyEval_Vector + 383
24  Python                              0x0000000100c9101f call_function + 175
25  Python                              0x0000000100c8a2c4 _PyEval_EvalFrameDefault + 34676
26  Python                              0x0000000100c801df _PyEval_Vector + 383
27  Python                              0x0000000100cf536d pyrun_file + 333
28  Python                              0x0000000100cf4b2d _PyRun_SimpleFileObject + 365
29  Python                              0x0000000100cf417f _PyRun_AnyFileObject + 143
30  Python                              0x0000000100d20047 pymain_run_file_obj + 199
31  Python                              0x0000000100d1f815 pymain_run_file + 85
32  Python                              0x0000000100d1ef9e pymain_run_python + 334
33  Python                              0x0000000100d1ee07 Py_RunMain + 23
34  Python                              0x0000000100d201e2 pymain_main + 50
35  Python                              0x0000000100d2048a Py_BytesMain + 42
36  dyld                                0x00007ff80741b310 start + 2432
</code></pre>
<ol>
<li>Why do these lines come out in the Debug Console window though there is nothing in any of my Python programs that would directly cause them to come out as far as I know?</li>
<li>How are they helpful and how can they be used if needed?</li>
<li>How can I prevent them from coming out by default?</li>
</ol>
<p>I checked out <a href=""https://code.visualstudio.com/docs/python/debugging"" rel=""nofollow noreferrer"">the Visual Studio Code documentation on Python debugging</a> but could not come across anything that would explain these lines. I am running Visual Studio Code on macOS Ventura v13.1.</p>
<hr />
<p><strong>UPDATE as of January 2, 2023</strong></p>
<p>I have figured out that the unwanted output in my initial post is being triggered by the matplotlib.pyplot.show function in my Python programs. I get that output even when I run a program as simple as that below:</p>
<pre><code>import matplotlib.pyplot as plt

x = [1, 2, 3]
y = [1, 2, 3]
plt.plot(x, y)

plt.show()
</code></pre>
<p>When I remove plt.show() from the code above, the 36-line unwanted output does not come out but then the graph is also not displayed. Again, other than the unwanted output, all of my Python programs with the show function appear to work correctly, including the graph display triggered by the show function. I have Matplotlib 3.5.2 installed on my Mac.</p>
<p>A very similar unwanted output comes out if I run the same program directly through the command line as (assume the Python program's name is <code>test.py</code>):</p>
<pre><code>python3 test.py
</code></pre>
<p>but not when I run <code>test.py</code> through IDLE, Python’s Integrated Development and Learning Environment, or the code in it from within a Jupyter notebook.</p>
<p>I can remove the show function from my Python programs to avoid the unwanted output but then the charts will not appear and I would prefer using the show function rather than a makeshift solution.</p>
<hr />
<p><strong>UPDATE as of January 4, 2023</strong></p>
<p>I was <a href=""https://discourse.matplotlib.org/t/show-function-triggering-unwanted-output/23442"" rel=""nofollow noreferrer"">suggested at a Matplotlib forum</a> that this might somehow be a macOS Ventura v13.1 issue because similar issues have started being reported recently with different programs executed under macOS Ventura v13.1. One user has reported encountering a similar output <a href=""https://www.reddit.com/r/learnpython/comments/zyukpk/weird_output_in_terminal_while_using_tkinter/"" rel=""nofollow noreferrer"">with code using Tkinter</a> and another <a href=""https://github.com/mpv-player/mpv/issues/11018"" rel=""nofollow noreferrer"">while using a video player called mpv</a>.</p>
<p>It is not implausible that the problem  is also related with macOS Ventura v13.1 but I don’t know how and my questions remain.</p>
<hr />
<p><strong>UPDATE as of January 6, 2023</strong></p>
<p>Upgraded Matplotlib to v3.6.2 but the unwanted output issue has not been resolved.</p>
<hr />
<p><strong>UPDATE as of January 8, 2023</strong></p>
<p>Tried Matplotlib v3.6.2 along with Python v3.11.1. The unwanted output issue remains.</p>
<hr />
<p><strong>UPDATE as of January 15, 2023</strong></p>
<p>Reported this issue as a bug to Matplotlib Developers on GitHub: &quot;<a href=""https://github.com/matplotlib/matplotlib/issues/24997"" rel=""nofollow noreferrer"">[Bug]: Show function triggering unwanted additional output #24997
</a>&quot;</p>
<hr />
<p><strong>UPDATE as of January 16, 2023</strong></p>
<p>I have found out that the unwanted output comes out only when the &quot;Automatically hide and show the menu bar&quot; option under Systems Settings-&gt;Desktop&amp;Dock-&gt;Menu Bar is set to <code>Always</code> (which is my setting) or <code>on Desktop Only</code>. The unwanted output does not come out if I set that option to <code>In Full Screen Only</code> or <code>Never</code>.</p>
<hr />
<p><strong>UPDATE as of January 18, 2023</strong></p>
<p>Both Matplotlib and Python developers on GitHub think that the unwanted output, which they can reproduce, is the result of a bug in macOS Ventura 13.1 and therefore there is not anything they can do about it.</p>
<p>For details, see the respective discussions following the bug report I mentioned submitting for Matplotlib on GitHub and also the one I later submitted for Tkinter through Python/CPython on GitHub again as &quot;<a href=""https://github.com/python/cpython/issues/101067"" rel=""nofollow noreferrer"">Tkinter causing unwanted output in most recent macOS</a>&quot;. I was also told in response to the latter that a Feedback Assistant report had now been submitted to Apple about the identified bug.</p>
<hr />
<p><strong>UPDATE as of January 25, 2023</strong></p>
<p>Upgraded the macOS on my Mac to Ventura 13.2 today (and further to Ventura 13.2.1 when it came out in mid-February). No change except that the unwanted output, when the small program is run, is now considerably longer (85 lines). As before, the program works fine otherwise and the unwanted output does not come out if I change my Mac's menu bar setting, for example, to <code>Never</code>.</p>
"
"<p>According to <a href=""https://docs.python.org/3/reference/datamodel.html#object.__del__"" rel=""noreferrer"">Python documentation</a>:</p>
<blockquote>
<p>It is not guaranteed that <code>__del__()</code> methods are called for objects that still exist when the interpreter exits.</p>
</blockquote>
<p>I know that in older versions of Python cyclic referencing would be one of the examples for this behaviour, however as I understand it, in Python 3 such cycles will successfully be destroyed upon interpreter exit.</p>
<p>I'm wondering what are the cases (as close to exhaustive list as possible) when the interpreter would not destroy an object upon exit.</p>
"
"<p>I'm having an error when installing/updating any pip module in python3. Purging and reinstalling <code>pip</code> and every package I can thing of hasn't helped. Here's the error that I get in response to running <code>python -m pip install --upgrade pip</code> specifically (but the error is the same for attempting to install or update any pip module):</p>
<pre><code>Traceback (most recent call last):
  File &quot;/usr/lib/python3.8/runpy.py&quot;, line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File &quot;/usr/lib/python3.8/runpy.py&quot;, line 87, in _run_code
    exec(code, run_globals)
  File &quot;/usr/lib/python3/dist-packages/pip/__main__.py&quot;, line 16, in &lt;module&gt;
    from pip._internal.cli.main import main as _main  # isort:skip # noqa
  File &quot;/usr/lib/python3/dist-packages/pip/_internal/cli/main.py&quot;, line 10, in &lt;module&gt;
    from pip._internal.cli.autocompletion import autocomplete
  File &quot;/usr/lib/python3/dist-packages/pip/_internal/cli/autocompletion.py&quot;, line 9, in &lt;module&gt;
    from pip._internal.cli.main_parser import create_main_parser
  File &quot;/usr/lib/python3/dist-packages/pip/_internal/cli/main_parser.py&quot;, line 7, in &lt;module&gt;
    from pip._internal.cli import cmdoptions
  File &quot;/usr/lib/python3/dist-packages/pip/_internal/cli/cmdoptions.py&quot;, line 24, in &lt;module&gt;
    from pip._internal.exceptions import CommandError
  File &quot;/usr/lib/python3/dist-packages/pip/_internal/exceptions.py&quot;, line 10, in &lt;module&gt;
    from pip._vendor.six import iteritems
  File &quot;/usr/lib/python3/dist-packages/pip/_vendor/__init__.py&quot;, line 65, in &lt;module&gt;
    vendored(&quot;cachecontrol&quot;)
  File &quot;/usr/lib/python3/dist-packages/pip/_vendor/__init__.py&quot;, line 36, in vendored
    __import__(modulename, globals(), locals(), level=0)
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 991, in _find_and_load
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 975, in _find_and_load_unlocked
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 655, in _load_unlocked
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 618, in _load_backward_compatible
  File &quot;&lt;frozen zipimport&gt;&quot;, line 259, in load_module
  File &quot;/usr/share/python-wheels/CacheControl-0.12.6-py2.py3-none-any.whl/cachecontrol/__init__.py&quot;, line 9, in &lt;module&gt;
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 991, in _find_and_load
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 975, in _find_and_load_unlocked
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 655, in _load_unlocked
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 618, in _load_backward_compatible
  File &quot;&lt;frozen zipimport&gt;&quot;, line 259, in load_module
  File &quot;/usr/share/python-wheels/CacheControl-0.12.6-py2.py3-none-any.whl/cachecontrol/wrapper.py&quot;, line 1, in &lt;module&gt;
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 991, in _find_and_load
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 975, in _find_and_load_unlocked
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 655, in _load_unlocked
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 618, in _load_backward_compatible
  File &quot;&lt;frozen zipimport&gt;&quot;, line 259, in load_module
  File &quot;/usr/share/python-wheels/CacheControl-0.12.6-py2.py3-none-any.whl/cachecontrol/adapter.py&quot;, line 5, in &lt;module&gt;
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 991, in _find_and_load
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 975, in _find_and_load_unlocked
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 655, in _load_unlocked
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 618, in _load_backward_compatible
  File &quot;&lt;frozen zipimport&gt;&quot;, line 259, in load_module
  File &quot;/usr/share/python-wheels/requests-2.22.0-py2.py3-none-any.whl/requests/__init__.py&quot;, line 95, in &lt;module&gt;
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 991, in _find_and_load
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 975, in _find_and_load_unlocked
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 655, in _load_unlocked
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 618, in _load_backward_compatible
  File &quot;&lt;frozen zipimport&gt;&quot;, line 259, in load_module
  File &quot;/usr/share/python-wheels/urllib3-1.25.8-py2.py3-none-any.whl/urllib3/contrib/pyopenssl.py&quot;, line 46, in &lt;module&gt;
  File &quot;/home/patrick/.local/lib/python3.8/site-packages/OpenSSL/__init__.py&quot;, line 8, in &lt;module&gt;
    from OpenSSL import crypto, SSL
  File &quot;/home/patrick/.local/lib/python3.8/site-packages/OpenSSL/crypto.py&quot;, line 3268, in &lt;module&gt;
    _lib.OpenSSL_add_all_algorithms()
AttributeError: module 'lib' has no attribute 'OpenSSL_add_all_algorithms'
Error in sys.excepthook:
Traceback (most recent call last):
  File &quot;/usr/lib/python3/dist-packages/apport_python_hook.py&quot;, line 72, in apport_excepthook
    from apport.fileutils import likely_packaged, get_recent_crashes
  File &quot;/usr/lib/python3/dist-packages/apport/__init__.py&quot;, line 5, in &lt;module&gt;
    from apport.report import Report
  File &quot;/usr/lib/python3/dist-packages/apport/report.py&quot;, line 32, in &lt;module&gt;
    import apport.fileutils
  File &quot;/usr/lib/python3/dist-packages/apport/fileutils.py&quot;, line 12, in &lt;module&gt;
    import os, glob, subprocess, os.path, time, pwd, sys, requests_unixsocket
  File &quot;/usr/lib/python3/dist-packages/requests_unixsocket/__init__.py&quot;, line 1, in &lt;module&gt;
    import requests
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 991, in _find_and_load
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 975, in _find_and_load_unlocked
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 655, in _load_unlocked
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 618, in _load_backward_compatible
  File &quot;&lt;frozen zipimport&gt;&quot;, line 259, in load_module
  File &quot;/usr/share/python-wheels/requests-2.22.0-py2.py3-none-any.whl/requests/__init__.py&quot;, line 95, in &lt;module&gt;
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 991, in _find_and_load
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 975, in _find_and_load_unlocked
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 655, in _load_unlocked
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 618, in _load_backward_compatible
  File &quot;&lt;frozen zipimport&gt;&quot;, line 259, in load_module
  File &quot;/usr/share/python-wheels/urllib3-1.25.8-py2.py3-none-any.whl/urllib3/contrib/pyopenssl.py&quot;, line 46, in &lt;module&gt;
  File &quot;/home/patrick/.local/lib/python3.8/site-packages/OpenSSL/__init__.py&quot;, line 8, in &lt;module&gt;
    from OpenSSL import crypto, SSL
  File &quot;/home/patrick/.local/lib/python3.8/site-packages/OpenSSL/crypto.py&quot;, line 3268, in &lt;module&gt;
    _lib.OpenSSL_add_all_algorithms()
AttributeError: module 'lib' has no attribute 'OpenSSL_add_all_algorithms'

Original exception was:
Traceback (most recent call last):
  File &quot;/usr/lib/python3.8/runpy.py&quot;, line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File &quot;/usr/lib/python3.8/runpy.py&quot;, line 87, in _run_code
    exec(code, run_globals)
  File &quot;/usr/lib/python3/dist-packages/pip/__main__.py&quot;, line 16, in &lt;module&gt;
    from pip._internal.cli.main import main as _main  # isort:skip # noqa
  File &quot;/usr/lib/python3/dist-packages/pip/_internal/cli/main.py&quot;, line 10, in &lt;module&gt;
    from pip._internal.cli.autocompletion import autocomplete
  File &quot;/usr/lib/python3/dist-packages/pip/_internal/cli/autocompletion.py&quot;, line 9, in &lt;module&gt;
    from pip._internal.cli.main_parser import create_main_parser
  File &quot;/usr/lib/python3/dist-packages/pip/_internal/cli/main_parser.py&quot;, line 7, in &lt;module&gt;
    from pip._internal.cli import cmdoptions
  File &quot;/usr/lib/python3/dist-packages/pip/_internal/cli/cmdoptions.py&quot;, line 24, in &lt;module&gt;
    from pip._internal.exceptions import CommandError
  File &quot;/usr/lib/python3/dist-packages/pip/_internal/exceptions.py&quot;, line 10, in &lt;module&gt;
    from pip._vendor.six import iteritems
  File &quot;/usr/lib/python3/dist-packages/pip/_vendor/__init__.py&quot;, line 65, in &lt;module&gt;
    vendored(&quot;cachecontrol&quot;)
  File &quot;/usr/lib/python3/dist-packages/pip/_vendor/__init__.py&quot;, line 36, in vendored
    __import__(modulename, globals(), locals(), level=0)
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 991, in _find_and_load
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 975, in _find_and_load_unlocked
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 655, in _load_unlocked
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 618, in _load_backward_compatible
  File &quot;&lt;frozen zipimport&gt;&quot;, line 259, in load_module
  File &quot;/usr/share/python-wheels/CacheControl-0.12.6-py2.py3-none-any.whl/cachecontrol/__init__.py&quot;, line 9, in &lt;module&gt;
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 991, in _find_and_load
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 975, in _find_and_load_unlocked
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 655, in _load_unlocked
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 618, in _load_backward_compatible
  File &quot;&lt;frozen zipimport&gt;&quot;, line 259, in load_module
  File &quot;/usr/share/python-wheels/CacheControl-0.12.6-py2.py3-none-any.whl/cachecontrol/wrapper.py&quot;, line 1, in &lt;module&gt;
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 991, in _find_and_load
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 975, in _find_and_load_unlocked
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 655, in _load_unlocked
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 618, in _load_backward_compatible
  File &quot;&lt;frozen zipimport&gt;&quot;, line 259, in load_module
  File &quot;/usr/share/python-wheels/CacheControl-0.12.6-py2.py3-none-any.whl/cachecontrol/adapter.py&quot;, line 5, in &lt;module&gt;
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 991, in _find_and_load
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 975, in _find_and_load_unlocked
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 655, in _load_unlocked
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 618, in _load_backward_compatible
  File &quot;&lt;frozen zipimport&gt;&quot;, line 259, in load_module
  File &quot;/usr/share/python-wheels/requests-2.22.0-py2.py3-none-any.whl/requests/__init__.py&quot;, line 95, in &lt;module&gt;
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 991, in _find_and_load
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 975, in _find_and_load_unlocked
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 655, in _load_unlocked
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 618, in _load_backward_compatible
  File &quot;&lt;frozen zipimport&gt;&quot;, line 259, in load_module
  File &quot;/usr/share/python-wheels/urllib3-1.25.8-py2.py3-none-any.whl/urllib3/contrib/pyopenssl.py&quot;, line 46, in &lt;module&gt;
  File &quot;/home/patrick/.local/lib/python3.8/site-packages/OpenSSL/__init__.py&quot;, line 8, in &lt;module&gt;
    from OpenSSL import crypto, SSL
  File &quot;/home/patrick/.local/lib/python3.8/site-packages/OpenSSL/crypto.py&quot;, line 3268, in &lt;module&gt;
    _lib.OpenSSL_add_all_algorithms()
AttributeError: module 'lib' has no attribute 'OpenSSL_add_all_algorithms'
</code></pre>
<p>I'm running Ubuntu 20.04 in WSL. Python <code>openssl</code> is already installed.</p>
<pre><code>sudo apt install python3-openssl
Reading package lists... Done
Building dependency tree       
Reading state information... Done
python3-openssl is already the newest version (19.0.0-1build1).
0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.
</code></pre>
<p>My assumption is that I need to re-install some stuff, but I'm not sure what. I've tried the obvious stuff like <code>python3-openssl</code>, <code>libssl-dev</code>, <code>libffi-dev</code>, and <code>python3-pip</code> itself and <code>python3</code> alltogether.</p>
"
"<p>I installed a package with <code>poetry add X</code>, and so now it shows up in the toml file and in the venv (mine's at <code>.venv/lib/python3.10/site-packages/</code>).</p>
<p>Now to remove that package, I could use <code>poetry remove X</code> and I know that would work properly.
But sometimes, it's easier to just go into the toml file and delete the package line there. So that's what I tried by removing the line for X.</p>
<p>I then tried doing <code>poetry install</code> but that didn't do anything
When I do <code>ls .venv/lib/python3.10/site-packages/</code>, I still see X is installed there.</p>
<p>I also tried <code>poetry lock</code> but no change with that either.</p>
<p>So is there some command to take the latest toml file and clean up packages from being installed that are no longer present in the toml?</p>
"
"<p>I'm working with datetime information in pandas and wanted to convert a bunch of <code>datetime64[ns]</code> columns to <code>str</code>. I noticed a different behavior from the two approaches that I expected to yield the same result.</p>
<p>Here's a <a href=""https://stackoverflow.com/help/minimal-reproducible-example"">MCVE</a>.</p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd

# Create a dataframe with dates according to ISO8601
df = pd.DataFrame({&quot;dt_column&quot;: [&quot;2023-01-01&quot;, &quot;2023-01-02&quot;, &quot;2023-01-02&quot;]})

# Convert the strings to datetimes
# (I expect the time portion to be 00:00:00)
df[&quot;dt_column&quot;] = pd.to_datetime(df[&quot;dt_column&quot;])

df[&quot;str_from_astype&quot;] = df[&quot;dt_column&quot;].astype(str)
df[&quot;str_from_apply&quot;] = df[&quot;dt_column&quot;].apply(str)

print(df)
print()
print(&quot;Datatypes of the dataframe&quot;)
print(df.dtypes)
</code></pre>
<p><strong>Output</strong></p>
<pre><code>   dt_column str_from_astype       str_from_apply
0 2023-01-01      2023-01-01  2023-01-01 00:00:00
1 2023-01-02      2023-01-02  2023-01-02 00:00:00
2 2023-01-02      2023-01-02  2023-01-02 00:00:00

Datatypes of the dataframe
dt_column          datetime64[ns]
str_from_astype            object
str_from_apply             object
dtype: object
</code></pre>
<p>If I use <code>.astype(str)</code> the time information is lost and when I use <code>.apply(str)</code> the time information is retained (or inferred).</p>
<p>Why is that?</p>
<p>(Pandas v1.5.2, Python 3.9.15)</p>
"
"<p>I always though that Callable is equivalent to having the dunder <code>__call__</code> but apparently there is also <code>__name__</code>, because the following code is correct for <code>mypy --strict</code>:</p>
<pre class=""lang-py prettyprint-override""><code>def print_name(f: Callable[..., Any]) -&gt; None:
    print(f.__name__)

def foo() -&gt; None:
    pass

print_name(foo)

print_name(lambda x: x)

</code></pre>
<p>What is actual interface of python Callable?</p>
<p>I dug out what <code>functools.wraps</code> does. AFAIU it sets <code>('__module__', '__name__', '__qualname__', '__doc__', '__annotations__')</code> - is that the same what the <code>Callable</code> is expected to have?</p>
"
"<p>Is there any difference between the typing.cast function and the built-in cast function?</p>
<pre><code>x = 123
y = str(x)
</code></pre>
<pre><code>from typing import cast
x = 123
y = cast(str, x)
</code></pre>
<p>I expected that mypy might not like the first case and would prefer the typing.cast but this was not the case.</p>
"
"<p>I am trying to convert <a href=""https://github.com/facebookresearch/detr"" rel=""nofollow noreferrer"">detr model</a> to tensor flow using onnx. I converted the model using <code>torch.onnx.export</code> with opset_version=12.(which produces a <code>detr.onnx</code> file)</p>
<p>Then I tried to convert the onnx file to tensorflow model using <a href=""https://github.com/onnx/onnx-tensorflow/blob/main/example/onnx_to_tf.py"" rel=""nofollow noreferrer"">this example</a>. I added <code>onnx.check_model</code> line to make sure model is loaded correctly.</p>
<pre><code>import math
from PIL import Image
import requests
import matplotlib.pyplot as plt
import torch
from torch import nn
from torchvision.models import resnet50
import onnx
from onnx_tf.backend import prepare
import torchvision.transforms as T

torch.set_grad_enabled(False)
model = torch.hub.load('facebookresearch/detr', 'detr_resnet50', pretrained=True)

url = 'http://images.cocodataset.org/val2017/000000039769.jpg'
im = Image.open(requests.get(url, stream=True).raw)
transform = T.Compose([
    T.Resize(800),
    T.ToTensor(),
    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])
img = transform(im).unsqueeze(0)

torch.onnx.export(model, img, 'detr.onnx', opset_version = 12)
    
onnx_model = onnx.load('./detr.onnx')
    
result = onnx.checker.check_model(onnx_model)
    
tf_rep = prepare(onnx_model)
tf_rep.export_graph('./model.pb')
</code></pre>
<p>This code raises an exception when it reaches    <code>tf_rep.export_graph('./model.pb')</code> line.
<br>onnx version = 1.13.0 , torch version = 1.13.0+cu117 , onnx_tf = 1.10.0</p>
<p>message of exception :</p>
<pre><code>KeyError                                  Traceback (most recent call last)
Cell In[19], line 26
     23 result = onnx.checker.check_model(onnx_model)
     25 tf_rep = prepare(onnx_model)
---&gt; 26 tf_rep.export_graph('./model.pb')

File ~\AppData\Local\Programs\Python\Python39\lib\site-packages\onnx_tf\backend_rep.py:143, in TensorflowRep.export_graph(self, path)
    129 &quot;&quot;&quot;Export backend representation to a Tensorflow proto file.
    130 
    131 This function obtains the graph proto corresponding to the ONNX
   (...)
    137 :returns: none.
    138 &quot;&quot;&quot;
    139 self.tf_module.is_export = True
    140 tf.saved_model.save(
    141     self.tf_module,
    142     path,
--&gt; 143     signatures=self.tf_module.__call__.get_concrete_function(
    144         **self.signatures))
    145 self.tf_module.is_export = False

File ~\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorflow\python\eager\def_function.py:1239, in Function.get_concrete_function(self, *args, **kwargs)
   1237 def get_concrete_function(self, *args, **kwargs):
   1238   # Implements GenericFunction.get_concrete_function.
-&gt; 1239   concrete = self._get_concrete_function_garbage_collected(*args, **kwargs)
   1240   concrete._garbage_collector.release()  # pylint: disable=protected-access
   1241   return concrete

File ~\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorflow\python\eager\def_function.py:1219, in Function._get_concrete_function_garbage_collected(self, *args, **kwargs)
   1217   if self._stateful_fn is None:
   1218     initializers = []
-&gt; 1219     self._initialize(args, kwargs, add_initializers_to=initializers)
   1220     self._initialize_uninitialized_variables(initializers)
   1222 if self._created_variables:
   1223   # In this case we have created variables on the first call, so we run the
   1224   # defunned version which is guaranteed to never create variables.

File ~\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorflow\python\eager\def_function.py:785, in Function._initialize(self, args, kwds, add_initializers_to)
    782 self._lifted_initializer_graph = lifted_initializer_graph
    783 self._graph_deleter = FunctionDeleter(self._lifted_initializer_graph)
    784 self._concrete_stateful_fn = (
--&gt; 785     self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access
    786         *args, **kwds))
    788 def invalid_creator_scope(*unused_args, **unused_kwds):
    789   &quot;&quot;&quot;Disables variable creation.&quot;&quot;&quot;

File ~\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorflow\python\eager\function.py:2523, in Function._get_concrete_function_internal_garbage_collected(self, *args, **kwargs)
   2521   args, kwargs = None, None
   2522 with self._lock:
-&gt; 2523   graph_function, _ = self._maybe_define_function(args, kwargs)
   2524 return graph_function

File ~\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorflow\python\eager\function.py:2760, in Function._maybe_define_function(self, args, kwargs)
   2758   # Only get placeholders for arguments, not captures
   2759   args, kwargs = placeholder_dict[&quot;args&quot;]
-&gt; 2760 graph_function = self._create_graph_function(args, kwargs)
   2762 graph_capture_container = graph_function.graph._capture_func_lib  # pylint: disable=protected-access
   2763 # Maintain the list of all captures

File ~\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorflow\python\eager\function.py:2670, in Function._create_graph_function(self, args, kwargs)
   2665 missing_arg_names = [
   2666     &quot;%s_%d&quot; % (arg, i) for i, arg in enumerate(missing_arg_names)
   2667 ]
   2668 arg_names = base_arg_names + missing_arg_names
   2669 graph_function = ConcreteFunction(
-&gt; 2670     func_graph_module.func_graph_from_py_func(
   2671         self._name,
   2672         self._python_function,
   2673         args,
   2674         kwargs,
   2675         self.input_signature,
   2676         autograph=self._autograph,
   2677         autograph_options=self._autograph_options,
   2678         arg_names=arg_names,
   2679         capture_by_value=self._capture_by_value),
   2680     self._function_attributes,
   2681     spec=self.function_spec,
   2682     # Tell the ConcreteFunction to clean up its graph once it goes out of
   2683     # scope. This is not the default behavior since it gets used in some
   2684     # places (like Keras) where the FuncGraph lives longer than the
   2685     # ConcreteFunction.
   2686     shared_func_graph=False)
   2687 return graph_function

File ~\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorflow\python\framework\func_graph.py:1247, in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, acd_record_initial_resource_uses)
   1244 else:
   1245   _, original_func = tf_decorator.unwrap(python_func)
-&gt; 1247 func_outputs = python_func(*func_args, **func_kwargs)
   1249 # invariant: `func_outputs` contains only Tensors, CompositeTensors,
   1250 # TensorArrays and `None`s.
   1251 func_outputs = nest.map_structure(
   1252     convert, func_outputs, expand_composites=True)

File ~\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorflow\python\eager\def_function.py:677, in Function._defun_with_scope.&lt;locals&gt;.wrapped_fn(*args, **kwds)
    673 with default_graph._variable_creator_scope(scope, priority=50):  # pylint: disable=protected-access
    674   # __wrapped__ allows AutoGraph to swap in a converted function. We give
    675   # the function a weak reference to itself to avoid a reference cycle.
    676   with OptionalXlaContext(compile_with_xla):
--&gt; 677     out = weak_wrapped_fn().__wrapped__(*args, **kwds)
    678   return out

File ~\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorflow\python\eager\function.py:3317, in class_method_to_instance_method.&lt;locals&gt;.bound_method_wrapper(*args, **kwargs)
   3312   return wrapped_fn(weak_instance(), *args, **kwargs)
   3314 # If __wrapped__ was replaced, then it is always an unbound function.
   3315 # However, the replacer is still responsible for attaching self properly.
   3316 # TODO(mdan): Is it possible to do it here instead?
-&gt; 3317 return wrapped_fn(*args, **kwargs)

File ~\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorflow\python\framework\func_graph.py:1233, in func_graph_from_py_func.&lt;locals&gt;.autograph_handler(*args, **kwargs)
   1231 except Exception as e:  # pylint:disable=broad-except
   1232   if hasattr(e, &quot;ag_error_metadata&quot;):
-&gt; 1233     raise e.ag_error_metadata.to_exception(e)
   1234   else:
   1235     raise

File ~\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorflow\python\framework\func_graph.py:1222, in func_graph_from_py_func.&lt;locals&gt;.autograph_handler(*args, **kwargs)
   1220 # TODO(mdan): Push this block higher in tf.function's call stack.
   1221 try:
-&gt; 1222   return autograph.converted_call(
   1223       original_func,
   1224       args,
   1225       kwargs,
   1226       options=autograph.ConversionOptions(
   1227           recursive=True,
   1228           optional_features=autograph_options,
   1229           user_requested=True,
   1230       ))
   1231 except Exception as e:  # pylint:disable=broad-except
   1232   if hasattr(e, &quot;ag_error_metadata&quot;):

File ~\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorflow\python\autograph\impl\api.py:439, in converted_call(f, args, kwargs, caller_fn_scope, options)
    437 try:
    438   if kwargs is not None:
--&gt; 439     result = converted_f(*effective_args, **kwargs)
    440   else:
    441     result = converted_f(*effective_args)

File ~\AppData\Local\Temp\__autograph_generated_fileq0h7j9t_.py:30, in outer_factory.&lt;locals&gt;.inner_factory.&lt;locals&gt;.tf____call__(self, **kwargs)
     28 node = ag__.Undefined('node')
     29 onnx_node = ag__.Undefined('onnx_node')
---&gt; 30 ag__.for_stmt(ag__.ld(self).graph_def.node, None, loop_body, get_state, set_state, (), {'iterate_names': 'node'})
     31 outputs = ag__.converted_call(ag__.ld(dict), (), None, fscope)
     33 def get_state_4():

File ~\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorflow\python\autograph\operators\control_flow.py:463, in for_stmt(iter_, extra_test, body, get_state, set_state, symbol_names, opts)
    459   _tf_distributed_iterable_for_stmt(
    460       iter_, extra_test, body, get_state, set_state, symbol_names, opts)
    462 else:
--&gt; 463   _py_for_stmt(iter_, extra_test, body, None, None)

File ~\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorflow\python\autograph\operators\control_flow.py:512, in _py_for_stmt(***failed resolving arguments***)
    510 else:
    511   for target in iter_:
--&gt; 512     body(target)

File ~\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorflow\python\autograph\operators\control_flow.py:478, in _py_for_stmt.&lt;locals&gt;.protected_body(protected_iter)
    477 def protected_body(protected_iter):
--&gt; 478   original_body(protected_iter)
    479   after_iteration()
    480   before_iteration()

File ~\AppData\Local\Temp\__autograph_generated_fileq0h7j9t_.py:23, in outer_factory.&lt;locals&gt;.inner_factory.&lt;locals&gt;.tf____call__.&lt;locals&gt;.loop_body(itr)
     21 node = itr
     22 onnx_node = ag__.converted_call(ag__.ld(OnnxNode), (ag__.ld(node),), None, fscope)
---&gt; 23 output_ops = ag__.converted_call(ag__.ld(self).backend._onnx_node_to_tensorflow_op, (ag__.ld(onnx_node), ag__.ld(tensor_dict), ag__.ld(self).handlers), dict(opset=ag__.ld(self).opset, strict=ag__.ld(self).strict), fscope)
     24 curr_node_output_map = ag__.converted_call(ag__.ld(dict), (ag__.converted_call(ag__.ld(zip), (ag__.ld(onnx_node).outputs, ag__.ld(output_ops)), None, fscope),), None, fscope)
     25 ag__.converted_call(ag__.ld(tensor_dict).update, (ag__.ld(curr_node_output_map),), None, fscope)

File ~\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorflow\python\autograph\impl\api.py:439, in converted_call(f, args, kwargs, caller_fn_scope, options)
    437 try:
    438   if kwargs is not None:
--&gt; 439     result = converted_f(*effective_args, **kwargs)
    440   else:
    441     result = converted_f(*effective_args)

File ~\AppData\Local\Temp\__autograph_generated_filetsq4l59p.py:62, in outer_factory.&lt;locals&gt;.inner_factory.&lt;locals&gt;.tf___onnx_node_to_tensorflow_op(cls, node, tensor_dict, handlers, opset, strict)
     60     pass
     61 handler = ag__.Undefined('handler')
---&gt; 62 ag__.if_stmt(ag__.ld(handlers), if_body_1, else_body_1, get_state_1, set_state_1, ('do_return', 'retval_'), 2)
     64 def get_state_2():
     65     return ()

File ~\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorflow\python\autograph\operators\control_flow.py:1363, in if_stmt(cond, body, orelse, get_state, set_state, symbol_names, nouts)
   1361   _tf_if_stmt(cond, body, orelse, get_state, set_state, symbol_names, nouts)
   1362 else:
-&gt; 1363   _py_if_stmt(cond, body, orelse)

File ~\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorflow\python\autograph\operators\control_flow.py:1416, in _py_if_stmt(cond, body, orelse)
   1414 def _py_if_stmt(cond, body, orelse):
   1415   &quot;&quot;&quot;Overload of if_stmt that executes a Python if statement.&quot;&quot;&quot;
-&gt; 1416   return body() if cond else orelse()

File ~\AppData\Local\Temp\__autograph_generated_filetsq4l59p.py:56, in outer_factory.&lt;locals&gt;.inner_factory.&lt;locals&gt;.tf___onnx_node_to_tensorflow_op.&lt;locals&gt;.if_body_1()
     54     nonlocal retval_, do_return
     55     pass
---&gt; 56 ag__.if_stmt(ag__.ld(handler), if_body, else_body, get_state, set_state, ('do_return', 'retval_'), 2)

File ~\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorflow\python\autograph\operators\control_flow.py:1363, in if_stmt(cond, body, orelse, get_state, set_state, symbol_names, nouts)
   1361   _tf_if_stmt(cond, body, orelse, get_state, set_state, symbol_names, nouts)
   1362 else:
-&gt; 1363   _py_if_stmt(cond, body, orelse)

File ~\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorflow\python\autograph\operators\control_flow.py:1416, in _py_if_stmt(cond, body, orelse)
   1414 def _py_if_stmt(cond, body, orelse):
   1415   &quot;&quot;&quot;Overload of if_stmt that executes a Python if statement.&quot;&quot;&quot;
-&gt; 1416   return body() if cond else orelse()

File ~\AppData\Local\Temp\__autograph_generated_filetsq4l59p.py:48, in outer_factory.&lt;locals&gt;.inner_factory.&lt;locals&gt;.tf___onnx_node_to_tensorflow_op.&lt;locals&gt;.if_body_1.&lt;locals&gt;.if_body()
     46 try:
     47     do_return = True
---&gt; 48     retval_ = ag__.converted_call(ag__.ld(handler).handle, (ag__.ld(node),), dict(tensor_dict=ag__.ld(tensor_dict), strict=ag__.ld(strict)), fscope)
     49 except:
     50     do_return = False

File ~\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorflow\python\autograph\impl\api.py:439, in converted_call(f, args, kwargs, caller_fn_scope, options)
    437 try:
    438   if kwargs is not None:
--&gt; 439     result = converted_f(*effective_args, **kwargs)
    440   else:
    441     result = converted_f(*effective_args)

File ~\AppData\Local\Temp\__autograph_generated_filec7_esoft.py:41, in outer_factory.&lt;locals&gt;.inner_factory.&lt;locals&gt;.tf__handle(cls, node, **kwargs)
     39     nonlocal retval_, do_return
     40     raise ag__.converted_call(ag__.ld(BackendIsNotSupposedToImplementIt), (ag__.converted_call('{} version {} is not implemented.'.format, (ag__.ld(node).op_type, ag__.ld(cls).SINCE_VERSION), None, fscope),), None, fscope)
---&gt; 41 ag__.if_stmt(ag__.ld(ver_handle), if_body, else_body, get_state, set_state, ('do_return', 'retval_'), 2)
     42 return fscope.ret(retval_, do_return)

File ~\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorflow\python\autograph\operators\control_flow.py:1363, in if_stmt(cond, body, orelse, get_state, set_state, symbol_names, nouts)
   1361   _tf_if_stmt(cond, body, orelse, get_state, set_state, symbol_names, nouts)
   1362 else:
-&gt; 1363   _py_if_stmt(cond, body, orelse)

File ~\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorflow\python\autograph\operators\control_flow.py:1416, in _py_if_stmt(cond, body, orelse)
   1414 def _py_if_stmt(cond, body, orelse):
   1415   &quot;&quot;&quot;Overload of if_stmt that executes a Python if statement.&quot;&quot;&quot;
-&gt; 1416   return body() if cond else orelse()

File ~\AppData\Local\Temp\__autograph_generated_filec7_esoft.py:33, in outer_factory.&lt;locals&gt;.inner_factory.&lt;locals&gt;.tf__handle.&lt;locals&gt;.if_body()
     31 try:
     32     do_return = True
---&gt; 33     retval_ = ag__.converted_call(ag__.ld(ver_handle), (ag__.ld(node),), dict(**ag__.ld(kwargs)), fscope)
     34 except:
     35     do_return = False

File ~\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorflow\python\autograph\impl\api.py:439, in converted_call(f, args, kwargs, caller_fn_scope, options)
    437 try:
    438   if kwargs is not None:
--&gt; 439     result = converted_f(*effective_args, **kwargs)
    440   else:
    441     result = converted_f(*effective_args)

File ~\AppData\Local\Temp\__autograph_generated_filevddqx9qt.py:12, in outer_factory.&lt;locals&gt;.inner_factory.&lt;locals&gt;.tf__version(cls, node, **kwargs)
     10 try:
     11     do_return = True
---&gt; 12     retval_ = ag__.converted_call(ag__.ld(cls)._common, (ag__.ld(node),), dict(**ag__.ld(kwargs)), fscope)
     13 except:
     14     do_return = False

File ~\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorflow\python\autograph\impl\api.py:439, in converted_call(f, args, kwargs, caller_fn_scope, options)
    437 try:
    438   if kwargs is not None:
--&gt; 439     result = converted_f(*effective_args, **kwargs)
    440   else:
    441     result = converted_f(*effective_args)

File ~\AppData\Local\Temp\__autograph_generated_filedezd6jrz.py:122, in outer_factory.&lt;locals&gt;.inner_factory.&lt;locals&gt;.tf___common(cls, node, **kwargs)
    120 paddings = ag__.Undefined('paddings')
    121 constant_values = ag__.Undefined('constant_values')
--&gt; 122 ag__.if_stmt(ag__.ld(cls).SINCE_VERSION &lt; 11, if_body_1, else_body_1, get_state_1, set_state_1, ('constant_values', 'paddings'), 2)
    123 cond = ag__.converted_call(ag__.ld(tf).cond, (ag__.converted_call(ag__.ld(check_positive), (ag__.ld(paddings),), None, fscope), ag__.autograph_artifact(lambda : ag__.converted_call(ag__.ld(process_pos_pads), (ag__.ld(x), ag__.ld(paddings), ag__.ld(constant_values)), None, fscope)), ag__.autograph_artifact(lambda : ag__.converted_call(ag__.ld(process_neg_pads), (ag__.ld(x), ag__.ld(paddings), ag__.ld(constant_values)), None, fscope))), None, fscope)
    124 try:

File ~\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorflow\python\autograph\operators\control_flow.py:1363, in if_stmt(cond, body, orelse, get_state, set_state, symbol_names, nouts)
   1361   _tf_if_stmt(cond, body, orelse, get_state, set_state, symbol_names, nouts)
   1362 else:
-&gt; 1363   _py_if_stmt(cond, body, orelse)

File ~\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorflow\python\autograph\operators\control_flow.py:1416, in _py_if_stmt(cond, body, orelse)
   1414 def _py_if_stmt(cond, body, orelse):
   1415   &quot;&quot;&quot;Overload of if_stmt that executes a Python if statement.&quot;&quot;&quot;
-&gt; 1416   return body() if cond else orelse()

File ~\AppData\Local\Temp\__autograph_generated_filedezd6jrz.py:119, in outer_factory.&lt;locals&gt;.inner_factory.&lt;locals&gt;.tf___common.&lt;locals&gt;.else_body_1()
    117 nonlocal paddings, constant_values
    118 paddings = ag__.ld(tensor_dict)[ag__.ld(node).inputs[1]]
--&gt; 119 constant_values = ag__.if_exp(ag__.converted_call(ag__.ld(len), (ag__.ld(node).inputs,), None, fscope) == 3, lambda : ag__.ld(tensor_dict)[ag__.ld(node).inputs[2]], lambda : 0, 'ag__.converted_call(len, (node.inputs,), None, fscope) == 3')

File ~\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorflow\python\autograph\operators\conditional_expressions.py:27, in if_exp(cond, if_true, if_false, expr_repr)
     25   return _tf_if_exp(cond, if_true, if_false, expr_repr)
     26 else:
---&gt; 27   return _py_if_exp(cond, if_true, if_false)

File ~\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorflow\python\autograph\operators\conditional_expressions.py:52, in _py_if_exp(cond, if_true, if_false)
     51 def _py_if_exp(cond, if_true, if_false):
---&gt; 52   return if_true() if cond else if_false()

File ~\AppData\Local\Temp\__autograph_generated_filedezd6jrz.py:119, in outer_factory.&lt;locals&gt;.inner_factory.&lt;locals&gt;.tf___common.&lt;locals&gt;.else_body_1.&lt;locals&gt;.&lt;lambda&gt;()
    117 nonlocal paddings, constant_values
    118 paddings = ag__.ld(tensor_dict)[ag__.ld(node).inputs[1]]
--&gt; 119 constant_values = ag__.if_exp(ag__.converted_call(ag__.ld(len), (ag__.ld(node).inputs,), None, fscope) == 3, lambda : ag__.ld(tensor_dict)[ag__.ld(node).inputs[2]], lambda : 0, 'ag__.converted_call(len, (node.inputs,), None, fscope) == 3')

KeyError: in user code:

    File &quot;C:\Users\alihe\AppData\Local\Programs\Python\Python39\lib\site-packages\onnx_tf\backend_tf_module.py&quot;, line 99, in __call__  *
        output_ops = self.backend._onnx_node_to_tensorflow_op(onnx_node,
    File &quot;C:\Users\alihe\AppData\Local\Programs\Python\Python39\lib\site-packages\onnx_tf\backend.py&quot;, line 347, in _onnx_node_to_tensorflow_op  *
        return handler.handle(node, tensor_dict=tensor_dict, strict=strict)
    File &quot;C:\Users\alihe\AppData\Local\Programs\Python\Python39\lib\site-packages\onnx_tf\handlers\handler.py&quot;, line 59, in handle  *
        return ver_handle(node, **kwargs)
    File &quot;C:\Users\alihe\AppData\Local\Programs\Python\Python39\lib\site-packages\onnx_tf\handlers\backend\pad.py&quot;, line 91, in version_11  *
        return cls._common(node, **kwargs)
    File &quot;C:\Users\alihe\AppData\Local\Programs\Python\Python39\lib\site-packages\onnx_tf\handlers\backend\pad.py&quot;, line 73, in _common  *
        constant_values = tensor_dict[node.inputs[2]] if len(

    KeyError: ''
</code></pre>
"
"<p>Those two concepts <strong>Field</strong> and <strong>Annotated</strong> seem very similar in functionality. For example, I can define the same variable in any way as:</p>
<pre><code>temperature: float = Field(0.0, ge=0, le=1)
temperature: Annotated[confloat(ge=0, le=1),...] = 0.0
</code></pre>
<p>Is there any drawback of using only Field or Annotated?</p>
"
"<p>After activating my venv and installing Pydantic through</p>
<pre class=""lang-bash prettyprint-override""><code>pip install pydantic
</code></pre>
<p>I tried creating a new file as follows:</p>
<pre class=""lang-py prettyprint-override""><code>from pydantic import Basemodel, EmailStr

class Person(BaseModel):
      id: int
      name: str
      email: EmailStr
</code></pre>
<p>and was just running this file with the unused class. But it returned the following error:</p>
<pre><code>ModuleNotFoundError: No module named 'email_validator'
.
.
.
ImportError: email-validator is not installed, run `pip install pydantic[email]`
</code></pre>
<p>When running the hint</p>
<pre class=""lang-bash prettyprint-override""><code>pip install pydantic[email]
</code></pre>
<p>it tells me:</p>
<pre><code>zsh: no matches found: pydantic[email]
</code></pre>
<p>Could someone help me with this stupid beginner error? Thanks!</p>
<p>I reinstalled Pydantic, recreated a venv, reactivated the venv.</p>
"
"<p>There are a lot of changes in scikit-learn 1.2.0 where it supports pandas output for all of the transformers but how can I use it in a custom transformer?</p>
<p><strong>In [1]:</strong> Here is my custom transformer which is a standard scaler: <br></p>
<pre><code>from sklearn.base import BaseEstimator, TransformerMixin
import numpy as np

class StandardScalerCustom(BaseEstimator, TransformerMixin):
    def fit(self, X, y=None):
        self.mean = np.mean(X, axis=0)
        self.std = np.std(X, axis=0)
        return self

    def transform(self, X):
        return (X - self.mean) / self.std
</code></pre>
<p><strong>In [2]:</strong> Created a specific <code>scale</code> pipeline</p>
<pre><code>scale_pipe = make_pipeline(StandardScalerCustom())
</code></pre>
<p><strong>In [3]:</strong> Added in a full pipeline where it may get mixed with scalers, imputers, encoders etc.</p>
<pre><code>full_pipeline = ColumnTransformer([
    (&quot;imputer&quot;, impute_pipe, ['column_1'])
    (&quot;scaler&quot;, scale_pipe, ['column_2'])
])

# From documentation
full_pipeline.set_output(transform=&quot;pandas&quot;)
</code></pre>
<p>Got this error: <br></p>
<p><strong>ValueError</strong>: Unable to configure output for StandardScalerCustom() because <code>set_output</code> is not available.</p>
<hr />
<p>There is a solution and it can be:
<code>set_config(transform_output=&quot;pandas&quot;)</code> <br></p>
<p>But in <strong>case-to-case basis</strong>, how can I create a function in StandardScalerCustom() class that can fix the error above?</p>
"
"<p>I have 3 simple classes like:</p>
<pre class=""lang-py prettyprint-override""><code>class Animal(abc.ABC):
    ...

class Cat(Animal):
    ...

class Dog(Animal):
    ...
</code></pre>
<p>Then I have a function which is annotated as such:</p>
<pre class=""lang-py prettyprint-override""><code>def speak(animals: List[Animal]) -&gt; List[str]:
   ...
</code></pre>
<p>My problem is that I want to constrain the <code>List[Animal]</code> to only include one type of animal, so:</p>
<pre class=""lang-py prettyprint-override""><code>speak([Dog(), Dog()]) # OK
speak([Cat(), Cat()]) # OK
speak([Cat(), Dog()]) # typing error
</code></pre>
<p>How would I annotate the <code>speak</code> function to allow for this? Is it even possible to do using typing or am I forced to check this at runtime?</p>
<p>I have tried to use the <code>List[Animal]</code> as above but that doesn't give me an error when calling <code>speak</code> like <code>speak([Cat(), Dog()])</code>.</p>
<p>I have also tried messing around with generics like <code>TypeVar('T', bound=Animal)</code> but this still allows me to pass in a <code>List</code> of any combination of subclasses.</p>
"
"<p>Both the FastAPI backend and the Next.js frontend are running on <code>localost</code>. On the same computer, the frontend makes API calls using <code>fetch</code> without any issues. However, on a different computer on the <strong>same</strong> network, e.g., on <code>192.168.x.x</code>, the frontend runs, but its API calls are no longer working.</p>
<p>I have tried using a proxy as next.js but that still does not work.</p>
<p>Frontend:</p>
<pre class=""lang-js prettyprint-override""><code>
export default function People({setPerson}:PeopleProps)  {
 const fetcher = async (url:string) =&gt; await axios.get(url).then((res) =&gt; res.data);
 const { data, error, isLoading } = useSWR(`${process.env.NEXT_PUBLIC_API}/people`, fetcher);
  if (error) return &lt;div&gt;&quot;Failed to load...&quot;&lt;/div&gt;;
  return (
      &lt;&gt;
        {isLoading? &quot;Loading...&quot; :data.map((person: Person) =&gt;
        &lt;div key={person.id}&gt; {person.name} &lt;/div&gt;)}
     &lt;/&gt; 
  )
 }
</code></pre>
<p>The Next.js app loads the <code>env.local</code> file at startup, which contains:
<code>NEXT_PUBLIC_API=http://locahost:20002</code></p>
<p>Backend:</p>
<pre class=""lang-py prettyprint-override""><code>rom typing import List
from fastapi import APIRouter, Depends
from ..utils.db import get_session as db
from sqlmodel import Session, select
from ..schemas.person import Person, PersonRead
router = APIRouter()

@router.get(&quot;/people&quot;, response_model = List[PersonRead])
async def get_people(sess: Session = Depends(db)):
    res = sess.exec(select(Person)).all()
    return res 
</code></pre>
<p>The frontend runs with: <code>npm run dev</code>, and outputs</p>
<pre><code>ready - started server on 0.0.0.0:3000, url: http://localhost:3000
</code></pre>
<p>The backend runs with: <code>uvicorn hogar_api.main:app --port=20002 --host=0.0.0.0 --reload</code>, and outputs:</p>
<pre><code>INFO:     Uvicorn running on http://0.0.0.0:20002 (Press CTRL+C to quit)
</code></pre>
<p>When I open the browser on <code>http://localhost:3000</code> <em>on the same machine</em> the list of <code>Person</code> is displayed on the screen.</p>
<p>When I open the browser on <code>http://192.168.x.x:3000</code> <em>on another machine on the <em>same</em> network</em>, I get the &quot;Failed to Load...&quot; message.</p>
<p>When I open the FastAPI swagger docs on either machine, the documentation is displayed correctly and all the endpoints work as expected.</p>
<p>CORS look like this:</p>
<pre class=""lang-py prettyprint-override""><code>from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware

app = FastAPI()

origins = [
    &quot;http://localhost:3000&quot;,
]

app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,
    allow_credentials=True,
    allow_methods=[&quot;*&quot;],
    allow_headers=[&quot;*&quot;],
)
</code></pre>
"
"<p>I'm trying to load tokenizer and seq2seq model from pretrained models.</p>
<pre><code>from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

tokenizer = AutoTokenizer.from_pretrained(&quot;ozcangundes/mt5-small-turkish-summarization&quot;)

model = AutoModelForSeq2SeqLM.from_pretrained(&quot;ozcangundes/mt5-small-turkish-summarization&quot;)
</code></pre>
<p>But I got this error.</p>
<pre><code>File ~/.local/lib/python3.8/site-packages/google/protobuf/descriptor.py:1028, in FileDescriptor.__new__(cls, name, package, options, serialized_options, serialized_pb, dependencies, public_dependencies, syntax, pool, create_key)
   1026     raise RuntimeError('Please link in cpp generated lib for %s' % (name))
   1027 elif serialized_pb:
-&gt; 1028   return _message.default_pool.AddSerializedFile(serialized_pb)
   1029 else:
   1030   return super(FileDescriptor, cls).__new__(cls)

    TypeError: Couldn't build proto file into descriptor pool: duplicate file name (sentencepiece_model.proto)
</code></pre>
<p>I tried updating or downgrading the protobuf version. But I couldn't fix</p>
"
"<p>I initiated pyspark in cmd and performed below to sharpen my skills.</p>
<pre><code>C:\Users\Administrator&gt;SUCCESS: The process with PID 5328 (child process of PID 4476) has been terminated.
SUCCESS: The process with PID 4476 (child process of PID 1092) has been terminated.
SUCCESS: The process with PID 1092 (child process of PID 3952) has been terminated.
pyspark
Python 3.11.1 (tags/v3.11.1:a7a450f, Dec  6 2022, 19:58:39) [MSC v.1934 64 bit (AMD64)] on win32
Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.
Setting default log level to &quot;WARN&quot;.
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
23/01/08 20:07:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 3.3.1
      /_/

Using Python version 3.11.1 (tags/v3.11.1:a7a450f, Dec  6 2022 19:58:39)
Spark context Web UI available at http://Mohit:4040
Spark context available as 'sc' (master = local[*], app id = local-1673188677388).
SparkSession available as 'spark'.
&gt;&gt;&gt; 23/01/08 20:08:10 WARN ProcfsMetricsGetter: Exception when trying to compute pagesize, as a result reporting of ProcessTree metrics is stopped
a = sc.parallelize([1,2,3,4,5,6,7,8,9,10])
</code></pre>
<p>When I execute a.take(1), I get &quot;_pickle.PicklingError: Could not serialize object: IndexError: tuple index out of range&quot; error and I am unable to find why. When same is run on google colab, it doesn't throw any error. Below is what I get in console.</p>
<pre><code>&gt;&gt;&gt; a.take(1)
Traceback (most recent call last):
  File &quot;C:\Spark\python\pyspark\serializers.py&quot;, line 458, in dumps
    return cloudpickle.dumps(obj, pickle_protocol)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;C:\Spark\python\pyspark\cloudpickle\cloudpickle_fast.py&quot;, line 73, in dumps
    cp.dump(obj)
  File &quot;C:\Spark\python\pyspark\cloudpickle\cloudpickle_fast.py&quot;, line 602, in dump
    return Pickler.dump(self, obj)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;C:\Spark\python\pyspark\cloudpickle\cloudpickle_fast.py&quot;, line 692, in reducer_override
    return self._function_reduce(obj)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;C:\Spark\python\pyspark\cloudpickle\cloudpickle_fast.py&quot;, line 565, in _function_reduce
    return self._dynamic_function_reduce(obj)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;C:\Spark\python\pyspark\cloudpickle\cloudpickle_fast.py&quot;, line 546, in _dynamic_function_reduce
    state = _function_getstate(func)
            ^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;C:\Spark\python\pyspark\cloudpickle\cloudpickle_fast.py&quot;, line 157, in _function_getstate
    f_globals_ref = _extract_code_globals(func.__code__)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;C:\Spark\python\pyspark\cloudpickle\cloudpickle.py&quot;, line 334, in _extract_code_globals
    out_names = {names[oparg]: None for _, oparg in _walk_global_ops(co)}
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;C:\Spark\python\pyspark\cloudpickle\cloudpickle.py&quot;, line 334, in &lt;dictcomp&gt;
    out_names = {names[oparg]: None for _, oparg in _walk_global_ops(co)}
                 ~~~~~^^^^^^^
IndexError: tuple index out of range
Traceback (most recent call last):
  File &quot;C:\Spark\python\pyspark\serializers.py&quot;, line 458, in dumps
    return cloudpickle.dumps(obj, pickle_protocol)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;C:\Spark\python\pyspark\cloudpickle\cloudpickle_fast.py&quot;, line 73, in dumps
    cp.dump(obj)
  File &quot;C:\Spark\python\pyspark\cloudpickle\cloudpickle_fast.py&quot;, line 602, in dump
    return Pickler.dump(self, obj)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;C:\Spark\python\pyspark\cloudpickle\cloudpickle_fast.py&quot;, line 692, in reducer_override
    return self._function_reduce(obj)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;C:\Spark\python\pyspark\cloudpickle\cloudpickle_fast.py&quot;, line 565, in _function_reduce
    return self._dynamic_function_reduce(obj)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;C:\Spark\python\pyspark\cloudpickle\cloudpickle_fast.py&quot;, line 546, in _dynamic_function_reduce
    state = _function_getstate(func)
            ^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;C:\Spark\python\pyspark\cloudpickle\cloudpickle_fast.py&quot;, line 157, in _function_getstate
    f_globals_ref = _extract_code_globals(func.__code__)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;C:\Spark\python\pyspark\cloudpickle\cloudpickle.py&quot;, line 334, in _extract_code_globals
    out_names = {names[oparg]: None for _, oparg in _walk_global_ops(co)}
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;C:\Spark\python\pyspark\cloudpickle\cloudpickle.py&quot;, line 334, in &lt;dictcomp&gt;
    out_names = {names[oparg]: None for _, oparg in _walk_global_ops(co)}
                 ~~~~~^^^^^^^
IndexError: tuple index out of range

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;
  File &quot;C:\Spark\python\pyspark\rdd.py&quot;, line 1883, in take
    res = self.context.runJob(self, takeUpToNumLeft, p)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;C:\Spark\python\pyspark\context.py&quot;, line 1486, in runJob
    sock_info = self._jvm.PythonRDD.runJob(self._jsc.sc(), mappedRDD._jrdd, partitions)
                                                           ^^^^^^^^^^^^^^^
  File &quot;C:\Spark\python\pyspark\rdd.py&quot;, line 3505, in _jrdd
    wrapped_func = _wrap_function(
                   ^^^^^^^^^^^^^^^
  File &quot;C:\Spark\python\pyspark\rdd.py&quot;, line 3362, in _wrap_function
    pickled_command, broadcast_vars, env, includes = _prepare_for_python_RDD(sc, command)
                                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;C:\Spark\python\pyspark\rdd.py&quot;, line 3345, in _prepare_for_python_RDD
    pickled_command = ser.dumps(command)
                      ^^^^^^^^^^^^^^^^^^
  File &quot;C:\Spark\python\pyspark\serializers.py&quot;, line 468, in dumps
    raise pickle.PicklingError(msg)
_pickle.PicklingError: Could not serialize object: IndexError: tuple index out of range
</code></pre>
<p>It should provide [1] as an answer but instead throws this error. Is it because of incorrect installation?</p>
<p>Package used - spark-3.3.1-bin-hadoop3.tgz, Java(TM) SE Runtime Environment (build 1.8.0_351-b10), Python 3.11.1</p>
<p>Can anyone help in troubleshooting this? Many thanks in advance.</p>
"
"<p>I have a Python project with a <code>pyproject.toml</code> similar to the one below. I use <code>python build</code> to generate an package I use to install in production environments. For development environments, I use <code>pip install -e .</code></p>
<p>I'm trying to figure out how to make sure test dependencies are installed for development environments, but not as part of a production environment.</p>
<pre><code>[build-system]
requires = [&quot;setuptools&gt;=61.0&quot;]
build-backend = &quot;setuptools.build_meta&quot;

[tool.setuptools]
package-dir = {&quot;&quot; = &quot;src&quot;}
packages = [
    &quot;package-a&quot;,
    &quot;package-b&quot;,
]

[tool.setuptools.package-data]
&quot;*&quot; = [
    &quot;path/to/*.txt&quot;
]

[project]
name = &quot;my-project&quot;
version = &quot;0.0.1&quot;

authors = [
  { name=&quot;devnectar&quot;, email=&quot;a@b.com&quot; },
]

description = &quot;description goes here&quot;
readme = &quot;README.md&quot;
requires-python = &quot;&gt;=3.10&quot;

classifiers = [
    &quot;Programming Language :: Python :: 3&quot;,
    &quot;License :: OSI Approved :: MIT License&quot;,
    &quot;Operating System :: OS Independent&quot;,
]

dependencies = [
    &quot;dep&quot;,
    &quot;another-dep&quot;,
    &quot;yet-another-dep&quot;
]

[project.optional-dependencies]
dev = [
    &quot;tox&quot;
]

[project.urls]
&quot;Homepage&quot; = &quot;https://some_url.com&quot;
&quot;Bug Tracker&quot; = &quot;https://some_url.com&quot;

[project.scripts]
nectarserver = &quot;entry-point-script&quot;

[tool.tox]
legacy_tox_ini = &quot;&quot;&quot;
    [tox]
    min_version = 4.0
    env_list = test_env

    [testenv]
    deps = pytest
    commands = pytest -s
&quot;&quot;&quot;
</code></pre>
<p>I also tried <code>test</code> and <code>tests</code> instead of <code>dev</code> when trying to get this to work.</p>
<p>I'm running into two issues:</p>
<ul>
<li>When I run <code>pip install -e .</code>, <code>tox</code> is not installed</li>
<li>The <code>requires.txt</code> generated by the <code>install</code> command ends up having a <code>[dev]</code> section in it, which causes <code>pip install -r mypackage/mypackage.egg-info/requires.txt</code> to error out. This causes other issues down the road in my build chain, which relies on requirements.txt files generated during the build to generate a Docker layer with project dependencies in it.</li>
</ul>
<p>How should I capture the dev only dependency on <code>tox</code> in my <code>pyproject.toml</code>?</p>
"
"<p>About a week ago I set up an application on google. Now when I tri and run:</p>
<pre><code>SCOPES = ['https://www.googleapis.com/auth/gmail.readonly']
creds = None
if os.path.exists('token.pickle'):
     with open(self.CREDENTIALS_PATH+self.conjoiner+'token.pickle', 'rb') as token:
        creds = pickle.load(token)
        if not creds or not creds.valid:
            if creds and creds.expired and creds.refresh_token:
                creds.refresh(Request()) ##error here
</code></pre>
<p>I get the following error:</p>
<pre><code>Exception has occurred: RefreshError
('invalid_grant: Token has been expired or revoked.', {'error': 'invalid_grant', 'error_description': 'Token has been expired or revoked.'})
</code></pre>
<p>What could be the problem?</p>
"
"<p>I have the following folder structure..where _app, and _infra are two different projects.  At the root of the workspace however are two files, the workspace project file itself and a .gitignore file.
Each project has it's own .vscode folder and own .env files.
The entire workspace is a single repository in git.</p>
<pre><code>my_app_workspace
   - proj1_app/
     - .venv/ (virtual environment)
     - vscode/
       - settings.json
       - launch.json
       - task.json
     - src/
       - config.py
     - .env
     - .env_linux
   - proj1_infra/
     - vscode/
       - settings.json
       - launch.json
       - task.json
     - src/
       - config.py
     - .env
     - .env_linux
  - .git_ignore
  -  my_app_workspace.code-workspace
</code></pre>
<p>the code-workspace file looks like this:</p>
<pre><code>{
    &quot;folders&quot;: [
        {
            &quot;path&quot;: &quot;./proj1_app&quot;
        },
        {
            &quot;path&quot;: &quot;./proj1_infra&quot;
        }
    ],
}
</code></pre>
<p>This is all good, but i want to include the .git_ignore and my_app_workspace.code-workspace files also into the vscode editor so that i can easy make modifications to them.
I know i can add another folder with '&quot;path&quot;: &quot;.&quot;', but this will add a folder with the project folders again - which seems redundant and not efficient.</p>
<p><strong>Is there a way to add individual files to the workspace? Is the problem here i should simply split these up into two different repository in git?  this way each has it's own .gitignore file as opposed to what im doing now is the entire workspace is a git repository</strong></p>
"
"<p>I'm starting Pytorch and still trying to understand the basic concepts.</p>
<p>If I have a network <code>n</code> on the GPU that produces an output tensor <code>out</code>, can it be printed to stdout directly? Or should it first be moved to the cpu, or be detached from the graph before printing?</p>
<p>Tried several combinations below involving <code>.cpu()</code> and <code>.detach()</code></p>
<pre class=""lang-py prettyprint-override""><code>import torch.nn as nn
import torch


class Net(nn.Module):
    def __init__(self):
        super().__init__()
        self.layers = nn.Sequential(
            nn.Linear(5, 10),
            nn.ReLU(),
            nn.Linear(10, 10),
            nn.ReLU(),
            nn.Linear(10, 3),
        )

    def forward(self, x):
        return self.layers(x)


device = torch.device(&quot;cuda:0&quot;)  # assume its available
x = torch.rand(10, 5).to(device)
net = Net().to(device)

# Pretend we are in a training loop iteration

out = net(x)
print(f&quot;The output is {out.max()}&quot;)
print(f&quot;The output is {out.max().detach()}&quot;)
print(f&quot;The output is {out.max().cpu()}&quot;)
print(f&quot;The output is {out.max().cpu().detach()}&quot;)

# continue training iteration and repeat more iterations in training loop
</code></pre>
<p>I got the same output for all 4 methods. Which is the correct way?</p>
"
"<p>I am getting below error when running <code>mlflow app</code></p>
<blockquote>
<p>raise AttributeError(&quot;module {!r} has no attribute &quot; AttributeError:
module 'numpy' has no attribute 'object'</p>
</blockquote>
<p>Can someone help me with this</p>
"
"<p>I'm trying to implement R code inside some Python (3.10) software using rpy2 (3.5.7). I want to know whether I can get rpy2 to work before trying anything complicated. This is an &quot;off-the-shelf&quot; execution, using one of the earliest examples in the documentation introduction. I am running this from inside the PyCharm IDE.  There is no mention of performing any prerequisites in the documentation.</p>
<p>There is a slight nuisance to this simple code. It is being executed within an event call (clicking a button) using the DearPyGUI package.</p>
<p>This is the rpy2 code:</p>
<pre><code>import rpy2.robjects as objects
print(robjects.r)
</code></pre>
<p>Unfortunately, this throws:</p>
<pre><code>...
    raise NotImplementedError(_missingconverter_msg)
NotImplementedError: 
    Conversion rules for `rpy2.robjects` appear to be missing. Those
    rules are in a Python contextvars.ContextVar. This could be caused
    by multithreading code not passing context to the thread.
</code></pre>
<p>This is a working example of the error:</p>
<pre><code>import dearpygui.dearpygui as dpg
import rpy2.robjects as robjects

def testFunction():
    print(robjects.r)

dpg.create_context()
dpg.create_viewport()
dpg.setup_dearpygui()

with dpg.window(label=&quot;Example Window&quot;):
    dpg.add_text(&quot;Hello world&quot;)
    dpg.add_button(label=&quot;Save&quot;, callback=testFunction)

dpg.show_viewport()
dpg.start_dearpygui()
dpg.destroy_context()
</code></pre>
<p>With the full error message:</p>
<pre><code>Traceback (most recent call last):
  File &quot;/home/anthony/CPRD-software/test.py&quot;, line 6, in testFunction
    print(robjects.r)
  File &quot;/home/anthony/anaconda3/envs/CPRD-software/lib/python3.10/site-packages/rpy2/robjects/__init__.py&quot;, line 451, in __str__
    version = self['version']
  File &quot;/home/anthony/anaconda3/envs/CPRD-software/lib/python3.10/site-packages/rpy2/robjects/__init__.py&quot;, line 440, in __getitem__
    res = conversion.get_conversion().rpy2py(res)
  File &quot;/home/anthony/anaconda3/envs/CPRD-software/lib/python3.10/functools.py&quot;, line 889, in wrapper
    return dispatch(args[0].__class__)(*args, **kw)
  File &quot;/home/anthony/anaconda3/envs/CPRD-software/lib/python3.10/site-packages/rpy2/robjects/conversion.py&quot;, line 370, in _raise_missingconverter
    raise NotImplementedError(_missingconverter_msg)
NotImplementedError: 
    Conversion rules for `rpy2.robjects` appear to be missing. Those
    rules are in a Python contextvars.ContextVar. This could be caused
    by multithreading code not passing context to the thread.
</code></pre>
<p>What is going on?</p>
"
"<p>I tried to upgrade my poetry from 1.1.x version to 1.3 but as an official manual (<a href=""https://python-poetry.org/docs/"" rel=""noreferrer"">https://python-poetry.org/docs/</a>) recommends I removed the old version manually. Unfortunately I probably deleted wrong files because after installing 1.3 version I was still receiving errors that seemed sth was in conflict with old poetry.
I tried to find all files in my account (it's a remote machine so I did not want to effect others) connected somehow with poetry (with <code>find /home/username -name *poetry*</code>) and (after uninstalling poetry 1.3) removed them. Then I installed poetry 1.3 back but still did not work.
Also tried to delete my whole repo and clone it again, but same problems remains. I guess I pissed it off already, but hope that there is some way to some hard reset. Is there any way how to get from this?</p>
<p>Here is beginning of my error message:</p>
<pre><code>
Package operations: 28 installs, 0 updates, 0 removals

  • Installing certifi (2021.10.8)
  • Installing charset-normalizer (2.0.12)
  • Installing idna (3.3)
  • Installing six (1.16.0)
  • Installing typing-extensions (4.2.0)
  • Installing urllib3 (1.26.9)

  DBusErrorResponse

  [org.freedesktop.DBus.Error.UnknownMethod] ('No such interface “org.freedesktop.DBus.Properties” on object at path /org/freedesktop/secrets/collection/login',)

  at ~/.local/share/pypoetry/venv/lib/python3.10/site-packages/secretstorage/util.py:48 in send_and_get_reply
       44│     def send_and_get_reply(self, msg: Message) -&gt; Any:
       45│         try:
       46│             resp_msg: Message = self._connection.send_and_get_reply(msg)
       47│             if resp_msg.header.message_type == MessageType.error:
    →  48│                 raise DBusErrorResponse(resp_msg)
       49│             return resp_msg.body
       50│         except DBusErrorResponse as resp:
       51│             if resp.name in (DBUS_UNKNOWN_METHOD, DBUS_NO_SUCH_OBJECT):
       52│                 raise ItemNotFoundException('Item does not exist!') from resp

The following error occurred when trying to handle this error:


  ItemNotFoundException

  Item does not exist!

  at ~/.local/share/pypoetry/venv/lib/python3.10/site-packages/secretstorage/util.py:52 in send_and_get_reply
       48│                 raise DBusErrorResponse(resp_msg)
       49│             return resp_msg.body
       50│         except DBusErrorResponse as resp:
       51│             if resp.name in (DBUS_UNKNOWN_METHOD, DBUS_NO_SUCH_OBJECT):
    →  52│                 raise ItemNotFoundException('Item does not exist!') from resp
       53│             elif resp.name in (DBUS_SERVICE_UNKNOWN, DBUS_EXEC_FAILED,
       54│                                DBUS_NO_REPLY):
       55│                 data = resp.data
       56│                 if isinstance(data, tuple):

The following error occurred when trying to handle this error:


  PromptDismissedException

  Prompt dismissed.

  at ~/.local/share/pypoetry/venv/lib/python3.10/site-packages/secretstorage/collection.py:159 in create_collection
      155│     if len(collection_path) &gt; 1:
      156│         return Collection(connection, collection_path, session=session)
      157│     dismissed, result = exec_prompt(connection, prompt)
      158│     if dismissed:
    → 159│         raise PromptDismissedException('Prompt dismissed.')
      160│     signature, collection_path = result
      161│     assert signature == 'o'
      162│     return Collection(connection, collection_path, session=session)
      163│ 

The following error occurred when trying to handle this error:


  InitError

  Failed to create the collection: Prompt dismissed..

  at ~/.local/share/pypoetry/venv/lib/python3.10/site-packages/keyring/backends/SecretService.py:63 in get_preferred_collection
       59│                 collection = secretstorage.Collection(bus, self.preferred_collection)
       60│             else:
       61│                 collection = secretstorage.get_default_collection(bus)
       62│         except exceptions.SecretStorageException as e:
    →  63│             raise InitError(&quot;Failed to create the collection: %s.&quot; % e)
       64│         if collection.is_locked():
       65│             collection.unlock()
       66│             if collection.is_locked():  # User dismissed the prompt
       67│                 raise KeyringLocked(&quot;Failed to unlock the collection!&quot;)


  DBusErrorResponse

  [org.freedesktop.DBus.Error.UnknownMethod] ('No such interface “org.freedesktop.DBus.Properties” on object at path /org/freedesktop/secrets/collection/login',)

  at ~/.local/share/pypoetry/venv/lib/python3.10/site-packages/secretstorage/util.py:48 in send_and_get_reply
       44│     def send_and_get_reply(self, msg: Message) -&gt; Any:
       45│         try:
       46│             resp_msg: Message = self._connection.send_and_get_reply(msg)
       47│             if resp_msg.header.message_type == MessageType.error:
    →  48│                 raise DBusErrorResponse(resp_msg)
       49│             return resp_msg.body
       50│         except DBusErrorResponse as resp:
       51│             if resp.name in (DBUS_UNKNOWN_METHOD, DBUS_NO_SUCH_OBJECT):
       52│                 raise ItemNotFoundException('Item does not exist!') from resp

The following error occurred when trying to handle this error:


  ItemNotFoundException

  Item does not exist!

  at ~/.local/share/pypoetry/venv/lib/python3.10/site-packages/secretstorage/util.py:52 in send_and_get_reply
       48│                 raise DBusErrorResponse(resp_msg)
       49│             return resp_msg.body
       50│         except DBusErrorResponse as resp:
       51│             if resp.name in (DBUS_UNKNOWN_METHOD, DBUS_NO_SUCH_OBJECT):
    →  52│                 raise ItemNotFoundException('Item does not exist!') from resp
       53│             elif resp.name in (DBUS_SERVICE_UNKNOWN, DBUS_EXEC_FAILED,
       54│                                DBUS_NO_REPLY):
       55│                 data = resp.data
       56│                 if isinstance(data, tuple):

The following error occurred when trying to handle this error:


  PromptDismissedException

  Prompt dismissed.

  at ~/.local/share/pypoetry/venv/lib/python3.10/site-packages/secretstorage/collection.py:159 in create_collection
      155│     if len(collection_path) &gt; 1:
      156│         return Collection(connection, collection_path, session=session)
      157│     dismissed, result = exec_prompt(connection, prompt)
      158│     if dismissed:
    → 159│         raise PromptDismissedException('Prompt dismissed.')
      160│     signature, collection_path = result
      161│     assert signature == 'o'
      162│     return Collection(connection, collection_path, session=session)
      163│ 

</code></pre>
"
"<p>I have a class decorator, which adds a few functions and fields to decorated class.</p>
<pre class=""lang-py prettyprint-override""><code>@mydecorator
@dataclass
class A:
    a: str = &quot;&quot;
</code></pre>
<p>Added (via <code>setattr()</code>) is a <code>.save()</code> function and a set of info for dataclass fields as a separate dict.</p>
<p>I'd like VScode and mypy to properly recognize that, so that when I use:</p>
<pre class=""lang-py prettyprint-override""><code>a=A()
a.save()
</code></pre>
<p>or <code>a.my_fields_dict</code> those 2 are properly recognized.</p>
<p>Is there any way to do that? Maybe modify class <code>A</code> type annotations at runtime?</p>
"
"<p>I have installed opencv-python-4.7.0.68 and opencv-contrib-python-4.7.0.68</p>
<p>The code below gives me the following error:<br/>
<code>AttributeError: module 'cv2.aruco' has no attribute 'CharucoBoard_create'</code></p>
<p>Sample code:</p>
<pre><code>import cv2

aruco_dict = cv2.aruco.getPredefinedDictionary(cv2.aruco.DICT_4X4_50)
board = cv2.aruco.CharucoBoard_create(11, 8, 0.015, 0.011, aruco_dict)
</code></pre>
"
"<p><img src=""https://i.sstatic.net/a2LDr.png"" alt=""error image"" /></p>
<p>I was trying to install daphne===4.0.0 but I keep getting this error,</p>
<pre><code>ERROR: Failed building wheel for twisted-iocpsupport

    Building wheels for collected packages: twisted-iocpsupport
  Building wheel for twisted-iocpsupport (pyproject.toml) ... error
  error: subprocess-exited-with-error

  × Building wheel for twisted-iocpsupport (pyproject.toml) did not run successfully.
  │ exit code: 1
  ╰─&gt; [5 lines of output]
      running bdist_wheel
      running build
      running build_ext
      building 'twisted_iocpsupport.iocpsupport' extension
      error: Microsoft Visual C++ 14.0 or greater is required. Get it with &quot;Microsoft C++ Build Tools&quot;: https://visualstudio.microsoft.com/visual-cpp-build-tools/
      [end of output]

  note: This error originates from a subprocess, and is likely not a problem with pip.
  ERROR: Failed building wheel for twisted-iocpsupport
Failed to build twisted-iocpsupport
ERROR: Could not build wheels for twisted-iocpsupport, which is required to install pyproject.toml-based projects
</code></pre>
"
"<p>I'm trying to make a get request to Azure DevOps.</p>
<p>I have the URL and the Personal_Access_Token. The URL was created following these intructions <a href=""https://learn.microsoft.com/en-us/rest/api/azure/devops/git/items/get?view=azure-devops-rest-6.1&amp;tabs=HTTP#definitions"" rel=""noreferrer"">https://learn.microsoft.com/en-us/rest/api/azure/devops/git/items/get?view=azure-devops-rest-6.1&amp;tabs=HTTP#definitions</a> , and it is working fine in the browser. It is possible to see the information of the file that I'm targeting.</p>
<p>However, when I execute the request in python:</p>
<pre><code>import requests

headers = {
    'Authorization': 'Bearer myPAT',
}

response = requests.get('exampleurl.com/content', headers=headers)
</code></pre>
<p>I'm getting the 203 response...</p>
<p>I have also try other options following this link <a href=""https://stackoverflow.com/questions/19069701/python-requests-library-how-to-pass-authorization-header-with-single-token"">Python requests library how to pass Authorization header with single token</a> without success. Including these headers:</p>
<pre><code>personal_access_token_encoded = base64.b64encode(personal_access_token.encode('utf-8')).decode('utf-8')    
headers={'Authorization': 'Basic '+personal_access_token_encoded}

headers={'Authorization': 'Basic '+personal_access_token}
</code></pre>
<p>But in both cases still having the same response.</p>
<p>For sure I'm not considering something. What could be missing?</p>
"
"<p>I am using project.toml file to package my module, I want to extract the version from git tag using <code>setuptools_scm</code> module.</p>
<p>When I run <code>python setup.p y --version</code> command it gives this output <code>0.0.1.post1.dev0</code>. How will I get only <code>0.0.1</code> value and omit the <code>.post.dev0</code> value?</p>
<p>Here is project.toml file settings:</p>
<pre><code>[build-system]
requires = [&quot;setuptools&gt;=46.1.0&quot;, &quot;setuptools_scm[toml]&gt;=5&quot;]
build-backend = &quot;setuptools.build_meta&quot;

[tool.setuptools_scm]
version_scheme = &quot;no-guess-dev&quot;
local_scheme=&quot;no-local-version&quot;
write_to = &quot;src/showme/version.py&quot;
git_describe_command = &quot;git describe --dirty --tags --long --match v* --first-parent&quot;

[tool.setuptools.dynamic]
version = {attr = &quot;showme.__version__&quot;}
</code></pre>
<p>output:</p>
<pre><code> python setup.py --version
setuptools/config/pyprojecttoml.py:108: _BetaConfiguration: Support for `[tool.setuptools]` in `pyproject.toml` is still *beta*.
  warnings.warn(msg, _BetaConfiguration)
0.0.1.post1.dev0
</code></pre>
<p>Thanks</p>
"
"<p>Python's Itertools has what is called <code>starmap</code>. Given a collection of collections and a function, it applies the function to each collection strictly inside the collection, using the elements of said internal collection as arguments to the function. For example,</p>
<pre><code>from itertools import starmap
 
NestedList = [(1, 2), (3, 4), (5, 6), (0, 0), (1, 1), (2, 2)]

list(starmap(lambda x, y:x + y, NestedList))
</code></pre>
<p>returns the list containing 3, 7, 11, 0, 2, and 4.</p>
<p>I refuse to believe that Python was the first to come up with this concept, but I'm drawing a blank when I try to think of what it was called in older languages. Does any analogous functionality exist in common lisp? I feel certain that it does, but I cannot name it.</p>
"
"<p>I have installed <a href=""https://python-telegram-bot.org"" rel=""noreferrer"">python-telegram-bot</a> like this:</p>
<pre class=""lang-bash prettyprint-override""><code>pip install python-telegram-bot
</code></pre>
<p>And when I'm trying to do this:</p>
<pre class=""lang-py prettyprint-override""><code>from telegram.ext import Updater, CommandHandler, MessageHandler, Filters, CallbackContext
</code></pre>
<p>I get this error:</p>
<pre><code>ImportError: cannot import name 'Filters' from 'telegram.ext' (/home/mobitnlh/virtualenv/db_application/3.8/lib/python3.8/site-packages/telegram/ext/__init__.py)
</code></pre>
"
"<p>In a conda environment with Python 3.8.15 I did
<code>pip install ultralytics</code></p>
<blockquote>
<p>successfully installed ...,ultralytics-8.0.4</p>
</blockquote>
<p>But when running <code>from ultralytics import YOLO</code> , it says</p>
<blockquote>
<p>ModuleNotFoundError: No module named 'ultralytics'</p>
</blockquote>
"
"<p>I am trying to create a shared memory for my Python application, which should be used in the parent process and in another process that is spawned from that parent process. In most cases that works fine, however, sometimes I get the following stacktrace:</p>
<pre class=""lang-py prettyprint-override""><code>Traceback (most recent call last):
  File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt;
  File &quot;/usr/lib/python3.8/multiprocessing/spawn.py&quot;, line 116, in spawn_main
    exitcode = _main(fd, parent_sentinel)
  File &quot;/usr/lib/python3.8/multiprocessing/spawn.py&quot;, line 126, in _main
    self = reduction.pickle.load(from_parent)
  File &quot;/usr/lib/python3.8/multiprocessing/synchronize.py&quot;, line 110, in __setstate__
    self._semlock = _multiprocessing.SemLock._rebuild(*state)
FileNotFoundError: [Errno 2] No such file or directory: '/psm_47f7f5d7'
</code></pre>
<p>I want to emphasize that our code/application works fine in 99% of the time. We are spawning these new processes with new shared memory for each such process on a regular basis in our application (which is a server process, so it's running 24/7). Nearly all the time this works fine, only from time to time this error above is thrown, which then kills the whole application.</p>
<p><strong>Update:</strong> I noticed that this problem occurs mainly when the application was running for a while already. When I start it up the creation of shared memory and spawning new processes works fine without this error.</p>
<p>The shared memory is created like this:</p>
<pre class=""lang-py prettyprint-override""><code># Spawn context for multiprocessing
_mp_spawn_ctxt = multiprocessing.get_context(&quot;spawn&quot;)
_mp_spawn_ctxt_pipe = _mp_spawn_ctxt.Pipe

# Create shared memory
mem_size = width * height * bpp
shared_mem = shared_memory.SharedMemory(create=True, size=mem_size)
image = np.ndarray((height, width, bpp), dtype=np.uint8, buffer=shared_mem.buf)
parent_pipe, child_pipe = _mp_spawn_ctxt_pipe()
time.sleep(0.1)

# Spawn new process
# _CameraProcess is a custom class derived from _mp_spawn_ctxt.Process
proc = _CameraProcess(shared_mem, child_pipe)
proc.start()
</code></pre>
<p>Any ideas what could be the issue here?</p>
"
"<p>I am not very familiar with python, I only done automation with so I am a new with packages and everything.<br />
I am creating an API with Flask, Gunicorn and Poetry.<br />
I noticed that there is a version number inside the pyproject.toml and I would like to create a route /version which returns the version of my app.<br />
My app structure look like this atm:</p>
<pre><code>├── README.md
├── __init__.py
├── poetry.lock
├── pyproject.toml
├── tests
│   └── __init__.py
└── wsgi.py
</code></pre>
<p>Where <code>wsgi.py</code> is my main file which run the app.</p>
<p>I saw peoples using importlib but I didn't find how to make it work as it is used with:<br />
<code> __version__ = importlib.metadata.version(&quot;__package__&quot;)</code><br />
But I have no clue what this <strong>package</strong> mean.</p>
"
"<p>I have made <code>2 ontologies</code> using <a href=""https://owlready2.readthedocs.io/en/v0.37/"" rel=""noreferrer""><code>owlready2</code></a> library</p>
<pre><code>list(ontology1.classes())             

[old_dataset_ontology.weather,
 old_dataset_ontology.season]

list(ontology1.individuals())

[old_dataset_ontology.rainy,
 old_dataset_ontology.windy,
 old_dataset_ontology.cold,
 old_dataset_ontology.clouds]


list(ontology2.classes())             

[new_dataset_ontology.weather,
 new_dataset_ontology.season,
 new_dataset_ontology.season1]

list(ontology2.individuals())

[new_dataset_ontology.rainy,
 new_dataset_ontology.windy,
 new_dataset_ontology.cold1]
</code></pre>
<p>I want to <code>merge</code> them but I do not find a way with <code>olwready2</code>. There is not something in the documentation. I want just a simple string matching and to delete the duplicate classes and indiv</p>
<p>Any ideas?</p>
"
"<p>The full error:</p>
<pre><code>NotImplementedError: Could not run 'torchvision::nms' with arguments from the 'CUDA' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'torchvision::nms' is only available for these backends: [CPU, QuantizedCPU, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradMPS, AutogradXPU, AutogradHPU, AutogradLazy, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].
</code></pre>
<p>I get this when attempting to train a YOLOv8 model on a Windows 11 machine, everything works for the first epoch then this occurs.</p>
<hr />
<p>I also get this error immediately after the first epoch ends but I don't think it is relevant.</p>
<pre><code>Error executing job with overrides: ['task=detect', 'mode=train', 'model=yolov8n.pt', 'data=custom.yaml', 'epochs=300', 'imgsz=160', 'workers=8', 'batch=4']
</code></pre>
<p>I was trying to train a YOLOv8 image detection model utilizing CUDA GPU.</p>
"
"<p>I am using Poetry of version 1.3.2 (currently the last version), and added group to .toml file as below: <code>[tool.poetry.group.dev.dependencies]</code>.</p>
<p>And following <a href=""https://python-poetry.org/docs/master/managing-dependencies/#adding-a-dependency-to-a-group"" rel=""noreferrer"">official documentation</a> tried to add library to this group using command:<br />
<code>poetry add pytest --group dev</code>.</p>
<p>But always getting error that says: <strong>The &quot;--group&quot; option does not exist</strong>.</p>
<p>(Using python of version 3.9.15)</p>
<p>I tried to change version of poetry to 1.2.x, but it did not help.</p>
"
"<p>I am trying google sign in with firebase and trying to load the page through pywebview.</p>
<pre><code>from tkinter import *
import webview as webview
root = Tk()
win_width = root.winfo_screenwidth()               
win_height = root.winfo_screenheight()
root.geometry(&quot;%dx%d&quot; % (win_width, win_height))
webview.create_window(title='My Window', url='http://localhost:81',confirm_close=True)
webview.start()
root.destroy()
</code></pre>
<p>When opened with browser, it works fine, but when opened with my code and clicked on sign in, it shows the following message</p>
<blockquote>
<p>Unable to establish a connection with the popup. it may have been blocked by the browser.</p>
</blockquote>
<p><img src=""https://i.sstatic.net/zrb98.png"" alt=""image"" /></p>
<p>What is the solution?</p>
"
"<p>An attempt has been made to start a new process before the
current process has finished its bootstrapping phase.</p>
<pre><code>    This probably means that you are not using fork to start your
    child processes and you have forgotten to use the proper idiom
    in the main module:

        if __name__ == '__main__':
            freeze_support()
            ...

    The &quot;freeze_support()&quot; line can be omitted if the program
    is not going to be frozen to produce an executable.
</code></pre>
<p>**
This error shows up when trying to train a YOLOv8 model in a python environment**
from ultralytics import YOLO</p>
<pre><code># Load a model
model = YOLO(&quot;yolov8n.yaml&quot;)  # build a new model from scratch
model = YOLO(&quot;yolov8n.pt&quot;)  # load a pretrained model (recommended for training)

# Use the model
results = model.train(data=&quot;coco128.yaml&quot;, epochs=3)  # train the model
results = model.val()  # evaluate model performance on the validation set
results = model(&quot;https://ultralytics.com/images/bus.jpg&quot;)  # predict on an image
success = YOLO(&quot;yolov8n.pt&quot;).export(format=&quot;onnx&quot;)  # export a model to ONNX format
</code></pre>
"
"<p>I am looking to learn how to improve the performance of code over a large dataframe (10 million rows) and my solution loops over multiple dates <code>(2023-01-10, 2023-01-20, 2023-01-30)</code> for different combinations of <code>category_a</code> and <code>category_b</code>.</p>
<p>The working approach is shown below, which iterates over the dates for different pairings of the two-category data by first locating a subset of a particular pair. However, I would want to refactor it to see if there is an approach that is more efficient.</p>
<p>My input (<code>df</code>) looks like:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: right;""></th>
<th style=""text-align: left;"">date</th>
<th style=""text-align: right;"">category_a</th>
<th style=""text-align: right;"">category_b</th>
<th style=""text-align: right;"">outflow</th>
<th style=""text-align: right;"">open</th>
<th style=""text-align: right;"">inflow</th>
<th style=""text-align: right;"">max</th>
<th style=""text-align: right;"">close</th>
<th style=""text-align: right;"">buy</th>
<th style=""text-align: left;"">random_str</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: right;"">0</td>
<td style=""text-align: left;"">2023-01-10</td>
<td style=""text-align: right;"">4</td>
<td style=""text-align: right;"">1</td>
<td style=""text-align: right;"">1</td>
<td style=""text-align: right;"">0</td>
<td style=""text-align: right;"">0</td>
<td style=""text-align: right;"">10</td>
<td style=""text-align: right;"">0</td>
<td style=""text-align: right;"">0</td>
<td style=""text-align: left;"">a</td>
</tr>
<tr>
<td style=""text-align: right;"">1</td>
<td style=""text-align: left;"">2023-01-20</td>
<td style=""text-align: right;"">4</td>
<td style=""text-align: right;"">1</td>
<td style=""text-align: right;"">2</td>
<td style=""text-align: right;"">0</td>
<td style=""text-align: right;"">0</td>
<td style=""text-align: right;"">20</td>
<td style=""text-align: right;"">nan</td>
<td style=""text-align: right;"">nan</td>
<td style=""text-align: left;"">a</td>
</tr>
<tr>
<td style=""text-align: right;"">2</td>
<td style=""text-align: left;"">2023-01-30</td>
<td style=""text-align: right;"">4</td>
<td style=""text-align: right;"">1</td>
<td style=""text-align: right;"">10</td>
<td style=""text-align: right;"">0</td>
<td style=""text-align: right;"">0</td>
<td style=""text-align: right;"">20</td>
<td style=""text-align: right;"">nan</td>
<td style=""text-align: right;"">nan</td>
<td style=""text-align: left;"">a</td>
</tr>
<tr>
<td style=""text-align: right;"">3</td>
<td style=""text-align: left;"">2023-01-10</td>
<td style=""text-align: right;"">4</td>
<td style=""text-align: right;"">2</td>
<td style=""text-align: right;"">2</td>
<td style=""text-align: right;"">0</td>
<td style=""text-align: right;"">0</td>
<td style=""text-align: right;"">10</td>
<td style=""text-align: right;"">0</td>
<td style=""text-align: right;"">0</td>
<td style=""text-align: left;"">b</td>
</tr>
<tr>
<td style=""text-align: right;"">4</td>
<td style=""text-align: left;"">2023-01-20</td>
<td style=""text-align: right;"">4</td>
<td style=""text-align: right;"">2</td>
<td style=""text-align: right;"">2</td>
<td style=""text-align: right;"">0</td>
<td style=""text-align: right;"">0</td>
<td style=""text-align: right;"">20</td>
<td style=""text-align: right;"">nan</td>
<td style=""text-align: right;"">nan</td>
<td style=""text-align: left;"">b</td>
</tr>
<tr>
<td style=""text-align: right;"">5</td>
<td style=""text-align: left;"">2023-01-30</td>
<td style=""text-align: right;"">4</td>
<td style=""text-align: right;"">2</td>
<td style=""text-align: right;"">0</td>
<td style=""text-align: right;"">0</td>
<td style=""text-align: right;"">0</td>
<td style=""text-align: right;"">20</td>
<td style=""text-align: right;"">nan</td>
<td style=""text-align: right;"">nan</td>
<td style=""text-align: left;"">b</td>
</tr>
</tbody>
</table>
</div>
<p>with 2 pairs <code>(4, 1)</code> and <code>(4,2)</code> over the days and my expected output (<code>results</code>) looks like this:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: right;""></th>
<th style=""text-align: left;"">date</th>
<th style=""text-align: right;"">category_a</th>
<th style=""text-align: right;"">category_b</th>
<th style=""text-align: right;"">outflow</th>
<th style=""text-align: right;"">open</th>
<th style=""text-align: right;"">inflow</th>
<th style=""text-align: right;"">max</th>
<th style=""text-align: right;"">close</th>
<th style=""text-align: right;"">buy</th>
<th style=""text-align: left;"">random_str</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: right;"">0</td>
<td style=""text-align: left;"">2023-01-10</td>
<td style=""text-align: right;"">4</td>
<td style=""text-align: right;"">1</td>
<td style=""text-align: right;"">1</td>
<td style=""text-align: right;"">0</td>
<td style=""text-align: right;"">0</td>
<td style=""text-align: right;"">10</td>
<td style=""text-align: right;"">-1</td>
<td style=""text-align: right;"">23</td>
<td style=""text-align: left;"">a</td>
</tr>
<tr>
<td style=""text-align: right;"">1</td>
<td style=""text-align: left;"">2023-01-20</td>
<td style=""text-align: right;"">4</td>
<td style=""text-align: right;"">1</td>
<td style=""text-align: right;"">2</td>
<td style=""text-align: right;"">-1</td>
<td style=""text-align: right;"">23</td>
<td style=""text-align: right;"">20</td>
<td style=""text-align: right;"">20</td>
<td style=""text-align: right;"">10</td>
<td style=""text-align: left;"">a</td>
</tr>
<tr>
<td style=""text-align: right;"">2</td>
<td style=""text-align: left;"">2023-01-30</td>
<td style=""text-align: right;"">4</td>
<td style=""text-align: right;"">1</td>
<td style=""text-align: right;"">10</td>
<td style=""text-align: right;"">20</td>
<td style=""text-align: right;"">10</td>
<td style=""text-align: right;"">20</td>
<td style=""text-align: right;"">20</td>
<td style=""text-align: right;"">nan</td>
<td style=""text-align: left;"">a</td>
</tr>
<tr>
<td style=""text-align: right;"">3</td>
<td style=""text-align: left;"">2023-01-10</td>
<td style=""text-align: right;"">4</td>
<td style=""text-align: right;"">2</td>
<td style=""text-align: right;"">2</td>
<td style=""text-align: right;"">0</td>
<td style=""text-align: right;"">0</td>
<td style=""text-align: right;"">10</td>
<td style=""text-align: right;"">-2</td>
<td style=""text-align: right;"">24</td>
<td style=""text-align: left;"">b</td>
</tr>
<tr>
<td style=""text-align: right;"">4</td>
<td style=""text-align: left;"">2023-01-20</td>
<td style=""text-align: right;"">4</td>
<td style=""text-align: right;"">2</td>
<td style=""text-align: right;"">2</td>
<td style=""text-align: right;"">-2</td>
<td style=""text-align: right;"">24</td>
<td style=""text-align: right;"">20</td>
<td style=""text-align: right;"">20</td>
<td style=""text-align: right;"">0</td>
<td style=""text-align: left;"">b</td>
</tr>
<tr>
<td style=""text-align: right;"">5</td>
<td style=""text-align: left;"">2023-01-30</td>
<td style=""text-align: right;"">4</td>
<td style=""text-align: right;"">2</td>
<td style=""text-align: right;"">0</td>
<td style=""text-align: right;"">20</td>
<td style=""text-align: right;"">0</td>
<td style=""text-align: right;"">20</td>
<td style=""text-align: right;"">20</td>
<td style=""text-align: right;"">nan</td>
<td style=""text-align: left;"">b</td>
</tr>
</tbody>
</table>
</div>
<p>I have a working solution using pandas dataframes to take a subset then loop over it to get a solution but I would like to see how I can improve the performance of this using perhaps ;<code>numpy</code>, <code>numba</code>, <code>pandas-multiprocessing</code> or <code>dask</code>. Another great idea was to rewrite it in BigQuery SQL.</p>
<p>I am not sure what the best solution would be and I would appreciate any help in improving the performance.</p>
<p><em><strong>Minimum working example</strong></em></p>
<p>The code below generates the input dataframe.</p>
<pre><code>import pandas as pd
import numpy as np

# prepare the input  df
df = pd.DataFrame({
'date' : ['2023-01-10', '2023-01-20','2023-01-30', '2023-01-10', '2023-01-20','2023-01-30'] ,
'category_a' : [4, 4,4,4, 4, 4] ,
'category_b' : [1, 1,1, 2, 2,2] ,
'outflow' : [1.0, 2.0,10.0, 2.0, 2.0, 0.0],
'open' : [0.0, 0.0, 0.0, 0.0, 0.0, 0.0] ,
'inflow' : [0.0, 0.0, 0.0, 0.0, 0.0, 0.0] ,
'max' : [10.0, 20.0, 20.0 , 10.0, 20.0,  20.0] ,
'close' : [0.0, np.nan,np.nan, 0.0, np.nan, np.nan] ,
'buy' : [0.0, np.nan,np.nan, 0.0, np.nan,np.nan],
'random_str' : ['a', 'a', 'a', 'b', 'b', 'b'] 
})

df['date'] = pd.to_datetime(df['date'])

# get unique pairs of category_a and category_b in a dictionary
unique_pairs = df.groupby(['category_a', 'category_b']).size().reset_index().rename(columns={0:'count'})[['category_a', 'category_b']].to_dict('records')
unique_dates = np.sort(df['date'].unique())
</code></pre>
<p>Using this input dataframe and Numpy, the code below is what I am trying to optmizize.</p>
<pre><code>df = df.set_index('date')
day_0 = unique_dates[0] # first date

# Using Dictionary comprehension

list_of_numbers = list(range(len(unique_pairs)))
myset  = {key: None for key in list_of_numbers}

for count_pair, value in enumerate(unique_pairs):
    
    # pair of category_a and category_b
    category_a = value['category_a']
    category_b = value['category_b']

    # subset the dataframe for the pair
    df_subset = df.loc[(df['category_a'] == category_a) &amp; (df['category_b'] == category_b)]

    log.info(f&quot; running for {category_a} and {category_b}&quot;)

    # day 0
    df_subset.loc[day_0, 'close'] = df_subset.loc[day_0, 'open'] + df_subset.loc[day_0, 'inflow'] - df_subset.loc[day_0, 'outflow']
    
    
    # loop over single pair using date
    for count, date in enumerate(unique_dates[1:], start=1):
        previous_date = unique_dates[count-1]

        df_subset.loc[date, 'open'] = df_subset.loc[previous_date, 'close']
        df_subset.loc[date, 'close'] = df_subset.loc[date, 'open'] + df_subset.loc[date, 'inflow'] - df_subset.loc[date, 'outflow']

        # check if closing value is negative, if so, set inflow to buy for next weeks deficit

        if df_subset.loc[date, 'close'] &lt; df_subset.loc[date, 'max']:
            df_subset.loc[previous_date, 'buy'] = df_subset.loc[date, 'max'] - df_subset.loc[date, 'close'] + df_subset.loc[date, 'inflow']
        elif df_subset.loc[date, 'close'] &gt; df_subset.loc[date, 'max']:
            df_subset.loc[previous_date, 'buy'] = 0
        else:
            df_subset.loc[previous_date, 'buy'] = df_subset.loc[date, 'inflow']
        
        df_subset.loc[date, 'inflow'] = df_subset.loc[previous_date, 'buy']
        df_subset.loc[date, 'close'] = df_subset.loc[date, 'open'] + df_subset.loc[date, 'inflow'] - df_subset.loc[date, 'outflow']
    
    # store all the dataframes in a container myset
    myset[count_pair] = df_subset
    
# make myset into a dataframe
result = pd.concat(myset.values()).reset_index(drop=False)
result

</code></pre>
<p>After which we can check that the solution is the same as what we expected.</p>
<pre><code>from pandas.testing import assert_frame_equal

expected = pd.DataFrame({
'date' : [pd.Timestamp('2023-01-10 00:00:00'), pd.Timestamp('2023-01-20 00:00:00'), pd.Timestamp('2023-01-30 00:00:00'), pd.Timestamp('2023-01-10 00:00:00'), pd.Timestamp('2023-01-20 00:00:00'), pd.Timestamp('2023-01-30 00:00:00')] ,
'category_a' : [4, 4, 4, 4, 4, 4] ,
'category_b' : [1, 1, 1, 2, 2, 2] ,
'outflow' : [1, 2, 10, 2, 2, 0] ,
'open' : [0.0, -1.0, 20.0, 0.0, -2.0, 20.0] ,
'inflow' : [0.0, 23.0, 10.0, 0.0, 24.0, 0.0] ,
'max' : [10, 20, 20, 10, 20, 20] ,
'close' : [-1.0, 20.0, 20.0, -2.0, 20.0, 20.0] ,
'buy' : [23.0, 10.0, np.nan, 24.0, 0.0, np.nan] ,
'random_str' : ['a', 'a', 'a', 'b', 'b', 'b'] 
})

# check that the result is the same as expected
assert_frame_equal(result, expected)
</code></pre>
<p><em><strong>SQL to create first table</strong></em></p>
<p>The solution can also be in sql, if so you can use the following code to create the initial table.</p>
<p>I am busy trying to implement a solution in big query sql using a user defined function to keep the logic going too. This would be a nice approach to solving the problem too.</p>
<pre><code>WITH data AS (
  SELECT 
    DATE '2023-01-10' as date, 4 as category_a, 1 as category_b, 1 as outflow, 0 as open, 0 as inflow, 10 as max, 0 as close, 0 as buy, 'a' as random_str
  UNION ALL
  SELECT 
    DATE '2023-01-20' as date, 4 as category_a, 1 as category_b, 2 as outflow, 0 as open, 0 as inflow, 20 as max, NULL as close, NULL as buy, 'a' as random_str
  UNION ALL
  SELECT 
    DATE '2023-01-30' as date, 4 as category_a, 1 as category_b, 10 as outflow, 0 as open, 0 as inflow, 20 as max, NULL as close, NULL as buy, 'a' as random_str
  UNION ALL
  SELECT 
    DATE '2023-01-10' as date, 4 as category_a, 2 as category_b, 2 as outflow, 0 as open, 0 as inflow, 10 as max, 0 as close, 0 as buy, 'b' as random_str
  UNION ALL
  SELECT 
    DATE '2023-01-20' as date, 4 as category_a, 2 as category_b, 2 as outflow, 0 as open, 0 as inflow, 20 as max, NULL as close, NULL as buy, 'b' as random_str
  UNION ALL
  SELECT 
    DATE '2023-01-30' as date, 4 as category_a, 2 as category_b, 0 as outflow, 0 as open, 0 as inflow, 20 as max, NULL as close, NULL as buy, 'b' as random_str
)

SELECT 
  ROW_NUMBER() OVER (ORDER BY date) as &quot; &quot;,
  date,
  category_a,
  category_b,
  outflow,
  open,
  inflow,
  max,
  close,
  buy,
  random_str
FROM data
</code></pre>
"
"<p>I created a new environment using conda and wanted to add it to jupyter-lab. I got a warning about frozen modules? (shown below)</p>
<pre class=""lang-none prettyprint-override""><code>$ ipython kernel install --user --name=testi2 
0.00s - Debugger warning: It seems that frozen modules are being used, which may
0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off
0.00s - to python to disable frozen modules.
0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.
Installed kernelspec testi2 in /home/michael/.local/share/jupyter/kernels/testi2
</code></pre>
<p>All I had installed were ipykernel, ipython, ipywidgets, jupyterlab_widgets, ipympl</p>
<p>Python Version 3.11.0, Conda version 22.11.0</p>
<p>And I used <code>conda install nodejs -c conda-forge --repodata-fn=repodata.json</code> to get the latest version of nodejs</p>
<p>I also tried re-installing ipykernel to a previous version (6.20.1 -&gt; 6.19.2)</p>
"
"<p>I've got a Django application with <code>djongo</code> as a database driver. The models are:</p>
<pre class=""lang-py prettyprint-override""><code>class Blog(models.Model):
    _id = models.ObjectIdField()
    name = models.CharField(max_length=100, db_column=&quot;Name&quot;)
    tagline = models.TextField()

class Entry(models.Model):
    _id = models.ObjectIdField()
    blog = models.EmbeddedField(
        model_container=Blog
    )
</code></pre>
<p>When I run this application, I got an error:</p>
<pre><code>File &quot;\.venv\lib\site-packages\djongo\models\fields.py&quot;, line 125, in _validate_container
    raise ValidationError(
django.core.exceptions.ValidationError: ['Field &quot;m.Blog.name&quot;  of model container:&quot;&lt;class \'project.m.models.Blog\'&gt;&quot; cannot be named as &quot;name&quot;, different from column name &quot;Name&quot;']
</code></pre>
<p>I want to keep the name of the field <code>name</code> in my model and database different because the database already exists, and I can't change it. The database uses camelCase for naming fields, whereas in the application, I want to use snake_case.</p>
<p>How to avoid this error?</p>
"
"<p>In python we can add lists to each other with the extend() method but it adds the second list at the end of the first list.</p>
<pre><code>lst1 = [1, 4, 5]
lst2 = [2, 3]

lst1.extend(lst2)

Output:
[1, 4, 5, 2, 3]
</code></pre>
<p>How would I add the second list to be apart of the 1st element? Such that the result is this;</p>
<pre><code>[1, 2, 3, 4, 5 ]
</code></pre>
<p>I've tried using <code>lst1.insert(1, *lst2)</code> and got an error;</p>
<pre><code>TypeError: insert expected 2 arguments, got 3
</code></pre>
"
"<p>How does mypy apply the Liskov substitution principle to <code>*args, **kwargs</code> parameters?</p>
<p>I thought the following code should fail a mypy check since some calls to <code>f</code> allowed by the <code>Base</code> class are not allowed by <code>C</code>, but it actually passed. Are there any reasons for this?</p>
<pre class=""lang-py prettyprint-override""><code>from abc import ABC, abstractmethod
from typing import Any


class Base(ABC):
    @abstractmethod
    def f(self, *args: Any, **kwargs: Any) -&gt; int:
        pass


class C(Base):
    def f(self, batch: int, train: bool) -&gt; int:
        return 1
</code></pre>
<p>I also tried to remove either <code>*args</code> or <code>**kwargs</code>, both failed.</p>
"
"<p>With the Firefox WebDriver I can read the local storage of my extension like so:</p>
<pre class=""lang-py prettyprint-override""><code>extension_path = &quot;/path/to/my/extension&quot;
info = {
    &quot;extension_id&quot;: f&quot;foobar&quot;,
    &quot;uuid&quot;: uuid.uuid4(),
}

base_url = f&quot;moz-extension://{info['uuid']}/&quot;

opts = FirefoxOptions()
opts.set_preference('extensions.webextensions.uuids', '{&quot;%s&quot;: &quot;%s&quot;}' % (
    info[&quot;extension_id&quot;], info[&quot;uuid&quot;]))
driver = webdriver.Firefox(options=opts)
driver.install_addon(extension_path, temporary=True)

driver.get(f&quot;{base_url}_generated_background_page.html&quot;)
results = self.driver.execute_async_script((
    &quot;let done = arguments[arguments.length - 1],&quot;
    &quot;  store_name = arguments[0];&quot;
    &quot;browser.storage.local.get([store_name], function (res) {&quot;
    &quot;  done(res[store_name]);&quot;
    &quot;});&quot;
), &quot;foo&quot;)
</code></pre>
<p>How can I do the same with the Safari WebDriver on macOS? I've ported the extension using <code>xcrun safari-web-extension-converter /path/to/my/extension</code> and built and manually tested that it works in Safari. In Safari I can go to <code>Develop -&gt; Web Extension Background Pages -&gt; &lt;my web extension&gt;</code> to find the id of the extension and see that a generated background page is located at <code>safari-web-extension://&lt;id&gt;/_generated_background_page.html</code></p>
<p>But running the following results in Selenium freezing at <code>driver.get(f&quot;{base_url}_generated_background_page.html&quot;)</code></p>
<pre class=""lang-py prettyprint-override""><code>base_url = f&quot;safari-web-extension://&lt;id&gt;/&quot;

driver = webdriver.Safari()
driver.get(f&quot;{base_url}_generated_background_page.html&quot;)
results = self.driver.execute_async_script((
    &quot;let done = arguments[arguments.length - 1],&quot;
    &quot;  store_name = arguments[0];&quot;
    &quot;browser.storage.local.get([store_name], function (res) {&quot;
    &quot;  done(res[store_name]);&quot;
    &quot;});&quot;
), &quot;foo&quot;)
</code></pre>
<p>What can I do?</p>
<h2>Update Feb 8th 2023</h2>
<p>I have also tried an approach using <code>browser.runtime.sendMessage</code> where in Python Selenium I do this:</p>
<pre><code>results = self.driver.execute_async_script((
    &quot;let done = arguments[arguments.length - 1],&quot;
    &quot;  store_name = arguments[0];&quot;
    &quot;  browser.runtime.sendMessage('com.oskar.foo.Extension (Apple Team ID)', {}, function (res) {&quot;
    &quot;    done(res[store_name]);&quot;
    &quot;  });&quot;
), &quot;foo&quot;)
</code></pre>
<p>and add the following to background.js in the extension:</p>
<pre><code>browser.runtime.onMessageExternal.addListener(function (
  request,
  sender,
  sendResponse
) {
  browser.storage.local.get(&quot;foo&quot;).then((j) =&gt; {
    sendResponse(j);
  });
  return true;
});
</code></pre>
<p>and this to the manifest.json</p>
<pre><code>&quot;externally_connectable&quot;: {
    &quot;ids&quot;: [&quot;*&quot;],
    &quot;matches&quot;: [&quot;https://example.org/*&quot;]
}
</code></pre>
<p>This way I actually get a value from the extension when running the test. But instead of reading the storage of the extension from the Safari instance started by Selenium, it reads the storage of the extension from the &quot;real&quot; safari instance.</p>
"
"<p>I had just upgraded to Ubuntu 22.04.1 LTS, which comes preinstalled with python3.10. I tried creating a virtual environment but it was unsuccessful. Trying to install the virtual env package gets an error <code>E: Package 'python3-venv' has no installation candidate</code></p>
<pre><code>python3 -m venv newpy310
The virtual environment was not created successfully because ensurepip is not
available.  On Debian/Ubuntu systems, you need to install the python3-venv
package using the following command.

    apt install python3.10-venv

You may need to use sudo with that command.  After installing the python3-venv
package, recreate your virtual environment.

Failing command: ['/home/user/Desktop/pyenvs/newpy310/bin/python3', '-Im', 'ensurepip', '--upgrade', '--default-pip']
</code></pre>
<p>Following which i used <code>sudo apt install python3.10-venv</code>, and was returned with</p>
<pre><code>sudo apt install python3.10-venv
Reading package lists... Done
Building dependency tree... Done
Reading state information... Done
Package python3.10-venv is not available, but is referred to by another package.
This may mean that the package is missing, has been obsoleted, or
is only available from another source

E: Package 'python3.10-venv' has no installation candidate
</code></pre>
<p>Something similar can be encountered if I used <code>sudo apt install python3.10-virtualenv</code></p>
<pre><code>sudo apt-get install python3.10-virtualenv
Reading package lists... Done
Building dependency tree... Done
Reading state information... Done
E: Unable to locate package python3.10-virtualenv
E: Couldn't find any package by glob 'python3.10-virtualenv'
E: Couldn't find any package by regex 'python3.10-virtualenv'
</code></pre>
<p>My sudo apt-get update also looks suspicious, but i am not entirely sure if it is the culprit</p>
<pre><code>sudo apt update
Hit:1 http://archive.ubuntu.com/ubuntu focal InRelease                                                 
Hit:2 http://security.ubuntu.com/ubuntu jammy-security InRelease                                       
Hit:3 http://archive.ubuntu.com/ubuntu focal-updates InRelease                                         
Hit:4 http://archive.ubuntu.com/ubuntu focal-security InRelease                  
Hit:5 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease
Hit:6 http://archive.ubuntu.com/ubuntu focal-backports InRelease
Hit:7 http://archive.ubuntu.com/ubuntu jammy InRelease
Hit:8 http://archive.ubuntu.com/ubuntu jammy-updates InRelease
Reading package lists... Done
Building dependency tree... Done
Reading state information... Done
8 packages can be upgraded. Run 'apt list --upgradable' to see them.
W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list:13 and /etc/apt/sources.list:14
W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list:13 and /etc/apt/sources.list:14
W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list:13 and /etc/apt/sources.list:14
W: Target Translations (main/i18n/Translation-en_US) is configured multiple times in /etc/apt/sources.list:13 and /etc/apt/sources.list:14
W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list:13 and /etc/apt/sources.list:14
W: Target Translations (main/i18n/Translation-en_SG) is configured multiple times in /etc/apt/sources.list:13 and /etc/apt/sources.list:14
W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list:13 and /etc/apt/sources.list:14
W: Target DEP-11 (main/dep11/Components-all.yml) is configured multiple times in /etc/apt/sources.list:13 and /etc/apt/sources.list:14
W: Target DEP-11-icons-small (main/dep11/icons-48x48.tar) is configured multiple times in /etc/apt/sources.list:13 and /etc/apt/sources.list:14
W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list:13 and /etc/apt/sources.list:14
W: Target DEP-11-icons-hidpi (main/dep11/icons-64x64@2.tar) is configured multiple times in /etc/apt/sources.list:13 and /etc/apt/sources.list:14
W: Target CNF (main/cnf/Commands-amd64) is configured multiple times in /etc/apt/sources.list:13 and /etc/apt/sources.list:14
W: Target CNF (main/cnf/Commands-all) is configured multiple times in /etc/apt/sources.list:13 and /etc/apt/sources.list:14
W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list:13 and /etc/apt/sources.list:14
W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list:13 and /etc/apt/sources.list:14
W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list:13 and /etc/apt/sources.list:14
W: Target Translations (main/i18n/Translation-en_US) is configured multiple times in /etc/apt/sources.list:13 and /etc/apt/sources.list:14
W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list:13 and /etc/apt/sources.list:14
W: Target Translations (main/i18n/Translation-en_SG) is configured multiple times in /etc/apt/sources.list:13 and /etc/apt/sources.list:14
W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list:13 and /etc/apt/sources.list:14
W: Target DEP-11 (main/dep11/Components-all.yml) is configured multiple times in /etc/apt/sources.list:13 and /etc/apt/sources.list:14
W: Target DEP-11-icons-small (main/dep11/icons-48x48.tar) is configured multiple times in /etc/apt/sources.list:13 and /etc/apt/sources.list:14
W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list:13 and /etc/apt/sources.list:14
W: Target DEP-11-icons-hidpi (main/dep11/icons-64x64@2.tar) is configured multiple times in /etc/apt/sources.list:13 and /etc/apt/sources.list:14
W: Target CNF (main/cnf/Commands-amd64) is configured multiple times in /etc/apt/sources.list:13 and /etc/apt/sources.list:14
W: Target CNF (main/cnf/Commands-all) is configured multiple times in /etc/apt/sources.list:13 and /etc/apt/sources.list:14
</code></pre>
<p>I also have already added the deadsnakes ppa repos. I noticed some other questions were for python 3.6 or 3.8, which had worked in the past when i was using python 3.6 and 3.8 respectively. However, the methods described within does not work for my current setup with Ubuntu 22.04 and python3.10.</p>
<ol>
<li><a href=""https://stackoverflow.com/questions/62773433/python3-8-venv-not-working-with-python3-8-m-venv-env"">python3.8-venv not working with python3.8 -m venv env</a></li>
<li><a href=""https://stackoverflow.com/questions/68211364/python3-8-venv-is-no-longer-working-after-pop-os-upgraded-to-21-04"">python3.8-venv is no longer working after Pop OS upgraded to 21.04</a></li>
<li><a href=""https://stackoverflow.com/questions/74368514/trouble-installing-python3-6-virtual-environment-on-ubuntu-22-04"">Trouble Installing Python3.6 Virtual Environment on Ubuntu 22.04</a></li>
</ol>
<p>These are the other links that I have consulted but did not work for me. I have also tried reinstalling python3.10 itself.</p>
<ol>
<li><a href=""https://stackoverflow.com/questions/69830431/how-to-use-python3-10-on-ubuntu"">How to use Python3.10 on Ubuntu?</a></li>
<li><a href=""https://stackoverflow.com/questions/74368514/trouble-installing-python3-6-virtual-environment-on-ubuntu-22-04"">Trouble Installing Python3.6 Virtual Environment on Ubuntu 22.04</a></li>
<li><a href=""https://stackoverflow.com/questions/39539110/pyvenv-not-working-because-ensurepip-is-not-available"">pyvenv not working because ensurepip is not available</a></li>
<li><a href=""https://stackoverflow.com/questions/39539110/pyvenv-not-working-because-ensurepip-is-not-available?noredirect=1&amp;lq=1"">pyvenv not working because ensurepip is not available</a></li>
<li><a href=""https://askubuntu.com/questions/879437/ensurepip-is-disabled-in-debian-ubuntu-for-the-system-python"">https://askubuntu.com/questions/879437/ensurepip-is-disabled-in-debian-ubuntu-for-the-system-python</a></li>
<li><a href=""https://stackoverflow.com/questions/71818928/python3-10-source-venv-has-changed"">Python3.10 source venv has changed</a></li>
</ol>
<p>Q: How to install python3.10 virtual environment when python3.10-venv has no installation candidate?</p>
"
"<p>Tried to train the model but got the mmcv error</p>
<pre><code>No module named 'mmcv._ext'
</code></pre>
<p>mmcv library is already installed and imported</p>
<pre><code>mmcv version = 1.4.0
Cuda version = 10.0

</code></pre>
<p>Any suggestions to fix the issue??</p>
"
"<h2>Context</h2>
<p>I am the author and maintainer of <a href=""https://github.com/paulbrodersen/netgraph"" rel=""nofollow noreferrer"">netgraph</a>, a python library for creating network visualisations.
I am currently trying to optimise a routine that computes a set of <code>N</code> node positions for networks in which each edge has a defined length.
An example can be found <a href=""https://stackoverflow.com/a/75083544/2912349"">here</a>.</p>
<h2>Problem</h2>
<p>At its core, the routine runs <code>scipy.optimize.minimize</code> to compute the positions that maximise the total distance between nodes:</p>
<pre><code>def cost_function(positions):
    return 1. / np.sum((pdist(positions.reshape((-1, 2))))**power)

result = minimize(cost_function, initial_positions.flatten(), method='SLSQP',
                  jac=&quot;2-point&quot;, constraints=[nonlinear_constraint])
</code></pre>
<ul>
<li><code>positions</code> are an (unravelled) numpy array of (x, y) tuples.</li>
<li><code>power</code> is a smallish number that limits the influence of large distances (to encourage compact node layouts) but for the purpose of this question could be assumed to be 1.</li>
<li><code>pdist</code> is the pairwise distance function in <code>scipy.spatial</code>.</li>
</ul>
<p>The minimisation ( / maximisation) is constrained using the following non-linear constraint:</p>
<pre><code>lower_bounds = ... # (squareform of an) (N, N) distance matrix of the sum of node sizes (i.e. nodes should not overlap)
upper_bounds = ... # (squareform of an) (N, N) distance matrix constructed from the given edge lengths

def constraint_function(positions):
    positions = np.reshape(positions, (-1, 2))
    return pdist(positions)

nonlinear_constraint = NonlinearConstraint(constraint_function, lb=lower_bounds, ub=upper_bounds, jac='2-point')
</code></pre>
<p>For toy examples, the optimisation completes correctly and quickly. However, even for smallish networks, the running time is fairly abysmal.
My current implementation uses finite differences to approximate the gradients (<code>jac='2-point'</code>).
To speed up the computation, I would like to compute the Jacobians explicitly.</p>
<p>Following several Math Stackexchange posts (<a href=""https://math.stackexchange.com/questions/4212391/is-the-hessian-matrix-of-euclidean-norm-function-positive-definite"">1</a>, <a href=""https://math.stackexchange.com/questions/1552653/jacobian-of-second-norm"">2</a>), I computed the the Jacobian of the pairwise distance function as follows:</p>
<pre><code>    def delta_constraint(positions):
        positions = np.reshape(positions, (-1, 2))
        total_positions = positions.shape[0]
        delta = positions[np.newaxis, :, :] - positions[:, np.newaxis, :]
        distance = np.sqrt(np.sum(delta ** 2, axis=-1))
        jac = delta / distance[:, :, np.newaxis]
        squareform_indices = np.triu_indices(total_positions, 1)
        return jac[squareform_indices]

nonlinear_constraint = NonlinearConstraint(constraint_function, lb=lower_bounds, ub=upper_bounds, jac=delta_constraint)
</code></pre>
<p>However, this results in a <code>ValueError</code>, as the shape of the output is incorrect. For the triangle example, the expected output shape is (3, 6), whereas the function above returns a (3, 2) array (i.e. 3 pairwise distance by 2 dimensions). For the square the expected output is (6, 8), whereas the actual is (6, 2). Any help deriving implementing the correct callable(s) for the <code>jac</code> arguments to <code>NonlinearConstraint</code> and <code>minimize</code> would be appreciated.</p>
<h2>Note</h2>
<p>I would like to avoid the use of autograd/jax/numdifftools (as in <a href=""https://stackoverflow.com/questions/41137092/jacobian-and-hessian-inputs-in-scipy-optimize-minimize"">this question</a>), as I would like to keep the number of dependencies of my library small.</p>
<h2>Minimal working example(s)</h2>
<pre><code>#!/usr/bin/env python
&quot;&quot;&quot;
Create a node layout with fixed edge lengths but unknown node positions.
&quot;&quot;&quot;

import numpy as np

from scipy.optimize import minimize, NonlinearConstraint
from scipy.spatial.distance import pdist, squareform


def get_geometric_node_layout(edges, edge_length, node_size=0., power=0.2, maximum_iterations=200, origin=(0, 0), scale=(1, 1)):
    &quot;&quot;&quot;Node layout for defined edge lengths but unknown node positions.

    Node positions are determined through non-linear optimisation: the
    total distance between nodes is maximised subject to the constraint
    imposed by the edge lengths, which are used as upper bounds.
    If provided, node sizes are used to set lower bounds.

    Parameters
    ----------
    edges : list
        The edges of the graph, with each edge being represented by a (source node ID, target node ID) tuple.
    edge_lengths : dict
        Mapping of edges to their lengths.
    node_size : scalar or dict, default 0.
        Size (radius) of nodes.
        Providing the correct node size minimises the overlap of nodes in the graph,
        which can otherwise occur if there are many nodes, or if the nodes differ considerably in size.
    power : float, default 0.2.
        The cost being minimised is the inverse of the sum of distances.
        The power parameter is the exponent applied to each distance before summation.
        Large values result in positions that are stretched along one axis.
        Small values decrease the influence of long distances on the cost
        and promote a more compact layout.
    maximum_iterations : int
        Maximum number of iterations of the minimisation.
    origin : tuple, default (0, 0)
        The (float x, float y) coordinates corresponding to the lower left hand corner of the bounding box specifying the extent of the canvas.
    scale : tuple, default (1, 1)
        The (float x, float y) dimensions representing the width and height of the bounding box specifying the extent of the canvas.

    Returns
    -------
    node_positions : dict
        Dictionary mapping each node ID to (float x, float y) tuple, the node position.

    &quot;&quot;&quot;
    # TODO: assert triangle inequality

    # TODO: assert that the edges fit within the canvas dimensions

    # ensure that graph is bi-directional
    edges = edges + [(target, source) for (source, target) in edges] # forces copy
    edges = list(set(edges))

    # upper bound: pairwise distance matrix with unknown distances set to the maximum possible distance given the canvas dimensions
    lengths = []
    for (source, target) in edges:
        if (source, target) in edge_length:
            lengths.append(edge_length[(source, target)])
        else:
            lengths.append(edge_length[(target, source)])

    sources, targets = zip(*edges)
    nodes = sources + targets
    unique_nodes = set(nodes)
    indices = range(len(unique_nodes))
    node_to_idx = dict(zip(unique_nodes, indices))
    source_indices = [node_to_idx[source] for source in sources]
    target_indices = [node_to_idx[target] for target in targets]

    total_nodes = len(unique_nodes)
    max_distance = np.sqrt(scale[0]**2 + scale[1]**2)
    distance_matrix = np.full((total_nodes, total_nodes), max_distance)
    distance_matrix[source_indices, target_indices] = lengths
    distance_matrix[np.diag_indices(total_nodes)] = 0
    upper_bounds = squareform(distance_matrix)

    # lower bound: sum of node sizes
    if isinstance(node_size, (int, float)):
        sizes = node_size * np.ones((total_nodes))
    elif isinstance(node_size, dict):
        sizes = np.array([node_size[node] if node in node_size else 0. for node in unique_nodes])

    sum_of_node_sizes = sizes[np.newaxis, :] + sizes[:, np.newaxis]
    sum_of_node_sizes -= np.diag(np.diag(sum_of_node_sizes)) # squareform requires zeros on diagonal
    lower_bounds = squareform(sum_of_node_sizes)

    def cost_function(positions):
        return 1. / np.sum((pdist(positions.reshape((-1, 2))))**power)

    def constraint_function(positions):
        positions = np.reshape(positions, (-1, 2))
        return pdist(positions)

    initial_positions = _initialise_geometric_node_layout(edges)
    nonlinear_constraint = NonlinearConstraint(constraint_function, lb=lower_bounds, ub=upper_bounds, jac='2-point')
    result = minimize(cost_function, initial_positions.flatten(), method='SLSQP',
                      jac=&quot;2-point&quot;, constraints=[nonlinear_constraint], options=dict(maxiter=maximum_iterations))

    if not result.success:
        print(&quot;Warning: could not compute valid node positions for the given edge lengths.&quot;)
        print(f&quot;scipy.optimize.minimize: {result.message}.&quot;)

    node_positions_as_array = result.x.reshape((-1, 2))
    node_positions = dict(zip(unique_nodes, node_positions_as_array))
    return node_positions


def _initialise_geometric_node_layout(edges):
    sources, targets = zip(*edges)
    total_nodes = len(set(sources + targets))
    return np.random.rand(total_nodes, 2)


if __name__ == '__main__':

    import matplotlib.pyplot as plt

    def plot_graph(edges, node_layout):
        # poor man's graph plotting
        fig, ax = plt.subplots()
        for source, target in edges:
            x1, y1 = node_layout[source]
            x2, y2 = node_layout[target]
            ax.plot([x1, x2], [y1, y2], color='darkgray')
        ax.set_aspect('equal')

    ################################################################################
    # triangle with right angle

    edges = [
        (0, 1),
        (1, 2),
        (2, 0)
    ]

    lengths = {
        (0, 1) : 3,
        (1, 2) : 4,
        (2, 0) : 5,
    }

    pos = get_geometric_node_layout(edges, lengths, node_size=0)

    plot_graph(edges, node_layout=pos)

    plt.show()

    ################################################################################
    # square

    edges = [
        (0, 1),
        (1, 2),
        (2, 3),
        (3, 0),
    ]

    lengths = {
        (0, 1) : 0.5,
        (1, 2) : 0.5,
        (2, 3) : 0.5,
        (3, 0) : 0.5,
    }

    pos = get_geometric_node_layout(edges, lengths, node_size=0)

    plot_graph(edges, node_layout=pos)

    plt.show()
</code></pre>
<h2>Edit: Realistic use case for timing</h2>
<p>Below is a more realistic use case that I am using to time my code.
I have incorporated @adrianop01's computation of the Jacobian for the constraint. It also includes a superior initialisation. It requires the additional dependencies <code>networkx</code> and <code>netgraph</code>, both of which can be installed via pip.</p>
<pre><code>#!/usr/bin/env python
&quot;&quot;&quot;
Create a node layout with fixed edge lengths but unknown node positions.
&quot;&quot;&quot;

import numpy as np

from itertools import combinations
from scipy.optimize import minimize, NonlinearConstraint
from scipy.spatial.distance import pdist, squareform

from netgraph._node_layout import _rescale_to_frame


def get_geometric_node_layout(edges, edge_length, node_size=0., power=0.2, maximum_iterations=200, origin=(0, 0), scale=(1, 1)):
    &quot;&quot;&quot;Node layout for defined edge lengths but unknown node positions.

    Node positions are determined through non-linear optimisation: the
    total distance between nodes is maximised subject to the constraint
    imposed by the edge lengths, which are used as upper bounds.
    If provided, node sizes are used to set lower bounds.

    Parameters
    ----------
    edges : list
        The edges of the graph, with each edge being represented by a (source node ID, target node ID) tuple.
    edge_lengths : dict
        Mapping of edges to their lengths.
    node_size : scalar or dict, default 0.
        Size (radius) of nodes.
        Providing the correct node size minimises the overlap of nodes in the graph,
        which can otherwise occur if there are many nodes, or if the nodes differ considerably in size.
    power : float, default 0.2.
        The cost being minimised is the inverse of the sum of distances.
        The power parameter is the exponent applied to each distance before summation.
        Large values result in positions that are stretched along one axis.
        Small values decrease the influence of long distances on the cost
        and promote a more compact layout.
    maximum_iterations : int
        Maximum number of iterations of the minimisation.
    origin : tuple, default (0, 0)
        The (float x, float y) coordinates corresponding to the lower left hand corner of the bounding box specifying the extent of the canvas.
    scale : tuple, default (1, 1)
        The (float x, float y) dimensions representing the width and height of the bounding box specifying the extent of the canvas.

    Returns
    -------
    node_positions : dict
        Dictionary mapping each node ID to (float x, float y) tuple, the node position.

    &quot;&quot;&quot;
    # TODO: assert triangle inequality

    # TODO: assert that the edges fit within the canvas dimensions

    # ensure that graph is bi-directional
    edges = edges + [(target, source) for (source, target) in edges] # forces copy
    edges = list(set(edges))

    # upper bound: pairwise distance matrix with unknown distances set to the maximum possible distance given the canvas dimensions
    lengths = []
    for (source, target) in edges:
        if (source, target) in edge_length:
            lengths.append(edge_length[(source, target)])
        else:
            lengths.append(edge_length[(target, source)])

    sources, targets = zip(*edges)
    nodes = sources + targets
    unique_nodes = set(nodes)
    indices = range(len(unique_nodes))
    node_to_idx = dict(zip(unique_nodes, indices))
    source_indices = [node_to_idx[source] for source in sources]
    target_indices = [node_to_idx[target] for target in targets]

    total_nodes = len(unique_nodes)
    max_distance = np.sqrt(scale[0]**2 + scale[1]**2)
    distance_matrix = np.full((total_nodes, total_nodes), max_distance)
    distance_matrix[source_indices, target_indices] = lengths
    distance_matrix[np.diag_indices(total_nodes)] = 0
    upper_bounds = squareform(distance_matrix)

    # lower bound: sum of node sizes
    if isinstance(node_size, (int, float)):
        sizes = node_size * np.ones((total_nodes))
    elif isinstance(node_size, dict):
        sizes = np.array([node_size[node] if node in node_size else 0. for node in unique_nodes])

    sum_of_node_sizes = sizes[np.newaxis, :] + sizes[:, np.newaxis]
    sum_of_node_sizes -= np.diag(np.diag(sum_of_node_sizes)) # squareform requires zeros on diagonal
    lower_bounds = squareform(sum_of_node_sizes)
    invalid = lower_bounds &gt; upper_bounds
    lower_bounds[invalid] = upper_bounds[invalid] - 1e-8

    def cost_function(positions):
        # return -np.sum((pdist(positions.reshape((-1, 2))))**power)
        return 1. / np.sum((pdist(positions.reshape((-1, 2))))**power)

    def cost_jacobian(positions):
        # TODO
        pass

    def constraint_function(positions):
        positions = np.reshape(positions, (-1, 2))
        return pdist(positions)

    # adapted from https://stackoverflow.com/a/75154395/2912349
    total_pairs = int((total_nodes - 1) * total_nodes / 2)
    source_indices, target_indices = np.array(list(combinations(range(total_nodes), 2))).T # node order thus (0,1) ... (0,N-1), (1,2),...(1,N-1),...,(N-2,N-1)
    rows = np.repeat(np.arange(total_pairs).reshape(-1, 1), 2, axis=1)
    source_columns = np.vstack((source_indices*2, source_indices*2+1)).T
    target_columns = np.vstack((target_indices*2, target_indices*2+1)).T

    def constraint_jacobian(positions):
        positions = np.reshape(positions, (-1, 2))
        pairwise_distances = constraint_function(positions)
        jac = np.zeros((total_pairs, 2 * total_nodes))
        jac[rows, source_columns] = (positions[source_indices] - positions[target_indices]) / pairwise_distances.reshape((-1, 1))
        jac[rows, target_columns] = -jac[rows, source_columns]
        return jac

    initial_positions = _initialise_geometric_node_layout(edges, edge_length)
    nonlinear_constraint = NonlinearConstraint(constraint_function, lb=lower_bounds, ub=upper_bounds, jac=constraint_jacobian)
    result = minimize(cost_function, initial_positions.flatten(), method='SLSQP',
                      jac='2-point', constraints=[nonlinear_constraint], options=dict(maxiter=maximum_iterations))
    # result = minimize(cost_function, initial_positions.flatten(), method='trust-constr',
    #                   jac=cost_jacobian, constraints=[nonlinear_constraint])

    if not result.success:
        print(&quot;Warning: could not compute valid node positions for the given edge lengths.&quot;)
        print(f&quot;scipy.optimize.minimize: {result.message}.&quot;)

    node_positions_as_array = result.x.reshape((-1, 2))
    node_positions_as_array = _rescale_to_frame(node_positions_as_array, np.array(origin), np.array(scale))
    node_positions = dict(zip(unique_nodes, node_positions_as_array))
    return node_positions


# # slow
# def _initialise_geometric_node_layout(edges, edge_length=None):
#     sources, targets = zip(*edges)
#     total_nodes = len(set(sources + targets))
#     return np.random.rand(total_nodes, 2)

# much faster
def _initialise_geometric_node_layout(edges, edge_length=None):
    &quot;&quot;&quot;Initialises the node positions using the FR algorithm with weights.
    Shorter edges are given a larger weight such that the nodes experience a strong attractive force.&quot;&quot;&quot;

    from netgraph import get_fruchterman_reingold_layout
    if edge_length:
        edge_weight = dict()
        for edge, length in edge_length.items():
            edge_weight[edge] = 1 / length
    else:
        edge_weight = None
    node_positions = get_fruchterman_reingold_layout(edges)
    return np.array(list(node_positions.values()))


if __name__ == '__main__':

    from time import time
    import matplotlib.pyplot as plt
    import networkx as nx # pip install networkx

    from netgraph import Graph # pip install netgraph


    fig, (ax1, ax2) = plt.subplots(1, 2)

    g = nx.random_geometric_graph(50, 0.3, seed=2)
    node_positions = nx.get_node_attributes(g, 'pos')
    plot_instance = Graph(g,
                          node_layout=node_positions,
                          node_size=1, # netgraph rescales node sizes by 0.01
                          node_edge_width=0.1,
                          edge_width=0.1,
                          ax=ax1,
    )
    ax1.axis([0, 1, 0, 1])
    ax1.set_title('Original node positions')

    def get_euclidean_distance(p1, p2):
        return np.sqrt(np.sum((np.array(p1)-np.array(p2))**2))

    edge_length = dict()
    for (source, target) in g.edges:
        edge_length[(source, target)] = get_euclidean_distance(node_positions[source], node_positions[target])

    tic = time()
    new_node_positions = get_geometric_node_layout(list(g.edges), edge_length, node_size=0.01)
    toc = time()

    print(f&quot;Time elapsed : {toc-tic}&quot;)

    Graph(g,
          node_layout=new_node_positions,
          node_size=1,
          node_edge_width=0.1,
          edge_width=0.1,
          ax=ax2,
    )
    ax2.axis([0, 1, 0, 1])
    ax2.set_title('Reconstructed node positions')

    plt.show()
</code></pre>
<hr />
<h2>2nd edit</h2>
<p>Here are some preliminary results I obtained when testing @spinkus' and related solutions.
My implementation of his code looks like this:</p>
<pre><code>def cost_function(positions):
    return -np.sum((pdist(positions.reshape((-1, 2))))**2)

def cost_jacobian(positions):
    positions = positions.reshape(-1, 2)
    delta = positions[np.newaxis, :] - positions[:, np.newaxis]
    jac = -2 * np.sum(delta, axis=0)
    return jac.ravel()
</code></pre>
<p>Unfortunately, this cost function takes significantly longer to converge: 13 seconds in a best of 5 with a large variance in the timings (up to a minute).
This is independent of whether I use the explicit Jacobian or approximate it using the finite difference approach.
Furthermore, the minimisation often ends prematurely with &quot;scipy.optimize.minimize: Inequality constraints incompatible.&quot; and &quot;scipy.optimize.minimize: Positive directional derivative for linesearch.&quot;
My bet (although I have little evidence to back it up) is that absolute value of the cost matters. My original cost function decreases both in value and in absolute value, whereas the minimisation increases the absolute value of the @spinkus cost function (however, see @spinkus' excellent comment below why that might be somewhat a red herring and lead to less accurate solutions).</p>
<p>I also understood (I think) why my original cost function is not amenable to computing the Jacobians.
Let <code>power</code> be 0.5, then the cost function and Jacobian take this form (unless my algebra is wrong again):</p>
<pre><code>def cost_function(positions):
    return 1. / np.sum((pdist(positions.reshape((-1, 2))))**0.5)

def cost_jacobian(positions):
    positions = positions.reshape(-1, 2)
    delta = positions[np.newaxis, :] - positions[:, np.newaxis]
    distance = np.sqrt(np.sum(delta**2, axis=-1))
    denominator = -2 * np.sqrt(delta) * distance[:, :, np.newaxis]
    denominator[np.diag_indices_from(denominator[:, :, 0]),:] = 1
    jac = 1 / denominator
    return np.sum(jac, axis=0).ravel() - 1
</code></pre>
<p>The problematic term is the <code>sqrt(delta)</code>, where <code>delta</code> are the vectors between all points.
Ignoring the diagonals, half of the entries in this matrix are necessarily negative and thus the Jacobian cannot be computed.</p>
<p>However, the purpose of the power is simply to decrease the importance of large distances on the cost.
Any monotonically increasing function with decreasing derivative will do.
Using <code>log(x + 1)</code> instead of the power results in these functions:</p>
<pre><code>def cost_function(positions):
    return 1 / np.sum(np.log(pdist(positions.reshape((-1, 2))) + 1))

def cost_jacobian(positions):
    positions = positions.reshape(-1, 2)
    delta = positions[np.newaxis, :] - positions[:, np.newaxis]
    distance2 = np.sum(delta**2, axis=-1)
    distance2[np.diag_indices_from(distance2)] = 1
    jac = -delta / (distance2 + np.sqrt(distance2))[..., np.newaxis]
    return np.sum(jac, axis=0).ravel()
</code></pre>
<p>With the finite difference approximation, the minimisation terminates in 0.5 seconds.
However, with the explicit Jacobian, the best run times were 4 seconds albeit still with a very large variance with run times up a minute and longer.</p>
<h4>Tl;dr.</h4>
<p>I still don't understand why the minimisation does not run faster with explicit Jacobians.</p>
"
"<p>I know that someone will face this problem. I had this problem today, but I could fix it promptly, and I want to share my solution:</p>
<p>Problem:</p>
<pre><code>from flask_socketio import SocketIO
</code></pre>
<p>You will receive an output error with something like:</p>
<blockquote>
<p>Attribute Error: module &quot;dns.rdtypes&quot; has no attribute ANY</p>
</blockquote>
<p>This only happens if you have installed eventlet, because it install dnspython with it.</p>
<p>The solution is simple, just reinstall dnspython for previous realease:</p>
<blockquote>
<p>python3 -m pip install dnspython==2.2.1</p>
</blockquote>
<p>The problem should disappear</p>
"
"<p>I have a problem starting Playwright in Python maximized. I found some articles for other languages but doesn't work in Python, also nothing is written about maximizing window in Python in the official documentation.</p>
<p>I tried <code>browser = p.chromium.launch(headless=False, args=[&quot;--start-maximized&quot;])</code></p>
<p>And it starts maximized but then automatically restores back to the default small window size.</p>
<p>Any ideas?
Thanks</p>
"
"<p>Goal: store a <code>dict()</code> or <code>{}</code> as the value for a key-value pair, to <code>set()</code> onto <strong>Redis</strong>.</p>
<p>Code</p>
<pre><code>import redis

r = redis.Redis()

value = 180

my_dict = dict(bar=value)

r.set('foo', my_dict)
</code></pre>
<pre><code>redis.exceptions.DataError: Invalid input of type: 'dict'. Convert to a bytes, string, int or float first.
</code></pre>
"
"<p>I have the following project structure:</p>
<pre><code>root
  - sample/
    - src/
    - tests/
    - pyproject.toml
  - libs/
    - lol/
      - src/
      - tests/
      - pyproject.toml
</code></pre>
<p>I'd like to specify <code>lol</code> as a dependency for <code>sample</code> in <code>sample/pyproject.toml</code>. How it can be done? I've tried:</p>
<pre><code>dependencies = [ 
   &quot;lol @ file://libs/lol&quot;
]
</code></pre>
<p>But it gives me:</p>
<pre><code>ValueError: non-local file URIs are not supported on this platform: 'file://libs/lol'
</code></pre>
<p>and that's ok however I cannot put absolute path here since this is going to be shared code. Same for <code>file://./lib/lol</code>.</p>
<p>What can be done about that? Can I use env variables here, or some placeholders? I don't want to use tools like poetry.</p>
"
"<p>I want to use <code>debian:bullseye</code> as a base image and then install a specific Python version - i.e. 3.11.1.  At the moment I am just learning docker and linux.</p>
<p>From what I understand I can either:</p>
<ol>
<li>Download and compile sources</li>
<li>Install binaries (using apt-get)</li>
<li>Use a Python base image</li>
</ol>
<p>I have come across countless questions on here and articles online.  Do I use <a href=""https://launchpad.net/%7Edeadsnakes/+archive/ubuntu/nightly/+packages"" rel=""noreferrer"">deadsnakes</a>?  What version do I need?  Are there any official python distributions (<a href=""https://askubuntu.com/questions/1398568/installing-python-who-is-deadsnakes-and-why-should-i-trust-them"">who is deadsnakes anyway</a>)?</p>
<p>But ultimately I want to know the best means of getting Python on there.  I don't want to use a Python base image - I am curious in the steps involved.  Compile sources - I am far from having that level of knowhow - and one for another day.</p>
<p>Currently I am rolling with the following:</p>
<pre><code>FROM debian:bullseye

RUN apt update &amp;&amp; apt upgrade -y
RUN apt install software-properties-common -y
RUN add-apt-repository &quot;ppa:deadsnakes/ppa&quot;
RUN apt install python3.11
</code></pre>
<p>This fails with:</p>
<pre><code>#8 1.546 E: Unable to locate package python3.11
#8 1.546 E: Couldn't find any package by glob 'python3.11'
</code></pre>
<p>Ultimately - it's not the error - its just finding a good way of getting a specific Python version on my container.</p>
"
"<p>I have a list of dictionaries like this:</p>
<pre><code>[{&quot;id&quot;: 1, &quot;name&quot;: &quot;Joe&quot;, &quot;lastname&quot;: &quot;Bloggs&quot;}, {&quot;id&quot;: 2, &quot;name&quot;: &quot;Bob&quot;, &quot;lastname&quot;: &quot;Wilson&quot;}]
</code></pre>
<p>And I would like to transform it to a polars dataframe. I've tried going via pandas but if possible, I'd like to avoid using pandas.</p>
<p>Any thoughts?</p>
"
"<p>Relevant portion of my code looks something like this:</p>
<pre><code>@directory_router.get(&quot;/youtube-dl/{relative_path:path}&quot;, tags=[&quot;directory&quot;])
def youtube_dl(relative_path, url, name=&quot;&quot;):
    &quot;&quot;&quot;
    Download
    &quot;&quot;&quot;

    relative_path, _ = set_path(relative_path)

    logger.info(f&quot;{DATA_PATH}{relative_path}&quot;)

    if name:
        name = f&quot;{DATA_PATH}{relative_path}/{name}.%(ext)s&quot;
    else:
        name = f&quot;{DATA_PATH}{relative_path}/%(title)s.%(ext)s&quot;

    ydl_opts = {
        &quot;outtmpl&quot;: name,
        # &quot;quiet&quot;: True
        &quot;logger&quot;: logger,
        &quot;progress_hooks&quot;: [yt_dlp_hook],
        # &quot;force-overwrites&quot;: True
    }

    with yt.YoutubeDL(ydl_opts) as ydl:
        try:
            ydl.download([url])
        except Exception as exp:
            logger.info(exp)
            return str(exp)
</code></pre>
<p>I am using this webhook/end point to allow an angular app to accept url/name input and download file to folder. I am able to logger.info .. etc. output the values of the yt_dlp_hook, something like this:</p>
<pre><code>def yt_dlp_hook(download):
    &quot;&quot;&quot;
    download Hook

    Args:
        download (_type_): _description_
    &quot;&quot;&quot;

    global TMP_KEYS

    if download.keys() != TMP_KEYS:
        logger.info(f'Status: {download[&quot;status&quot;]}')
        logger.info(f'Dict Keys: {download.keys()}')
        TMP_KEYS = download.keys()
        logger.info(download)
</code></pre>
<p>Is there a way to stream a string of relevant variables like ETA, download speed etc. etc. to the front end? Is there a better way to do this?</p>
"
"<p>I was thinking about using <code>polars</code> in place of <code>numpy</code> in a parsing problem where I turn a structured text file into a character table and operate on different columns. However, it seems that <code>polars</code> is about 5 times slower than <code>numpy</code> in most operations I'm performing. I was wondering why that's the case and whether I'm doing something wrong given that <code>polars</code> is supposed to be faster.</p>
<p>Example:</p>
<pre class=""lang-py prettyprint-override""><code>import requests
import numpy as np
import polars as pl

# Download the text file
text = requests.get(&quot;https://files.rcsb.org/download/3w32.pdb&quot;).text

# Turn it into a 2D array of characters
char_tab_np = np.array(file.splitlines()).view(dtype=(str,1)).reshape(-1, 80)

# Create a polars DataFrame from the numpy array
char_tab_pl = pl.DataFrame(char_tab_np)

# Sort by first column with numpy
char_tab_np[np.argsort(char_tab_np[:,0])]

# Sort by first column with polars
char_tab_pl.sort(by=&quot;column_0&quot;)
</code></pre>
<p>Using <code>%%timeit</code> in <code>Jupyter</code>, the <code>numpy</code> sorting takes about <strong>320 microseconds</strong>, whereas the <code>polars</code> sort takes about <strong>1.3 milliseconds</strong>, i.e. about five times slower.</p>
<p>I also tried <code>char_tab_pl.lazy().sort(by=&quot;column_0&quot;).collect()</code>, but it had no effect on the duration.</p>
<p>Another example (Take all rows where the first column is equal to 'A'):</p>
<pre class=""lang-py prettyprint-override""><code># with numpy
%%timeit
char_tab_np[char_tab_np[:, 0] == &quot;A&quot;]
</code></pre>
<pre class=""lang-py prettyprint-override""><code># with polars
%%timeit
char_tab_pl.filter(pl.col(&quot;column_0&quot;) == &quot;A&quot;)
</code></pre>
<p>Again, <code>numpy</code> takes 226 microseconds, whereas <code>polars</code> takes 673 microseconds, about three times slower.</p>
<h2>Update</h2>
<p>Based on the comments I tried two other things:</p>
<p><strong>1. Making the file 1000 times larger to see whether polars performs better on larger data.</strong></p>
<p>Results: <code>numpy</code> was still about 2 times faster (1.3 ms vs. 2.1 ms). In addition, creating the character array took <code>numpy</code> about 2 seconds, whereas <code>polars</code> needed about <strong>2 minutes</strong> to create the dataframe, i.e. 60 times slower.</p>
<p>To re-produce, just add <code>text *= 1000</code> before creating the numpy array in the code above.</p>
<p><strong>2. Casting to integer.</strong></p>
<p>For the original (smaller) file, casting to int sped up the process for both <code>numpy</code> and <code>polars</code>. The filtering in <code>numpy</code> was still about 5 times faster than <code>polars</code> (30 microseconds vs. 120), wheres the sorting time became more similar (150 microseconds for numpy vs. 200 for polars).</p>
<p>However, for the large file, <code>polars</code> was marginally faster than <code>numpy</code>, but the huge instantiation time makes it only worth if the dataframe is to be queried thousands of times.</p>
"
"<p>I am trying to run Selenium on Colab, but an error occurs. It ran well a few weeks ago, but an error occurred suddenly today at the last line of code. What should I do?</p>
<pre><code>!pip install selenium
!apt-get update
!apt install chromium-chromedriver
!cp /usr/lib/chromium-browser/chromedriver /usr/bin

import time
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.common.keys import Keys
import sys

options = webdriver.ChromeOptions()
options.add_argument('--headless')        
options.add_argument('--no-sandbox')
options.add_argument('--disable-dev-shm-usage')
driver = webdriver.Chrome('chromedriver', options=options)
</code></pre>
"
"<p>TypeScript has a <a href=""https://www.typescriptlang.org/docs/handbook/utility-types.html#picktype-keys"" rel=""nofollow noreferrer"">Pick type</a> which can take an existing type and create a new one by picking individual attributes.  I would like to have a similar functionality in Python, so for example:</p>
<pre><code>from dataclasses import dataclass
from pick import Pick

@dataclasses.dataclass
class Foo:
    x: int
    y: float
    z: str

Bar = Pick(Foo, ('x', 'y'))

foo = Foo(1, 2.0, '3')
bar = Bar(1, 2.0)
</code></pre>
<p>I want this to avoid the problem of <a href=""https://scotthannen.org/blog/2021/04/19/partial-optional-object-population.html"" rel=""nofollow noreferrer"">partial object population</a> without having to explicitly define many different classes which are really just subsets of a larger one.  Ideally, this will be type-safe and recognized by mypy.</p>
<p>I looked at other solutions like defining a new class for every subset of attributes, but that produced too many classes which are all similar but just slightly different, making the code harder to read.  I also tried having a large class with every field being optional, but that still makes the code hard to read because I have to reason about the lineage of an object to tell whether it will have some field, or if that field is null.</p>
<p>Bonus points if there is also a way to <em>add</em> a field via something like <code>Baz = Add(Foo, {'w': List}); baz(1, 2.0, '3', w=[])</code></p>
"
"<p>I am working with an ex termly large datfarem. Here is a sample:</p>
<pre><code>import pandas as pd
import numpy as np
df = pd.DataFrame({ 
'ID': ['A', 'A', 'A', 'X', 'X', 'Y'], 
})
 ID
0  A
1  A
2  A
3  X
4  X
5  Y
</code></pre>
<p>Now, given the frequency of each value in column '''ID''', I want to calculate a weight using the function below and add a column that has the weight associated with each value in '''ID'''.</p>
<pre><code>def get_weights_inverse_num_of_samples(label_counts, power=1.):
    no_of_classes = len(label_counts)
    weights_for_samples = 1.0/np.power(np.array(label_counts), power)
    weights_for_samples = weights_for_samples/ np.sum(weights_for_samples)*no_of_classes
    return weights_for_samples

freq = df.value_counts()
print(freq)
ID
A     3
X     2
Y     1

weights = get_weights_inverse_num_of_samples(freq)
print(weights)
[0.54545455 0.81818182 1.63636364]
</code></pre>
<p>So, I am looking for an efficient way to get a dataframe like this given the above weights:</p>
<pre><code>   ID  sample_weight
0  A   0.54545455
1  A   0.54545455
2  A   0.54545455
3  X   0.81818182
4  X   0.81818182
5  Y   1.63636364
</code></pre>
"
"<p>I'm a big fan of pylint's built-in docstring checker. I'm very happy to require docstrings on all my classes, all my functions, and all my modules.</p>
<p>What I don't like, however, is that pylint also wants docstrings on all my test modules and all my <code>pytest</code> test functions. This leads to low-value docstrings like the following:</p>
<pre class=""lang-py prettyprint-override""><code>&quot;&quot;&quot;Tests for foo.py&quot;&quot;&quot;

from foo import bar

def test_bar():
    &quot;&quot;&quot;Tests for bar&quot;&quot;&quot;
    assert bar(1) == 2
</code></pre>
<p>I've been able to disable the function-level docstring requirement using <code>no-docstring-rgx</code> in my <a href=""https://www.codeac.io/documentation/pylint-configuration.html"" rel=""noreferrer""><code>.pylintrc</code> file</a>:</p>
<pre><code>[MASTER]
no-docstring-rgx=^(_|test_)
</code></pre>
<p>This takes care of the <a href=""https://pylint.pycqa.org/en/latest/user_guide/messages/convention/missing-function-docstring.html"" rel=""noreferrer""><code>missing-function-docstring</code></a> / <code>C0116</code> error.</p>
<p>But I haven't been able to find a way to disable the <a href=""https://pylint.pycqa.org/en/latest/user_guide/messages/convention/missing-module-docstring.html"" rel=""noreferrer""><code>missing-module-docstring</code></a> / <code>C0114</code> error just for files ending with <code>_test.py</code>. Is this possible with pylint?</p>
"
"<p>Given this Jupyter notebook cell:</p>
<pre><code>x = [1,2,3,4,5]
y = {1,2,3,4,5}
x
y
</code></pre>
<p>When the cell executes, it generates this output:</p>
<pre><code>{1, 2, 3, 4, 5}
</code></pre>
<p>The last line in the cell generates output, the line above it has no effect. This works for any data type, as far as I can tell.</p>
<p>Here's a snip of the same code as above:</p>
<p><a href=""https://i.sstatic.net/GmXmL.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/GmXmL.png"" alt=""Same code as above!"" /></a></p>
"
"<p>Hello i am trying to integrate Google Sign In into my python script and i am running this this error:</p>
<p><em>Access blocked: This app’s request is invalid</em></p>
<p><em>You can’t sign in because this app sent an invalid request. You can try again later, or contact the developer about this issue. <strong>Learn more about this error</strong></em></p>
<p><em>If you are a developer of this app, see <strong>error details</strong>.</em></p>
<p><em>Error 400: redirect_uri_mismatch</em></p>
<p>My python script im trying to run is the basic one from Google sources:</p>
<pre><code># import the required libraries
from googleapiclient.discovery import build
from google_auth_oauthlib.flow import InstalledAppFlow
from google.auth.transport.requests import Request
import pickle
import os.path
import base64
import email
from bs4 import BeautifulSoup

# Define the SCOPES. If modifying it, delete the token.pickle file.
SCOPES = ['https://www.googleapis.com/auth/gmail.readonly']

def getEmails():
    # Variable creds will store the user access token.
    # If no valid token found, we will create one.
    creds = None

    # The file token.pickle contains the user access token.
    # Check if it exists
    if os.path.exists('token.pickle'):

        # Read the token from the file and store it in the variable creds
        with open('token.pickle', 'rb') as token:
            creds = pickle.load(token)

    # If credentials are not available or are invalid, ask the user to log in.
    if not creds or not creds.valid:
        if creds and creds.expired and creds.refresh_token:
            creds.refresh(Request())
        else:
            flow = InstalledAppFlow.from_client_secrets_file('credentials.json', SCOPES)
            creds = flow.run_local_server(port=0)

        # Save the access token in token.pickle file for the next run
        with open('token.pickle', 'wb') as token:
            pickle.dump(creds, token)

    # Connect to the Gmail API
    service = build('gmail', 'v1', credentials=creds)

    # request a list of all the messages
    result = service.users().messages().list(userId='me').execute()

    # We can also pass maxResults to get any number of emails. Like this:
    # result = service.users().messages().list(maxResults=200, userId='me').execute()
    messages = result.get('messages')

    # messages is a list of dictionaries where each dictionary contains a message id.

    # iterate through all the messages
    for msg in messages:
        # Get the message from its id
        txt = service.users().messages().get(userId='me', id=msg['id']).execute()

        # Use try-except to avoid any Errors
        try:
            # Get value of 'payload' from dictionary 'txt'
            payload = txt['payload']
            headers = payload['headers']

            # Look for Subject and Sender Email in the headers
            for d in headers:
                if d['name'] == 'Subject':
                    subject = d['value']
                if d['name'] == 'From':
                    sender = d['value']

            # The Body of the message is in Encrypted format. So, we have to decode it.
            # Get the data and decode it with base 64 decoder.
            parts = payload.get('parts')[0]
            data = parts['body']['data']
            data = data.replace(&quot;-&quot;,&quot;+&quot;).replace(&quot;_&quot;,&quot;/&quot;)
            decoded_data = base64.b64decode(data)

            # Now, the data obtained is in lxml. So, we will parse
            # it with BeautifulSoup library
            soup = BeautifulSoup(decoded_data , &quot;lxml&quot;)
            body = soup.body()

            # Printing the subject, sender's email and message
            print(&quot;Subject: &quot;, subject)
            print(&quot;From: &quot;, sender)
            print(&quot;Message: &quot;, body)
            print('\n')
        except:
            pass


getEmails()
</code></pre>
<p>I created a project in Google Console and gave it redirect uri of my http://localhost:8000/</p>
<p>if anyone knows how to fix it, i will be glad<br />
thanks.</p>
"
"<p>How can I download a specific format without using options like &quot;best video&quot;, using the format ID... example: 139, see the <a href=""https://i.sstatic.net/7MlOf.jpg"" rel=""noreferrer"">picture</a></p>
<pre class=""lang-none prettyprint-override""><code>❯ yt-dlp --list-formats https://www.youtube.com/watch?v=BaW_jenozKc
[youtube] Extracting URL: https://www.youtube.com/watch?v=BaW_jenozKc 
[youtube] BaW_jenozKc: Downloading webpage 
[youtube] BaW_jenozKc: Downloading android player API JSON 
[info] Available formats for BaW_jenozKc: 
ID  EXT  RESOLUTION FPS CH │   FILESIZE   TBR PROTO │ VCODEC        VBR ACODEC      ABR ASR MORE INFO
───────────────────────────────────────────────────────────────────────────────────────────────────────────── 
139 m4a  audio only      2 │   58.59KiB   48k https │ audio only        mp4a.40.5   48k 22k low, m4a_dash     
249 webm audio only      2 │   58.17KiB   48k https │ audio only        opus        48k 48k low, webm_dash    
250 webm audio only      2 │   76.07KiB   63k https │ audio only        opus        63k 48k low, webm_dash    
140 m4a  audio only      2 │  154.06KiB  128k https │ audio only        mp4a.40.2  128k 44k medium, m4a_dash  
251 webm audio only      2 │  138.96KiB  116k https │ audio only        opus       116k 48k medium, webm_dash 
17  3gp  176x144     12  1 │   55.79KiB   45k https │ mp4v.20.3     45k mp4a.40.2    0k 22k 144p
160 mp4  256x144     15    │  135.08KiB  113k https │ avc1.4d400c  113k video only          144p, mp4_dash    
278 webm 256x144     30    │   52.22KiB   44k https │ vp9           44k video only          144p, webm_dash   
133 mp4  426x240     30    │  294.27KiB  246k https │ avc1.4d4015  246k video only          240p, mp4_dash    
242 webm 426x240     30    │   33.27KiB   28k https │ vp9           28k video only          240p, webm_dash   
134 mp4  640x360     30    │  349.59KiB  292k https │ avc1.4d401e  292k video only          360p, mp4_dash    
18  mp4  640x360     30  2 │ ~525.60KiB  420k https │ avc1.42001E  420k mp4a.40.2    0k 44k 360p
243 webm 640x360     30    │   75.55KiB   63k https │ vp9           63k video only          360p, webm_dash   
135 mp4  854x480     30    │  849.41KiB  710k https │ avc1.4d401f  710k video only          480p, mp4_dash    
244 webm 854x480     30    │  165.49KiB  138k https │ vp9          138k video only          480p, webm_dash   
22  mp4  1280x720    30  2 │ ~  1.82MiB 1493k https │ avc1.64001F 1493k mp4a.40.2    0k 44k 720p
136 mp4  1280x720    30    │    1.60MiB 1366k https │ avc1.4d401f 1366k video only          720p, mp4_dash    
247 webm 1280x720    30    │  504.68KiB  420k https │ vp9          420k video only          720p, webm_dash   
137 mp4  1920x1080   30    │    2.11MiB 1803k https │ avc1.640028 1803k video only          1080p, mp4_dash   
248 webm 1920x1080   30    │  965.31KiB  804k https │ vp9          804k video only          1080p, webm_dash  
</code></pre>
<p>I tried using the format url, but it didn't work</p>
"
"<p>I am trying to sift through a big database that is compressed in a .zst. I am aware that I can simply just decompress it and then work on the resulting file, but that uses up a lot of space on my ssd and takes 2+ hours so I would like to avoid that if possible.</p>
<p>Often when I work with large files I would stream it line by line with code like</p>
<pre><code>with open(filename) as f:
    for line in f.readlines():
        do_something(line)
</code></pre>
<p>I know gzip has this</p>
<pre><code>with gzip.open(filename,'rt') as f:
    for line in f:
        do_something(line)
</code></pre>
<p>but it doesn't seem to work with .zsf, so I am wondering if there're any libraries that can decompress and stream the decompressed data in a similar way. For example:</p>
<pre><code>with zstlib.open(filename) as f:
    for line in f.zstreadlines():
        do_something(line)
</code></pre>
"
"<p>Consider a Python protocol attribute which is also annotated with a protocol. I found in that case, both mypy and Pyright report an error even when my custom datatype follows the nested protocol. For example in the code below <code>Outer</code> follows the <code>HasHasA</code> protocol in that it has <code>hasa: HasA</code> because <code>Inner</code> follows <code>HasA</code> protocol.</p>
<pre class=""lang-py prettyprint-override""><code>from dataclasses import dataclass
from typing import Protocol

class HasA(Protocol):
    a: int

class HasHasA(Protocol):
    hasa: HasA

@dataclass
class Inner:
    a: int

@dataclass
class Outer:
    hasa: Inner

def func(b: HasHasA): ...

o = Outer(Inner(0))
func(o)
</code></pre>
<p>However, mypy shows the following error.</p>
<pre><code>nested_protocol.py:22: error: Argument 1 to &quot;func&quot; has incompatible type &quot;Outer&quot;; expected &quot;HasHasA&quot;  [arg-type]
nested_protocol.py:22: note: Following member(s) of &quot;Outer&quot; have conflicts:
nested_protocol.py:22: note:     hasa: expected &quot;HasA&quot;, got &quot;Inner&quot;
</code></pre>
<p>What's wrong with my code?</p>
"
"<p>Whenever I try to type-hint a list of strings, such as</p>
<pre><code>tricks: list[str] = []
</code></pre>
<p>, I get <em>TypeError: 'type' object is not subscriptable</em>. I follow a course where they use the same code, but it works for them. So I guess the problem is one of these differences between my an the courses environments. I use:</p>
<ul>
<li>vs code</li>
<li>anaconda</li>
<li>python 3.8.15</li>
<li>jupyter notebook</li>
</ul>
<p>Can someone help me fix that?</p>
<p>I used the same code in normal .py files and it sill doesn't work, so that is probably not it.
The python version should also not be the problem as this is kind of basic.
Anaconda should also not cause such Error messages.
Leaves the difference between vscode and pycharm, which is also strange.
Therefore I don't know what to try.</p>
"
"<p>In Python3.11 it's suggested to use <code>TaskGroup</code> for spawning Tasks rather than using <code>gather</code>. Given Gather will also return the result of a co-routine, what's the best approach with TaskGroup.</p>
<p>Currently I have</p>
<pre class=""lang-py prettyprint-override""><code>async with TaskGroup() as tg:
      r1 = tg.create_task(foo())
      r2 = tg.create_task(bar())
res = [r1.result(), r2.result()]
</code></pre>
<p>Is there a more concise approach that can be used to achieve the same result?</p>
"
"<p>I am making a timer using tkinter in python. The widget simply has a single button. This button doubles as the element displaying the time remaining. The timer has a thread that simply updates what time is shown on the button.</p>
<p>The thread simply uses a while loop that should stop when an <a href=""https://stackoverflow.com/a/48611452"">event</a> is set.
When the window is closed, I use <a href=""https://stackoverflow.com/a/111160"">protocol</a> to call a function that sets this event then attempts to join the thread. This works <em>most</em> of the time. However, if I close the program just as a certain call is being made, this fails and the thread continues after the window has been closed.</p>
<p>I'm aware of the <a href=""https://stackoverflow.com/questions/43738907/thread-wont-let-tkinter-application-close"">other</a> <a href=""https://stackoverflow.com/questions/23739453/cannot-close-multithreaded-tkinter-app-on-x-button"">similar</a> threads about closing threads when closing a tkinter window. But these answers are old, and I would like to avoid using thread.stop() if possible.</p>
<p>I tried reducing this as much as possible while still showing my intentions for the program.</p>
<pre><code>import tkinter as tk
from tkinter import TclError, ttk
from datetime import timedelta
import time
import threading
from threading import Event

def strfdelta(tdelta):
    # Includes microseconds
    hours, rem = divmod(tdelta.seconds, 3600)
    minutes, seconds = divmod(rem, 60)
    return str(hours).rjust(2, '0') + &quot;:&quot; + str(minutes).rjust(2, '0') + \
           &quot;:&quot; + str(seconds).rjust(2, '0') + &quot;:&quot; + str(tdelta.microseconds).rjust(6, '0')[0:2]

class App(tk.Tk):
    def __init__(self):
        super().__init__()
        self.is_running = False
        is_closing = Event()
        self.start_time = timedelta(seconds=4, microseconds=10, minutes=0, hours=0)
        self.current_start_time = self.start_time
        self.time_of_last_pause = time.time()
        self.time_of_last_unpause = None
        # region guisetup
        self.time_display = None
        self.geometry(&quot;320x110&quot;)
        self.title('Replace')
        self.resizable(False, False)
        box1 = self.create_top_box(self)
        box1.place(x=0, y=0)
        # endregion guisetup
        self.timer_thread = threading.Thread(target=self.timer_run_loop, args=(is_closing, ))
        self.timer_thread.start()

        def on_close():  # This occasionally fails when we try to close.
            is_closing.set()  # This used to be a boolean property self.is_closing. Making it an event didn't help.
            print(&quot;on_close()&quot;)
            try:
                self.timer_thread.join(timeout=2)
            finally:
                if self.timer_thread.is_alive():
                    self.timer_thread.join(timeout=2)
                    if self.timer_thread.is_alive():
                        print(&quot;timer thread is still alive again..&quot;)
                    else:
                        print(&quot;timer thread is finally finished&quot;)
                else:
                    print(&quot;timer thread finished2&quot;)
            self.destroy()  # https://stackoverflow.com/questions/111155/how-do-i-handle-the-window-close-event-in-tkinter
        self.protocol(&quot;WM_DELETE_WINDOW&quot;, on_close)

    def create_top_box(self, container):
        box = tk.Frame(container, height=110, width=320)
        box_m = tk.Frame(box, bg=&quot;blue&quot;, width=320, height=110)
        box_m.place(x=0, y=0)
        self.time_display = tk.Button(box_m, text=strfdelta(self.start_time), command=self.toggle_timer_state)
        self.time_display.place(x=25, y=20)
        return box

    def update_shown_time(self, time_to_show: timedelta = None):
        print(&quot;timer_run_loop must finish. flag 0015&quot;)  # If the window closes at this point, everything freezes
        self.time_display.configure(text=strfdelta(time_to_show))
        print(&quot;timer_run_loop must finish. flag 016&quot;)

    def toggle_timer_state(self):
        # update time_of_last_unpause if it has never been set
        if not self.is_running and self.time_of_last_unpause is None:
            self.time_of_last_unpause = time.time()
        if self.is_running:
            self.pause_timer()
        else:
            self.start_timer_running()

    def pause_timer(self):
        pass  # Uses self.time_of_last_unpause, Alters self.is_running, self.time_of_last_pause, self.current_start_time

    def timer_run_loop(self, event):
        while not event.is_set():
            if not self.is_running:
                print(&quot;timer_run_loop must finish. flag 008&quot;)
                self.update_shown_time(self.current_start_time)
            print(&quot;timer_run_loop must finish. flag 018&quot;)
        print(&quot;timer_run_loop() ending&quot;)

    def start_timer_running(self):
        pass  # Uses self.current_start_time; Alters self.is_running, self.time_of_last_unpause

if __name__ == &quot;__main__&quot;:
    app = App()
    app.mainloop()

</code></pre>
<p>You don't even have to press the button for this bug to manifest, but it does take trail and error. I just run it and hit alt f4 until it happens.</p>
<p>If you run this and encounter the problem, you will see that &quot;timer_run_loop must finish. flag 0015&quot; is the last thing printed before we check if the thread has ended.
That means,
<code>self.time_display.configure(text=strfdelta(time_to_show))</code> hasn't finished yet. I think closing the tkinter window while a thread is using this tkinter button inside of it is somehow causing a problem.</p>
<p>There seems to be very little solid documentation about the configure method in tkinter.
Python's official <a href=""https://docs.python.org/3/library/tkinter.html?highlight=configure"" rel=""noreferrer"">documention of tkinter</a> mentions the function only in passing. It's just used as a read-only dictionary.
<br>A tkinter style class gets a <a href=""https://docs.python.org/3/library/tkinter.ttk.html#tkinter.ttk.Style.configure"" rel=""noreferrer"">little bit of detail</a> about it's configure method, but this is unhelpful. <br>The <a href=""https://tkdocs.com/pyref/onepage.html"" rel=""noreferrer"">tkdocs</a> lists configure aka config as one of the methods available for all widgets. <br>This <a href=""https://coderslegacy.com/python/tkinter-config/"" rel=""noreferrer"">tutorial article</a> seems to be the only place that shows the function actually being used. But it doesn't mention any possible problems or exceptions the method could encounter.</p>
<p>Is there some resource sharing pattern I'm not using? Or is there a better way to end this thread?</p>
"
"<p>According to the <a href=""https://docs.pydantic.dev/usage/models/#orm-mode-aka-arbitrary-class-instances"" rel=""nofollow noreferrer"">docs</a>, Pydantic &quot;ORM mode&quot; (enabled with <code>orm_mode = True</code> in <code>Config</code>) is needed to enable the <code>from_orm</code> method in order to create a model instance by reading attributes from another class instance. If ORM mode is not enabled, the <code>from_orm</code> method raises an exception.</p>
<p>My questions are:</p>
<ol>
<li>Are there any other effects (in functionality, performance, etc.) in enabling ORM mode?</li>
<li>If not, why is it an opt-in feature?</li>
</ol>
"
"<p>so the problem is that pypi.org hase been filtered by iranian government(<strong>yes , i know it's ridiculous!</strong>). i tried to install some python modules from  Github downloaded files:
<code>pip install moduleName</code>
but every module has it's own dependencies and try to connect to pipy.org to reach them. then there will be an error during installation.
is there any solution?
your help will be much appreciated.</p>
"
"<p>I have a data set with three columns. Column A is to be checked for strings. If the string matches <code>foo</code> or <code>spam</code>, the values in the same row for the other two columns <code>L</code> and <code>G</code> should be changed to <code>XX</code>. For this I have tried the following.</p>
<pre><code>df = pl.DataFrame(
    {
        &quot;A&quot;: [&quot;foo&quot;, &quot;ham&quot;, &quot;spam&quot;, &quot;egg&quot;,],
        &quot;L&quot;: [&quot;A54&quot;, &quot;A12&quot;, &quot;B84&quot;, &quot;C12&quot;],
        &quot;G&quot;: [&quot;X34&quot;, &quot;C84&quot;, &quot;G96&quot;, &quot;L6&quot;,],
    }
)
print(df)

shape: (4, 3)
┌──────┬─────┬─────┐
│ A    ┆ L   ┆ G   │
│ ---  ┆ --- ┆ --- │
│ str  ┆ str ┆ str │
╞══════╪═════╪═════╡
│ foo  ┆ A54 ┆ X34 │
│ ham  ┆ A12 ┆ C84 │
│ spam ┆ B84 ┆ G96 │
│ egg  ┆ C12 ┆ L6  │
└──────┴─────┴─────┘
</code></pre>
<p>expected outcome</p>
<pre><code>shape: (4, 3)
┌──────┬─────┬─────┐
│ A    ┆ L   ┆ G   │
│ ---  ┆ --- ┆ --- │
│ str  ┆ str ┆ str │
╞══════╪═════╪═════╡
│ foo  ┆ XX  ┆ XX  │
│ ham  ┆ A12 ┆ C84 │
│ spam ┆ XX  ┆ XX  │
│ egg  ┆ C12 ┆ L6  │
└──────┴─────┴─────┘
</code></pre>
<p>I tried this</p>
<pre><code>df = df.with_columns(
    pl.when((pl.col(&quot;A&quot;) == &quot;foo&quot;) | (pl.col(&quot;A&quot;) == &quot;spam&quot;))
    .then((pl.col(&quot;L&quot;)= &quot;XX&quot;) &amp; (pl.col( &quot;G&quot;)= &quot;XX&quot;))
    .otherwise((pl.col(&quot;L&quot;))&amp;(pl.col( &quot;G&quot;)))
)
</code></pre>
<p>However, this does not work. Can someone help me with this?</p>
"
"<p>I have been using polars but it seems like it lacks qcut functionality as pandas do.</p>
<p>I am not sure about the reason but is it possible to achieve the same effect as pandas qcut using current available polars functionalities?</p>
<p>The following shows an example about what I can do with pandas qcut.</p>
<pre><code>import pandas as pd

data = pd.Series([11, 1, 2, 2, 3, 4, 5, 1, 2, 3, 4, 5])
pd.qcut(data, [0, 0.2, 0.4, 0.6, 0.8, 1], labels=['q1', 'q2', 'q3', 'q4', 'q5'])
</code></pre>
<p>The results are as follows:</p>
<pre><code>0     q5
1     q1
2     q1
3     q1
4     q3
5     q4
6     q5
7     q1
8     q1
9     q3
10    q4
11    q5
dtype: category
</code></pre>
<p>So, I am curious how can I get the same result by using polars?</p>
<p>Thanks for your help.</p>
"
"<p>I am trying to use the SAM CLI on my M1 Mac.</p>
<p>I followed the steps outlined in <a href=""https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/serverless-getting-started-hello-world.html"" rel=""noreferrer"">these docs</a>:</p>
<pre><code>sam init
cd sam-app
sam build
sam deploy --guided
</code></pre>
<p>I did not modify the code or the yaml files.
I can start the local Lambda function as expected:</p>
<pre><code>➜  sam-app sam local start-api
Mounting HelloWorldFunction at http://127.0.0.1:3000/hello [GET]
You can now browse to the above endpoints to invoke your functions. You do not need to restart/reload SAM CLI while working on your functions, changes will be reflected instantly/automatically. If you used sam build before running local commands, you will need to re-run sam build for the changes to be picked up. You only need to restart SAM CLI if you update your AWS SAM template
2023-01-23 17:54:06  * Running on http://127.0.0.1:3000/ (Press CTRL+C to quit)
</code></pre>
<p>But as soon as I hit the endpoint by doing:</p>
<pre><code>curl http://localhost:3000/hello
</code></pre>
<p>The Lambda RIE starts throwing errors and returns a 502.</p>
<pre><code>Invoking app.lambda_handler (python3.9)
Image was not found.
Removing rapid images for repo public.ecr.aws/sam/emulation-python3.9
Building image...................
Failed to build Docker Image
NoneType: None
Exception on /hello [GET]
Traceback (most recent call last):
  File &quot;/opt/homebrew/Cellar/aws-sam-cli/1.70.0/libexec/lib/python3.11/site-packages/flask/app.py&quot;, line 2073, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/homebrew/Cellar/aws-sam-cli/1.70.0/libexec/lib/python3.11/site-packages/flask/app.py&quot;, line 1518, in full_dispatch_request
    rv = self.handle_user_exception(e)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/homebrew/Cellar/aws-sam-cli/1.70.0/libexec/lib/python3.11/site-packages/flask/app.py&quot;, line 1516, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/homebrew/Cellar/aws-sam-cli/1.70.0/libexec/lib/python3.11/site-packages/flask/app.py&quot;, line 1502, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**req.view_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/homebrew/Cellar/aws-sam-cli/1.70.0/libexec/lib/python3.11/site-packages/samcli/local/apigw/local_apigw_service.py&quot;, line 361, in _request_handler
    self.lambda_runner.invoke(route.function_name, event, stdout=stdout_stream_writer, stderr=self.stderr)
  File &quot;/opt/homebrew/Cellar/aws-sam-cli/1.70.0/libexec/lib/python3.11/site-packages/samcli/commands/local/lib/local_lambda.py&quot;, line 137, in invoke
    self.local_runtime.invoke(
  File &quot;/opt/homebrew/Cellar/aws-sam-cli/1.70.0/libexec/lib/python3.11/site-packages/samcli/lib/telemetry/metric.py&quot;, line 315, in wrapped_func
    return_value = func(*args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/homebrew/Cellar/aws-sam-cli/1.70.0/libexec/lib/python3.11/site-packages/samcli/local/lambdafn/runtime.py&quot;, line 177, in invoke
    container = self.create(function_config, debug_context, container_host, container_host_interface)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/homebrew/Cellar/aws-sam-cli/1.70.0/libexec/lib/python3.11/site-packages/samcli/local/lambdafn/runtime.py&quot;, line 73, in create
    container = LambdaContainer(
                ^^^^^^^^^^^^^^^^
  File &quot;/opt/homebrew/Cellar/aws-sam-cli/1.70.0/libexec/lib/python3.11/site-packages/samcli/local/docker/lambda_container.py&quot;, line 93, in __init__
    image = LambdaContainer._get_image(
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/homebrew/Cellar/aws-sam-cli/1.70.0/libexec/lib/python3.11/site-packages/samcli/local/docker/lambda_container.py&quot;, line 236, in _get_image
    return lambda_image.build(runtime, packagetype, image, layers, architecture, function_name=function_name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/homebrew/Cellar/aws-sam-cli/1.70.0/libexec/lib/python3.11/site-packages/samcli/local/docker/lambda_image.py&quot;, line 164, in build
    self._build_image(
  File &quot;/opt/homebrew/Cellar/aws-sam-cli/1.70.0/libexec/lib/python3.11/site-packages/samcli/local/docker/lambda_image.py&quot;, line 279, in _build_image
    raise ImageBuildException(&quot;Error building docker image: {}&quot;.format(log[&quot;error&quot;]))
samcli.commands.local.cli_common.user_exceptions.ImageBuildException: Error building docker image: The command '/bin/sh -c mv /var/rapid/aws-lambda-rie-x86_64 /var/rapid/aws-lambda-rie &amp;&amp; chmod +x /var/rapid/aws-lambda-rie' returned a non-zero code: 1
</code></pre>
<p>I found this <a href=""https://github.com/aws/aws-sam-cli/issues/3169#issuecomment-906729604"" rel=""noreferrer"">Github issue</a> where someone recommended to do the following:</p>
<pre><code> docker run --rm --privileged multiarch/qemu-user-static --reset -p yes
</code></pre>
<p>But it yielded no results.</p>
<p>Does anyone know what I'm doing wrong, or how to resolve this issue so the docker container can build correctly? Thanks.</p>
"
"<p>On Windows 10 I want to read data from UDP port 9001. I have created the following script which does not give any output (python 3.10.9):</p>
<pre><code>import socket

sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
sock.bind((&quot;&quot;, 9001))
    
while True:
    data, addr = sock.recv(1024)
    print(f&quot;received message: {data.decode()} from {addr}&quot;)
</code></pre>
<p>I checked that a device is sending UDP data on port 9001 using <strong>wireshark</strong>. But the above code just &quot;runs&quot; on powershell without any output (and without any errors).</p>
<p>Any ideas how to fix this?</p>
<p>I found <a href=""https://cloudbrothers.info/en/test-udp-connection-powershell/"" rel=""noreferrer"">this page</a> with a <strong>powershell</strong> script that is supposed to listen to a UDP port. So I tried this and created a file <code>Start-UDPServer.ps1</code> with the content as described in that page as follows:</p>
<pre><code>function Start-UDPServer {
    [CmdletBinding()]
    param (
        # Parameter help description
        [Parameter(Mandatory = $false)]
        $Port = 10000
    )
    
    # Create a endpoint that represents the remote host from which the data was sent.
    $RemoteComputer = New-Object System.Net.IPEndPoint([System.Net.IPAddress]::Any, 0)
    Write-Host &quot;Server is waiting for connections - $($UdpObject.Client.LocalEndPoint)&quot;
    Write-Host &quot;Stop with CRTL + C&quot;

    # Loop de Loop
    do {
        # Create a UDP listender on Port $Port
        $UdpObject = New-Object System.Net.Sockets.UdpClient($Port)
        # Return the UDP datagram that was sent by the remote host
        $ReceiveBytes = $UdpObject.Receive([ref]$RemoteComputer)
        # Close UDP connection
        $UdpObject.Close()
        # Convert received UDP datagram from Bytes to String
        $ASCIIEncoding = New-Object System.Text.ASCIIEncoding
        [string]$ReturnString = $ASCIIEncoding.GetString($ReceiveBytes)

        # Output information
        [PSCustomObject]@{
            LocalDateTime = $(Get-Date -UFormat &quot;%Y-%m-%d %T&quot;)
            SourceIP      = $RemoteComputer.address.ToString()
            SourcePort    = $RemoteComputer.Port.ToString()
            Payload       = $ReturnString
        }
    } while (1)
}
</code></pre>
<p>and started it in an <strong>Powershell</strong> terminal (as admin) as</p>
<pre><code>.\Start-UDPServer.ps1 -Port 9001
</code></pre>
<p>and it returned to the Powershell immediately without ANY output (or error message). Maybe windows is broken?</p>
<p>If there is a solution to finally listen to UDP port 9001, I still strongly prefer a <strong>python</strong> solution!</p>
"
"<p>I was building a documentation site for my python project using mkdocstrings.</p>
<p>For generating the code referece files I followed this instructions <a href=""https://mkdocstrings.github.io/recipes/"" rel=""noreferrer"">https://mkdocstrings.github.io/recipes/</a></p>
<p>I get these errors:</p>
<pre><code>    INFO     
    -  Building documentation... INFO     
    -  Cleaning site directory INFO     
    -  The following pages exist in the docs directory, but are not included in the &quot;nav&quot; configuration:               - reference\SUMMARY.md               
    - reference_init_.md 
    ... ...               
    - reference\tests\manual_tests.md ERROR    
    -  mkdocstrings: No module named ' ' ERROR    
    -  Error reading page 'reference/init.md': ERROR    
    -  Could not collect ' '
</code></pre>
<p>This is my file structure:
<a href=""https://i.sstatic.net/MWC6H.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/MWC6H.png"" alt=""project structure"" /></a></p>
<p>This is my docs folder:
<a href=""https://i.sstatic.net/2GBKj.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/2GBKj.png"" alt=""docs folder"" /></a></p>
<p>I have the same gen_ref_pages.py file shown in the page:</p>
<pre><code>
    from pathlib import Path

    import mkdocs_gen_files

    nav = mkdocs_gen_files.Nav()

    for path in sorted(Path(&quot;src&quot;).rglob(&quot;*.py&quot;)):
        module_path = path.relative_to(&quot;src&quot;).with_suffix(&quot;&quot;)
        doc_path = path.relative_to(&quot;src&quot;).with_suffix(&quot;.md&quot;)
        full_doc_path = Path(&quot;reference&quot;, doc_path)

        parts = tuple(module_path.parts)

        if parts[-1] == &quot;__init__&quot;:
            parts = parts[:-1]
        elif parts[-1] == &quot;__main__&quot;:
            continue

        nav[parts] = doc_path.as_posix()  # 

        with mkdocs_gen_files.open(full_doc_path, &quot;w&quot;) as fd:
            ident = &quot;.&quot;.join(parts)
            fd.write(f&quot;::: {ident}&quot;)

        mkdocs_gen_files.set_edit_path(full_doc_path, path)

    with mkdocs_gen_files.open(&quot;reference/SUMMARY.md&quot;, &quot;w&quot;) as nav_file:  # 
        nav_file.writelines(nav.build_literate_nav())  #    ```

This is my mkdocs.yml:

```    site_name: CA Prediction Docs

    theme:
      name: &quot;material&quot;
      palette:
        primary: deep purple
      logo: assets/logo.png
      favicon: assets/favicon.png
      features:
        - navigation.instant
        - navigation.tabs
        - navigation.expand
        - navigation.top
        # - navigation.sections
        - search.highlight
        - navigation.footer
      icon:
        repo: fontawesome/brands/git-alt

    copyright: Copyright &amp;copy; 2022 - 2023 Ezequiel González

    extra:
      social:
        - icon: fontawesome/brands/github
          link: https://github.com/ezegonmac
        - icon: fontawesome/brands/linkedin
          link: https://www.linkedin.com/in/ezequiel-gonzalez-macho-329583223/

    repo_url: https://github.com/ezegonmac/TFG-CellularAutomata
    repo_name: ezegonmac/TFG-CellularAutomata

    plugins:
      - search
      - gen-files:
          scripts:
          - docs/gen_ref_pages.py
      - mkdocstrings

    nav:
      - Introduction: index.md
      - Getting Started: getting-started.md
      - API Reference: reference.md
      # - Reference: reference/
      - Explanation: explanation.md
</code></pre>
"
"<p>A related question came up at <a href=""https://stackoverflow.com/questions/75193175/why-i-cant-use-multiprocessing-queue-with-processpoolexecutor"">Why I can't use multiprocessing.Queue with ProcessPoolExecutor?</a>. I provided a partial answer along with a workaround but admitted that the question raises another question, namely why a <code>multiprocessing.Queue</code> instance <em>can</em> be passed as the argument to a <code>multiprocessing.Process</code> worker function.</p>
<p>For example, the following code fails under platforms that use either the <em>spawn</em> or <em>fork</em> method of creating new processes:</p>
<pre class=""lang-py prettyprint-override""><code>from multiprocessing import Pool, Queue

def worker(q):
    print(q.get())

with Pool(1) as pool:
    q = Queue()
    q.put(7)
    pool.apply(worker, args=(q,))
</code></pre>
<p>The above raises:</p>
<p><code>RuntimeError: Queue objects should only be shared between processes through inheritance</code></p>
<p>Yet the following program runs without a problem:</p>
<pre class=""lang-py prettyprint-override""><code>from multiprocessing import Process, Queue

def worker(q):
    print(q.get())

q = Queue()
q.put(7)
p = Process(target=worker, args=(q,))
p.start()
p.join()
</code></pre>
<p>It appears that arguments to a multiprocessing pool worker function ultimately get put on the pool's input queue, which is implemented as a <code>multiprocessing.SimpleQueue</code>, and you cannot put a <code>multiprocessing.Queue</code> instance to a <code>multiprocessing.SimpleQueue</code> instance, which uses a <code>ForkingPickler</code> for serialization.</p>
<p>So how is the <code>multiprocessing.Queue</code> serialized when passed as an argument to a <code>multiprocessing.Process</code> that allows it to be used in this way?</p>
"
"<h2>Context</h2>
<p>I am currently working on a team project, where we need to train neural networks. Some members are working on their local computer, and some on Colab (for GPU usage). We need to have the same dependencies. I am already familiar in using poetry on a local computer, but not on Colab, and I was wondering how to use it in Colab.</p>
<p>So I did some tests, and I encountered some issues. Maybe I can find some answers here. Thank you in advance! 😄</p>
<h2>Issues</h2>
<p><strong>1st issue</strong>: <code>poetry add &lt;package&gt;</code> does not update <em>pyproject.toml</em></p>
<p>I want to add a new package, suppose torch.
According to poetry's documentation, to install a new package, we need to run <code>poetry add &lt;package&gt;</code>. Since I run this command for the first time, a virtual environment is created, as well as the <em>poetry.lock</em>. But no packages were installed. Moreover, the <em>poetry.lock</em> file is updated, but not the <em>pyproject.toml</em>. This happens only on Colab. I tried on my local computer, and the command indeed automatically update the <em>pyproject.toml</em> file as well.</p>
<p><strong>2nd issue</strong>: <code>poetry run pip install &lt;package&gt;</code> does not update pyproject.toml</p>
<p>Instead, we can install the package with poetry run <code>pip install &lt;package&gt;</code>. I saw this command on the following <a href=""https://gist.github.com/DPaletti/da7729c35ba7f9274c3635849608b7bc"" rel=""noreferrer"">GitHub gist</a>. The packages are now installed in the virtual environment, but the <em>pyproject.toml</em> was not updated.</p>
<p>Here is a link to the <a href=""https://colab.research.google.com/drive/1fYyWHDHeEThyjUwsm9xgYETPICIf4i-h?usp=sharing"" rel=""noreferrer"">Colab notebook</a> I used for those tests.</p>
<p>Thank you again!</p>
"
"<p>I currently use <code>setuptools</code> to build my Python's package and I have declared the two authors that way in my <code>pyproject.toml</code> file:</p>
<pre><code>authors = [
     {name = &quot;X Y&quot;, email = &quot;x.y@tt.net&quot;},
     {name = &quot;Z H&quot;, email = &quot;z.h@tt.net&quot;},
]
</code></pre>
<p>Everything works and I can publish it on PyPI but only the first author is published. How can I display both authors.</p>
<p>I have tried to use the following syntax</p>
<pre><code>authors = [&quot;X Y &lt;x.y@tt.net&gt;, Z H &lt;z.h@tt.net&gt;&quot;]
</code></pre>
<p>But I have the following error</p>
<pre><code>ValueError: invalid pyproject.toml config: `project.authors[{data__authors_x}]`.
configuration error: `project.authors[{data__authors_x}]` must be object
</code></pre>
<p>Notice that I specify:</p>
<pre><code>[build-system]
requires = [&quot;setuptools&quot;,&quot;numpy&quot;,&quot;scipy&quot;,&quot;wheel&quot;]
build-backend = &quot;setuptools.build_meta&quot;
</code></pre>
"
"<p>I just discovered new features of Python 3.11 like ExceptionGroup and TaskGroup and I'm confused with the following TaskGroup behavior: if one or more tasks inside the group fails then all other normal tasks are cancelled and <strong>I have no chance to change that behavior</strong>
Example:</p>
<pre><code>async def f_error():
    raise ValueError()

async def f_normal(arg):
    print('starting', arg)
    await asyncio.sleep(1)
    print('ending', arg)


async with asyncio.TaskGroup() as tg:
    tg.create_task(f_normal(1))
    tg.create_task(f_normal(2))
    tg.create_task(f_error())

# starting 1
# starting 2
#----------
#&lt; traceback of the error here &gt;
</code></pre>
<p>In the example above I cannot make &quot;ending 1&quot; and &quot;ending 2&quot; to be printed. Meanwhile it will be very useful to have something like <code>asyncio.gather(return_exceptions=True)</code> option to do not cancel the remaining tasks when an error occurs.</p>
<p>You can say &quot;just do not use TaskGroup if you do not want this cancellation behavior&quot;, but the answer is I want to use new <strong>exception groups</strong> feature and it's strictly bound to TaskGroup</p>
<p>So the questions are:</p>
<ol>
<li>May I somehow utilize exception groups in asyncio without this all-or-nothing cancellation policy in TaskGroup?</li>
<li>If for the previous the answer is &quot;NO&quot;: why python developers eliminated the possibility to disable cancellation in the TaskGroup API?</li>
</ol>
"
"<p>I trying to test my endpoint with pytest</p>
<p>main.py:</p>
<pre><code>from fastapi import FastAPI, status, HTTPException, Depends
from sqlalchemy.ext.asyncio import AsyncSession
from .schema import ClientIn, ClientOut, ClientInWithID, Client, ValidationErrorSchema
from . import clients
from .database import SessionLocal


app = FastAPI()


async def get_db() -&gt; AsyncSession:
    if hasattr(get_db, &quot;db&quot;):
        db: AsyncSession = get_db.db
        return db
    db = SessionLocal()
    setattr(get_db, &quot;db&quot;, db)
    return db


@app.post(&quot;/client/&quot;,
          response_model=ClientOut,
          tags=[&quot;client&quot;],
          responses={422: {&quot;model&quot;: ValidationErrorSchema}}
          )
async def create_client(client_in: ClientIn, db: AsyncSession = Depends(get_db)) -&gt; Client:
    client = await clients.create_client(db, client_in)
    return client


@app.put(&quot;/client/&quot;,
         response_model=ClientOut | None,
         tags=[&quot;client&quot;],
         responses={422: {&quot;model&quot;: ValidationErrorSchema}, 404: {}}
         )
async def update_client(client: ClientInWithID, db: AsyncSession = Depends(get_db)) -&gt; Client | None:
    db_client = await clients.get_client_by_id(db, client.id)
    if not db_client:
        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND)
    return await clients.update_client(db, client)
</code></pre>
<p>test_main.py:</p>
<pre><code>import pytest

from httpx import AsyncClient

from app import main


@pytest.mark.asyncio
async def test_create_client():
    data = {
            &quot;phone_number&quot;: &quot;+79009999999&quot;,
            &quot;phone_operator_code&quot;: 900,
            &quot;timezone&quot;: &quot;Europe/Amsterdam&quot;,
            &quot;tag&quot;: {
                &quot;text&quot;: &quot;Any text&quot;
            }
        }

    async with AsyncClient(app=main.app, base_url=&quot;http://localhost:8000&quot;) as client:
        response = await client.post(url=&quot;client/&quot;, json=data)

    assert response.status_code == 200


@pytest.mark.asyncio
async def test_update_client():
    data = {
            &quot;id&quot;: 1,
            &quot;phone_number&quot;: &quot;+79009900000&quot;,
            &quot;phone_operator_code&quot;: 900,
            &quot;timezone&quot;: &quot;Europe/Amsterdam&quot;,
            &quot;tag&quot;: {
                &quot;text&quot;: &quot;Fuck this shit&quot;
            }
        }

    async with AsyncClient(app=main.app, base_url=&quot;http://localhost:8000&quot;) as client:
        response = await client.put(url=&quot;client/&quot;, json=data)

    assert response.status_code == 200
</code></pre>
<p>I use sqlalchemy and its connects to postgres with asyncpg, cause of asyncpg I have error:</p>
<pre><code>venv/lib/python3.11/site-packages/asyncpg/connection.py:565: in prepare
    return await self._prepare(
venv/lib/python3.11/site-packages/asyncpg/connection.py:583: in _prepare
    stmt = await self._get_statement(
venv/lib/python3.11/site-packages/asyncpg/connection.py:397: in _get_statement
    statement = await self._protocol.prepare(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

&gt;   ???
E   RuntimeError: Task &lt;Task pending name='Task-3' coro=&lt;test_update_client() running at /home/kryseyt/code/python/BackendTask1/tests/test_main.py:38&gt; cb=[_run_until_complete_cb() at /home/kryseyt/.python3.11/lib/python3.11/asyncio/base_events.py:180]&gt; got Future &lt;Future pending cb=[Protocol._on_waiter_completed()]&gt; attached to a different loop

asyncpg/protocol/protocol.pyx:168: RuntimeError
================================================ short test summary info =========================================
FAILED tests/test_main.py::test_update_client - RuntimeError: Task &lt;Task pending name='Task-3' coro=&lt;test_update_client() running at /home/kryseyt/code/python/BackendTask1/tests/test_main.py:38&gt; cb=[_run_until_complet...
======================================================================== 1 failed, 1 passed in 5.82s =========================================================================
</code></pre>
<p>I this this happens cause there are creating another event loop for work with db, but what can I do with that?<br />
Can I do something with this without mocking my database CRUD?</p>
"
"<p>Imagine I have a base class and two derived classes. I also have a factory method, that returns an object of one of the classes. The problem is, mypy or IntelliJ can't figure out which type the object is. They know it can be both, but not which one exactly. Is there any way I can help mypy/IntelliJ to figure this out WITHOUT putting a type hint next to the <code>conn</code> variable name?</p>
<pre><code>import abc
import enum
import typing


class BaseConnection(abc.ABC):
    @abc.abstractmethod
    def sql(self, query: str) -&gt; typing.List[typing.Any]:
        ...


class PostgresConnection(BaseConnection):

    def sql(self, query: str) -&gt; typing.List[typing.Any]:
        return &quot;This is a postgres result&quot;.split()

    def only_postgres_things(self):
        pass


class MySQLConnection(BaseConnection):

    def sql(self, query: str) -&gt; typing.List[typing.Any]:
        return &quot;This is a mysql result&quot;.split()

    def only_mysql_things(self):
        pass


class ConnectionType(enum.Enum):
    POSTGRES = 1
    MYSQL = 2


def connect(conn_type: ConnectionType) -&gt; typing.Union[PostgresConnection, MySQLConnection]:
    if conn_type is ConnectionType.POSTGRES:
        return PostgresConnection()
    if conn_type is ConnectionType.MYSQL:
        return MySQLConnection()


conn = connect(ConnectionType.POSTGRES)
conn.only_postgres_things()
</code></pre>
<p>Look at how IntelliJ handles this:
<a href=""https://i.sstatic.net/0DBY4.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/0DBY4.png"" alt=""enter image description here"" /></a></p>
<p>As you can see both methods: <code>only_postgres_things</code> and <code>only_mysql_things</code> are suggested when I'd like IntelliJ/mypy to figure it out out of the type I'm passing to the <code>connect</code> function.</p>
"
"<p>I am trying to move from pandas to polars but I am running into the following issue.</p>
<pre class=""lang-py prettyprint-override""><code>df = pl.DataFrame(
    {
        &quot;integer&quot;: [1, 2, 3], 
        &quot;date&quot;: [
            &quot;2010-01-31T23:00:00+00:00&quot;,
            &quot;2010-02-01T00:00:00+00:00&quot;,
            &quot;2010-02-01T01:00:00+00:00&quot;
        ]
    }
)

df = df.with_columns(
    pl.col(&quot;date&quot;).str.to_datetime().dt.convert_time_zone(&quot;Europe/Amsterdam&quot;)
)
</code></pre>
<p>Yields the following dataframe:</p>
<pre><code>shape: (3, 2)
┌─────────┬────────────────────────────────┐
│ integer ┆ date                           │
│ ---     ┆ ---                            │
│ i64     ┆ datetime[μs, Europe/Amsterdam] │
╞═════════╪════════════════════════════════╡
│ 1       ┆ 2010-02-01 00:00:00 CET        │
│ 2       ┆ 2010-02-01 01:00:00 CET        │
│ 3       ┆ 2010-02-01 02:00:00 CET        │
└─────────┴────────────────────────────────┘
</code></pre>
<p>As you can see, I transformed the datetime string from UTC to CET succesfully.</p>
<p>However, if I try to cast to <code>pl.Date</code> (as suggested <a href=""https://stackoverflow.com/questions/73212628/retrieve-date-from-datetime-column-in-polars/73212748#73212748"">here</a>), it seems to extract the date from the UTC string even though it has been transformed, e.g.:</p>
<pre class=""lang-py prettyprint-override""><code>df = df.with_columns(
    pl.col(&quot;date&quot;).cast(pl.Date).alias(&quot;valueDay&quot;)
)
</code></pre>
<pre><code>shape: (3, 3)
┌─────────┬────────────────────────────────┬────────────┐
│ integer ┆ date                           ┆ valueDay   │
│ ---     ┆ ---                            ┆ ---        │
│ i64     ┆ datetime[μs, Europe/Amsterdam] ┆ date       │
╞═════════╪════════════════════════════════╪════════════╡
│ 1       ┆ 2010-02-01 00:00:00 CET        ┆ 2010-01-31 │ # &lt;- NOT OK
│ 2       ┆ 2010-02-01 01:00:00 CET        ┆ 2010-02-01 │
│ 3       ┆ 2010-02-01 02:00:00 CET        ┆ 2010-02-01 │
└─────────┴────────────────────────────────┴────────────┘
</code></pre>
<p>The <code>valueDay</code> should be 2010-02-01 for all 3 values.</p>
<p>Can anyone help me fix this? A pandas dt.date like way to approach this would be nice.</p>
<p>By the way, what is the best way to optimize this code? Do I constantly have to assign everything to <code>df</code> or is there a way to chain all of this?</p>
"
"<p>Yolov8 and I suspect Yolov5 handle non-square images well. I cannot see any evidence of cropping the input image, i.e. detections seem to go to the enge of the longest side. Does it resize to a square 640x604 which would change the aspect ratio of objects making them more difficult to detect?</p>
<p>When training on a custom dataset starting from a pre-trained model, what does the <code>imgsz</code> (image size) parameter actually do?</p>
"
"<p>This is what I see when I send mail via Django:</p>
<blockquote>
<p>[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)</p>
</blockquote>
<p>This is my email configuration in settings.py:</p>
<pre class=""lang-py prettyprint-override""><code># Email Settings
EMAIL_BACKEND = &quot;django.core.mail.backends.smtp.EmailBackend&quot;
EMAIL_HOST = &quot;smtp.gmail.com&quot;
EMAIL_PORT = 587
EMAIL_HOST_USER = &quot;xxxxxxxx@gmail.com&quot;
EMAIL_HOST_PASSWORD = &quot;xxxxxxxxxxxxx&quot;
EMAIL_USE_TLS = True
</code></pre>
<p>And this is how I send mail:</p>
<pre class=""lang-py prettyprint-override""><code>send_mail(
    &quot;Subject&quot;,
    &quot;Test message&quot;,
    settings.EMAIL_HOST_USER,
    [&quot;xxxxxxxxx@gmail.com&quot;],
    fail_silently=False,
)
</code></pre>
"
"<p><a href=""https://pre-commit.com/"" rel=""noreferrer"">pre-commit</a> suddenly started to fail installing the <a href=""https://github.com/pycqa/isort"" rel=""noreferrer"">isort</a> hook in our builds today with the following error</p>
<pre><code>[INFO] Installing environment for https://github.com/pycqa/isort.
[INFO] Once installed this environment will be reused.
[INFO] This may take a few minutes...
An unexpected error has occurred: CalledProcessError: command: ('/builds/.../.cache/pre-commit/repo0_h0f938/py_env-python3.8/bin/python', '-mpip', 'install', '.')
return code: 1
expected return code: 0
[...]
stderr:
      ERROR: Command errored out with exit status 1:
[...]
        File &quot;/tmp/pip-build-env-_3j1398p/overlay/lib/python3.8/site-packages/poetry/core/masonry/api.py&quot;, line 40, in prepare_metadata_for_build_wheel
          poetry = Factory().create_poetry(Path(&quot;.&quot;).resolve(), with_groups=False)
        File &quot;/tmp/pip-build-env-_3j1398p/overlay/lib/python3.8/site-packages/poetry/core/factory.py&quot;, line 57, in create_poetry
          raise RuntimeError(&quot;The Poetry configuration is invalid:\n&quot; + message)
      RuntimeError: The Poetry configuration is invalid:
        - [extras.pipfile_deprecated_finder.2] 'pip-shims&lt;=0.3.4' does not match '^[a-zA-Z-_.0-9]+$'
</code></pre>
<p>It seems to be related with poetry configuration..</p>
"
"<p>I tried to translate my web app with 'babel' but i trapped with the error:
&quot;AttributeError: 'Babel' object has no attribute 'localeselector'&quot;
I use python 3.11.1 and Babel 2.11.0
This is the code related to the problem.</p>
<pre><code>from flask_babel import Babel, get_locale, gettext

@babel.localeselector
def determine_locale():
    if 'language' in session:
        return session['language']
    return request.accept_languages.best_match(['en', 'es'])
</code></pre>
<p>I succssesfully created all the files like (messages.pot, messages.po and messages.mo) using command promt. I think I have poor knowlages about decorators themselves, and can't figure out where to look to find solution.</p>
<p>I tried to import localeselector using different ways, and read a lot of examples from peoples who did manage with it. But still can'n get the point what the step I've missed.</p>
"
"<p>When writing a DataFrame to a csv file, I would like to append to the file, instead of overwriting it.</p>
<p>While pandas DataFrame has the <code>.to_csv()</code> method with the <em>mode</em> parameter available, thus allowing to append the DataFrame to a file,
None of the Polars DataFrame write methods seem to have that parameter.</p>
"
"<pre><code>    self.engine=create_engine(&quot;postgresql://postgres:12345@localhost/postgres&quot;)
            self.con = self.engine.connect()
            self.conn.autocommit = True
            self.cursor = self.conn.cursor()

     
df.to_sql(symbol, schema='xxx', con=self.con, if_exists='append',
                      index=False)


    df.to_sql(symbol, con=self.con, if_exists='append',
                      index=False)
</code></pre>
<p>I am getting this error in both cases while adding the dataframe to the postgre sql database</p>
<pre><code>meta = MetaData(self.connectable, schema=schema)
</code></pre>
<p>TypeError: <strong>init</strong>() got multiple values for argument 'schema'</p>
"
"<p>I'm using the requests library in python and keep observing some funny behavior. I am running my code in a docker container in AWS ECS and get the below error when using the request call without a proxy:</p>
<pre><code>HTTPSConnectionPool(host='www.foo.com', port=443): Max retries exceeded with url: /foo/JMD87FHD6GA068ZD9SS (Caused by SSLError(SSLZeroReturnError(6, 'TLS/SSL connection has been closed (EOF) (_ssl.c:997)')))
</code></pre>
<p>When I set a proxy for the request, (in this case using a residential based proxy), I no longer get the error. This has been consistent and I can't figure out what would cause this. I'd be happy if anyone could even just point me in the right investigative direction (most related posts are old and refer to python libraries that are very out of date).</p>
<p>I've tried both EC2 and Fargate for running the task and get the same results.</p>
<p>More information about my setup:</p>
<p>Dockerfile snippet:</p>
<pre><code>FROM python:3.10-alpine

# machine requirements
RUN apk --no-cache update
RUN apk --no-cache add python3-dev gcc libc-dev libffi-dev bash

# project requirements
COPY ./requirements ./requirements
RUN pip install --no-cache-dir -r requirements/pip-tools.txt
RUN pip-sync requirements/prod.txt --pip-args '--no-cache-dir'
</code></pre>
<p>Contents of requirements/prod.txt</p>
<pre><code>#
# This file is autogenerated by pip-compile with python 3.10
# To update, run:
#
#    pip-compile requirements/prod.in
#
anyio==3.6.1
    # via
    #   starlette
    #   watchfiles
bcrypt==4.0.0
    # via passlib
beautifulsoup4==4.10.0
    # via -r requirements/base.in
certifi==2022.6.15
    # via
    #   elasticsearch
    #   requests
cffi==1.15.0
    # via cryptography
charset-normalizer==2.1.1
    # via requests
click==8.1.3
    # via uvicorn
cloudscraper==1.2.66
    # via -r requirements/base.in
cryptography==37.0.2
    # via
    #   pymysql
    #   python-jose
dependency-injector==4.39.1
    # via -r requirements/base.in
dnspython==2.2.1
    # via email-validator
ecdsa==0.18.0
    # via python-jose
elasticsearch==7.13.4
    # via -r requirements/base.in
email-validator==1.3.0
    # via fastapi
fastapi[all]==0.85.0
    # via -r requirements/base.in
h11==0.14.0
    # via uvicorn
httptools==0.5.0
    # via uvicorn
idna==3.3
    # via
    #   anyio
    #   email-validator
    #   requests
itsdangerous==2.1.2
    # via fastapi
jinja2==3.1.2
    # via fastapi
markupsafe==2.1.1
    # via jinja2
orjson==3.8.0
    # via fastapi
passlib[bcrypt]==1.7.4
    # via -r requirements/base.in
pyasn1==0.4.8
    # via
    #   python-jose
    #   rsa
pycparser==2.21
    # via cffi
pydantic==1.10.2
    # via fastapi
pymysql[rsa]==1.0.2
    # via -r requirements/base.in
pyparsing==3.0.9
    # via cloudscraper
python-dotenv==0.21.0
    # via uvicorn
python-jose[cryptography]==3.3.0
    # via -r requirements/base.in
python-multipart==0.0.5
    # via fastapi
pyyaml==6.0
    # via
    #   fastapi
    #   uvicorn
requests==2.28.1
    # via
    #   cloudscraper
    #   fastapi
    #   requests-toolbelt
requests-toolbelt==0.9.1
    # via cloudscraper
rsa==4.9
    # via python-jose
six==1.16.0
    # via
    #   dependency-injector
    #   ecdsa
    #   python-multipart
    #   user-agent
sniffio==1.3.0
    # via anyio
soupsieve==2.3.2.post1
    # via beautifulsoup4
starlette==0.20.4
    # via fastapi
tqdm==4.63.0
    # via -r requirements/base.in
typing-extensions==4.3.0
    # via pydantic
ujson==5.5.0
    # via fastapi
urllib3==1.26.12
    # via
    #   elasticsearch
    #   requests
user-agent==0.1.10
    # via -r requirements/base.in
uvicorn[standard]==0.18.3
    # via
    #   -r requirements/base.in
    #   fastapi
uvloop==0.17.0
    # via uvicorn
watchfiles==0.17.0
    # via uvicorn
websockets==10.3
    # via uvicorn
</code></pre>
<p>Thanks in advance!</p>
"
"<p>The z-label does not show up in my figure. What is wrong?</p>
<pre><code>import matplotlib.pyplot as plt

fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')
ax.set_xlabel(&quot;x&quot;)
ax.set_ylabel(&quot;y&quot;)
ax.set_zlabel(&quot;z&quot;)
plt.show()
</code></pre>
<p><a href=""https://i.sstatic.net/DFtnt.png"" rel=""noreferrer"">Output</a></p>
<p>Neither <code>ax.set_zlabel(&quot;z&quot;)</code> nor <code>ax.set(zlabel=&quot;z&quot;)</code> works. The x- and y-labels work fine.</p>
"
"<p>In my GitHub CI I get errors like the one below since today:</p>
<pre class=""lang-none prettyprint-override""><code>File &quot;/home/runner/.local/lib/python3.8/site-packages/fb4/login_bp.py&quot;, line 12, in &lt;module&gt;
    from fb4.sqldb import db
  File &quot;/home/runner/.local/lib/python3.8/site-packages/fb4/sqldb.py&quot;, line 8, in &lt;module&gt;
    db = SQLAlchemy()
  File &quot;/home/runner/.local/lib/python3.8/site-packages/flask_sqlalchemy/__init__.py&quot;, line 758, in __init__
    _include_sqlalchemy(self, query_class)
  File &quot;/home/runner/.local/lib/python3.8/site-packages/flask_sqlalchemy/__init__.py&quot;, line 112, in _include_sqlalchemy
    for key in module.__all__:
AttributeError: module 'sqlalchemy' has no attribute '__all__'

CRITICAL: Exiting due to uncaught exception &lt;class 'ImportError'&gt;
</code></pre>
<p>without being aware of any significant commit that might cause this.
My local tests and my Jenkins CI still work.</p>
<p>I changed the matrix to stick to python 3.8 instead of also trying 3.9, 3.10 and 3.11 also taking into account that a similar problem in <a href=""https://stackoverflow.com/questions/66659283/python-3-9-attributeerror-module-posix-has-no-attribute-all"">python 3.9 AttributeError: module &#39;posix&#39; has no attribute &#39;__all__&#39;</a> was due to missing 3.9 support.</p>
<p><strong>How can the above error could be debugged and mitigated?</strong></p>
<p>My assumption is that the problem is in the setup/environment or some strange behaviour change of GitHub actions, Python, pip or the test environment or whatever.</p>
<p>I am a committer of the projects involved which are:</p>
<ul>
<li><a href=""https://github.com/WolfgangFahl/pyOnlineSpreadSheetEditing"" rel=""noreferrer"">https://github.com/WolfgangFahl/pyOnlineSpreadSheetEditing</a>
and potentially</li>
<li><a href=""https://github.com/WolfgangFahl/pyFlaskBootstrap4"" rel=""noreferrer"">https://github.com/WolfgangFahl/pyFlaskBootstrap4</a></li>
</ul>
<p><strong>Update:</strong>
After following the suggestions by @snakecharmerb the logs
Now show a version conflict</p>
<pre><code>RROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts
The conflict is caused by:
    The user requested Flask~=2.0.2
    bootstrap-flask 1.8.0 depends on Flask
    flask-dropzone 1.6.0 depends on Flask
    flask-login 0.6.2 depends on Flask&gt;=1.0.4
    flask-httpauth 1.0.0 depends on Flask
    flask-sqlalchemy 3.0.2 depends on Flask&gt;=2.2
</code></pre>
<p>Which is interesting since i am trying to avoid the ~ notation ... and indeed it was  a typo ... let's see whether the fix to upgrade Flask-SQLAlchemy&gt;=3.0.2 works now.</p>
<p>I have accepted the answer after setting the version as suggested.
There are followup problems but the question is answered.</p>
"
"<p>I just want to get class data in my python script like: <em>person, car, truck, dog</em>  but my output more than this. Also I can not use results as a string.</p>
<p><strong>Python script:</strong></p>
<pre><code>from ultralytics import YOLO

model = YOLO(&quot;yolov8n.pt&quot;) 
results = model.predict(source=&quot;0&quot;)
</code></pre>
<p><strong>Output:</strong></p>
<pre><code>0: 480x640 1 person, 1 car, 7.1ms
0: 480x640 1 person, 1 car, 7.2ms
0: 480x640 1 person, 1 car, 7.1ms
0: 480x640 1 person, 1 car, 7.1ms
0: 480x640 1 person, 1 car, 7.1ms
0: 480x640 1 person, 7.9ms
0: 480x640 1 person, 7.1ms
0: 480x640 1 person, 1 car, 7.1ms
0: 480x640 1 person, 1 car, 7.1ms
</code></pre>
"
"<pre><code>[INFO] Installing environment for https://github.com/pycqa/isort.
[INFO] Once installed this environment will be reused.
[INFO] This may take a few minutes...
An unexpected error has occurred: CalledProcessError: command: ('/builds/.../.cache/pre-commit/repo0_h0f938/py_env-python3.8/bin/python', '-mpip', 'install', '.')
return code: 1
expected return code: 0
[...]
stderr:
      ERROR: Command errored out with exit status 1:
[...]
        File &quot;/tmp/pip-build-env-_3j1398p/overlay/lib/python3.8/site-packages/poetry/core/masonry/api.py&quot;, line 40, in prepare_metadata_for_build_wheel
          poetry = Factory().create_poetry(Path(&quot;.&quot;).resolve(), with_groups=False)
        File &quot;/tmp/pip-build-env-_3j1398p/overlay/lib/python3.8/site-packages/poetry/core/factory.py&quot;, line 57, in create_poetry
          raise RuntimeError(&quot;The Poetry configuration is invalid:\n&quot; + message)
      RuntimeError: The Poetry configuration is invalid:
        - [extras.pipfile_deprecated_finder.2] 'pip-shims&lt;=0.3.4' does not match '^[a-zA-Z-_.0-9]+$'
</code></pre>
<p>I know I can upgrade the hook to isort-5.12.0 to fix the issue.
However, our project are using python-3.7, isort-5.12.0 does not support it. Considering compatibility, we don't want to update python for now. What should I do?</p>
"
"<p>I have dataframe in pandas :- purchase_df. I want to convert it to sql table so I can perform sql query in pandas. I tried this method</p>
<pre><code>purchase_df.to_sql('purchase_df', con=engine, if_exists='replace', index=False)
</code></pre>
<p>It throw an error</p>
<pre><code>TypeError: __init__() got multiple values for argument 'schema'
</code></pre>
<p>I have dataframe name purchase_df and I need to perform sql query on it. I need to perform sql query on this dataframe like this ....<code>engine.execute('''select * from purchase_df where condition''')</code>. For this I need to convert dataframe into sql table as in our server pandas_sql is not installed only sql alchemy is installed.</p>
<p>I ran this code in pycharm locally and it work perfectly fine but when i tried this in databrick notebook it is showing an error. Even though week ago it was running fine in databrick notebook too. Help me to fix this.</p>
<p>note:- pandas version '1.3.4'
Name: SQLAlchemy
Version: 2.0.0</p>
"
"<p>I'm working with optuna for hyperparameter tuning of ML models in Python. While defining the objective function for tuning a Deep Learning model I tried to define a list of choices from which the <code>trail.suggest_int</code> can pick up values from.
<strong>For example</strong> -</p>
<pre><code>'batch_size': trial.suggest_int('batch_size', [16, 32, 64, 128, 256])
</code></pre>
<p>optuna documentation suggest that <code>trial.suggest_int</code> should be in the following format</p>
<pre><code>'some_param': trial.suggest_int('some_param', low, high, step)
</code></pre>
<p>my code looks something like below</p>
<pre><code>def objective(trial):
        DL_param = {
            'learning_rate': trial.suggest_float('learning_rate', 1e-3, 1e-1),
            'optimizer': trial.suggest_categorical('optimizer', [&quot;Adam&quot;, &quot;RMSprop&quot;, &quot;SGD&quot;]),
            'h_units': trial.suggest_int('h_units', 50, 250, step = 50),
            'alpha': trial.suggest_float('alpha', [0.001,0.01, 0.1, 0.2, 0.3]),
            'batch_size': trial.suggest_int('batch_size', [16, 32, 64, 128, 256]),
        }
        DL_model = build_model(DL_param)
        DL_model.compile(optimizer=DL_param['optimizer'], loss='mean_squared_error')
        DL_model.fit(x_train, y_train, validation_split = 0.3, shuffle = True,
                                  batch_size = DL_param['batch_size'], epochs = 30)
        y_pred_2 = DL_model.predict(x_test)
        return mse(y_test_2, y_pred_2, squared=True)
</code></pre>
<p>I'm facing problem in defining a list for the parameters <code>'alpha'</code> and <code>'batch_size'</code>. Is there a way? something like <code>trial.suggest_categorical</code> can pick strings from the given list like in the above code</p>
<pre><code>'optimizer': trial.suggest_categorical('optimizer', [&quot;Adam&quot;, &quot;RMSprop&quot;, &quot;SGD&quot;])
</code></pre>
<p>Any suggestions are welcome. Thanks in advance.</p>
"
"<p>A is co occurrence dataframe. Why it shown AttributeError: module 'networkx' has no attribute 'from_numpy_matrix'</p>
<pre><code>import numpy as np
import networkx as nx
import matplotlib
A=np.matrix(coocc)
G=nx.from_numpy_matrix(A)
</code></pre>
"
"<h1>Problem summary</h1>
<p>I am very new to python package development. I developed a package and <a href=""https://test.pypi.org/project/spark-map/"" rel=""nofollow noreferrer"">published it at TestPyPI</a>. I install this package trough <code>pip</code> with no errors. However, python is giving me a &quot;ModuleNotFoundError&quot; when I try to import it, and I have no idea why. Can someone help me?</p>
<h1>Repro steps</h1>
<p>First, I install the package with:</p>
<pre><code>pip install -i https://test.pypi.org/simple/ spark-map==0.2.76
</code></pre>
<p>Then, I open a new terminal, start the python interpreter, and try to import this package, but python gives me a <code>ModuleNotFoundError</code>:</p>
<pre class=""lang-py prettyprint-override""><code>&gt;&gt;&gt; import spark_map
Traceback (most recent call last):
  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;
ModuleNotFoundError: No module named 'spark_map'
</code></pre>
<h1>What I discover</h1>
<ul>
<li><p>When I <code>cd</code> to the root folder of the package, and open the python interpreter, and run <code>import spark_map</code>, it works fine with no errors;</p>
</li>
<li><p>That <code>pip</code> did not installed the package succesfully; However I checked this. I got no error messages when I install the package, and when I run <code>pip list</code> after the <code>pip install</code> command, I see <code>spark_map</code> on the list of installed packages.</p>
</li>
</ul>
<pre class=""lang-bash prettyprint-override""><code>&gt; pip list
... many packages
spark-map                0.2.76
... more packages
</code></pre>
<ul>
<li>The folder where <code>spark_map</code> was installed can be out of the module search path of Python; I checked this as well. <code>pip</code> is installing the package on a folder called <code>Python310\lib\site-packages</code>, and this folder is included inside the <code>sys.path</code> variable:</li>
</ul>
<pre><code>&gt;&gt;&gt; import sys
&gt;&gt;&gt; for path in sys.path:
...     print(path)

C:\Users\pedro\AppData\Local\Programs\Python\Python310\python310.zip
C:\Users\pedro\AppData\Local\Programs\Python\Python310\DLLs
C:\Users\pedro\AppData\Local\Programs\Python\Python310\lib
C:\Users\pedro\AppData\Local\Programs\Python\Python310
C:\Users\pedro\AppData\Local\Programs\Python\Python310\lib\site-packages
C:\Users\pedro\AppData\Local\Programs\Python\Python310\lib\site-packages\win32
C:\Users\pedro\AppData\Local\Programs\Python\Python310\lib\site-packages\win32\lib
C:\Users\pedro\AppData\Local\Programs\Python\Python310\lib\site-packages\Pythonwin
</code></pre>
<h1>Information about the system</h1>
<p>I am on Windows 10, Python 3.10.9, trying to install and import the <code>spark_map</code> package, version 0.2.76.(<a href=""https://test.pypi.org/project/spark-map/"" rel=""nofollow noreferrer"">https://test.pypi.org/project/spark-map/</a>).</p>
<h1>Information about the code</h1>
<p><a href=""https://github.com/pedropark99/spark_map"" rel=""nofollow noreferrer"">The package source code is hosted at GitHub</a>, and the folder structure of this package is essentially this:</p>
<pre><code>root
│
├───spark_map
│   ├───__init__.py
│   ├───functions.py
│   └───mapping.py
│
├───tests
│   ├───functions
│   └───mapping
│
├───.gitignore
├───LICENSE
├───pyproject.toml
├───README.md
└───README.rst
</code></pre>
<p>The <code>pyproject.toml</code> file of the package:</p>
<pre class=""lang-ini prettyprint-override""><code>[build-system]
requires = [&quot;setuptools&gt;=61.0&quot;, &quot;toml&quot;]
build-backend = &quot;setuptools.build_meta&quot;

[project]
name = &quot;spark_map&quot;
version = &quot;0.2.76&quot;
authors = [
  { name=&quot;Pedro Faria&quot;, email=&quot;pedropark99@gmail.com&quot; }
]
description = &quot;Pyspark implementation of `map()` function for spark DataFrames&quot;
readme = &quot;README.md&quot;
requires-python = &quot;&gt;=3.7&quot;
license = { file = &quot;LICENSE.txt&quot; }
classifiers = [
    &quot;Programming Language :: Python :: 3&quot;,
    &quot;License :: OSI Approved :: MIT License&quot;,
    &quot;Operating System :: OS Independent&quot;,
]

dependencies = [
    &quot;pyspark&quot;,
    &quot;setuptools&quot;,
    &quot;toml&quot;
]

[project.urls]
Homepage = &quot;https://pedropark99.github.io/spark_map/&quot;
Repo = &quot;https://github.com/pedropark99/spark_map&quot;
Issues = &quot;https://github.com/pedropark99/spark_map/issues&quot;


[tool.pytest.ini_options]
pythonpath = [
  &quot;.&quot;
]


[tool.setuptools]
py-modules = []
</code></pre>
<h1>What I tried</h1>
<p>As @Dorian Turba suggested, I moved the source code into a <code>src</code> folder. Now, the structure of the package is this:</p>
<pre><code>root
├───src
│   └───spark_map
│       ├───__init__.py
│       ├───functions.py
│       └───mapping.py
│
├───tests
├───.gitignore
├───LICENSE
├───pyproject.toml
├───README.md
└───README.rst
</code></pre>
<p>After that, I executed <code>python -m pip install -e .</code> (the log of this command is on the image below). The package was compiled and installed succesfully. However, when I open a new terminal, in a different location, and try to run <code>python -c &quot;import spark_map&quot;</code>, I still get the same error.</p>
<p><a href=""https://i.sstatic.net/zVNQQ.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/zVNQQ.png"" alt=""enter image description here"" /></a></p>
<p>I also tried to start a virtual environment (with <code>python -m venv env</code>), and install the package inside this virtual environment (with <code>pip install -e .</code>). Then, I executed <code>python -c &quot;import spark_map&quot;</code>. But the problem still remains. I executed <code>pip list</code> too, to check if the package was installed. The full log of commands is on the image below:</p>
<p><a href=""https://i.sstatic.net/ZG7tW.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/ZG7tW.png"" alt=""enter image description here"" /></a></p>
"
"<p>I was reading an article and about collection.abc and typing class in the python standard library and discover both classes have the same features.</p>
<p>I tried both options using the code below and got the same results</p>
<pre><code>from collections.abc import Sequence

def average(sequence: Sequence):
    return sum(sequence) / len(sequence)

print(average([1, 2, 3, 4, 5]))  # result is 3.0

from typing import Sequence

def average(sequence: Sequence):
    return sum(sequence) / len(sequence)

print(average([1, 2, 3, 4, 5])) # result is 3.0


</code></pre>
<p>Under what condition will collection.abc become a better option to typing. Are there benefits of using one over the other?</p>
"
"<p>I'm sending my servers microphone's audio to the browser (mostly like <a href=""https://stackoverflow.com/a/56037682/4871482"">this</a> post but with some modified options).</p>
<p>All works fine, until you head over to a mobile or safari, where it doesn't work at all. I've tried using something like <a href=""https://howlerjs.com/"" rel=""nofollow noreferrer"">howler</a> to take care of the frontend but with not success (still works in chrome and on the computer but not on the phones Safari/Chrome/etc). <code>&lt;audio&gt; ... &lt;/audio&gt;</code> works fine in chrome but only on the computer.</p>
<pre><code>function play_audio() {
  var sound = new Howl({
    src: ['audio_feed'],
    format: ['wav'],
    html5: true,
    autoplay: true
  });
  sound.play();
}
</code></pre>
<p><strong>How does one send a wav-generated audio feed which is 'live' that works in any browser?</strong></p>
<p><strong>EDIT 230203:</strong></p>
<p>I have narrowed the error down to headers (at least what I think is causing the errors).</p>
<p>What headers should one use to make the sound available in all browsers?</p>
<p>Take this simple <code>app.py</code> for example:</p>
<pre><code>from flask import Flask, Response, render_template
import pyaudio
import time

app = Flask(__name__)

@app.route('/')
def index():
    return render_template('index.html', headers={'Content-Type': 'text/html'})

def generate_wav_header(sampleRate, bitsPerSample, channels):
    datasize = 2000*10**6
    o = bytes(&quot;RIFF&quot;,'ascii')
    o += (datasize + 36).to_bytes(4,'little')
    o += bytes(&quot;WAVE&quot;,'ascii')
    o += bytes(&quot;fmt &quot;,'ascii')
    o += (16).to_bytes(4,'little')
    o += (1).to_bytes(2,'little')
    o += (channels).to_bytes(2,'little')
    o += (sampleRate).to_bytes(4,'little')
    o += (sampleRate * channels * bitsPerSample // 8).to_bytes(4,'little')
    o += (channels * bitsPerSample // 8).to_bytes(2,'little')
    o += (bitsPerSample).to_bytes(2,'little')
    o += bytes(&quot;data&quot;,'ascii')
    o += (datasize).to_bytes(4,'little')
    return o

def get_sound(InputAudio):

    FORMAT = pyaudio.paInt16
    CHANNELS = 2
    CHUNK = 1024
    SAMPLE_RATE = 44100
    BITS_PER_SAMPLE = 16

    wav_header = generate_wav_header(SAMPLE_RATE, BITS_PER_SAMPLE, CHANNELS)

    stream = InputAudio.open(
        format=FORMAT,
        channels=CHANNELS,
        rate=SAMPLE_RATE,
        input=True,
        input_device_index=1,
        frames_per_buffer=CHUNK
    )

    first_run = True
    while True:
       if first_run:
           data = wav_header + stream.read(CHUNK)
           first_run = False
       else:
           data = stream.read(CHUNK)
       yield(data)


@app.route('/audio_feed')
def audio_feed():

    return Response(
        get_sound(pyaudio.PyAudio()),
        content_type = 'audio/wav',
    )

if __name__ == '__main__':
    app.run(debug=True)
</code></pre>
<p>With a index.html looking like this:</p>
<pre><code>&lt;html&gt;
  &lt;head&gt;
    &lt;title&gt;Test audio&lt;/title&gt;
  &lt;/head&gt;
  &lt;body&gt;
    &lt;button onclick=&quot;play_audio()&quot;&gt;
      Play audio
    &lt;/button&gt;
    &lt;div id=&quot;audio-feed&quot;&gt;&lt;/div&gt;
  &lt;/body&gt;
&lt;script&gt;

  function play_audio() {
    var audio_div = document.getElementById('audio-feed');
    const audio_url = &quot;{{ url_for('audio_feed') }}&quot;
    audio_div.innerHTML = &quot;&lt;audio controls&gt;&lt;source src=&quot;+audio_url+&quot; type='audio/x-wav;codec=pcm'&gt;&lt;/audio&gt;&quot;;
  }

&lt;/script&gt;
&lt;/html&gt;
</code></pre>
<p>Fire upp the flask development server <code>python app.py</code> and test with chrome, if you have a microphone you will hear the input sound (headphones preferably, otherwise you'll get a sound loop). Firefox works fine too.</p>
<p>But If you try the same app with any browser on an iPhone you'll get no sound, and the same goes for safari on MacOS.</p>
<p>There's no errors and you can see that the byte stream of the audio is getting downloaded in safari, but still no sound.</p>
<p>What is causing this? I think I should use some kind of headers in the audio_feed response but with hours of debugging I cannot seem to find anything for this.</p>
<p><strong>EDIT 230309:</strong></p>
<p>@Markus is pointing out to follow <code>RFC7233 HTTP Range Request</code>. And that's probably it. While firefox, chrome and probably more browsers on desktop send <code>byte=0-</code> as header request, safari and browsers used on iOS send <code>byte=0-1</code> as header request.</p>
"
"<pre class=""lang-py prettyprint-override""><code>from typing import Union
from pydantic import BaseModel, Field


class Category(BaseModel):
    name: str = Field(alias=&quot;name&quot;)


class OrderItems(BaseModel):
    name: str = Field(alias=&quot;name&quot;)
    category: Category = Field(alias=&quot;category&quot;)
    unit: Union[str, None] = Field(alias=&quot;unit&quot;)
    quantity: int = Field(alias=&quot;quantity&quot;)
</code></pre>
<p>When instantiated like this:</p>
<pre class=""lang-py prettyprint-override""><code>OrderItems(**{'name': 'Test','category':{'name': 'Test Cat'}, 'unit': 'kg', 'quantity': 10})
</code></pre>
<p>It returns data like this:</p>
<pre class=""lang-py prettyprint-override""><code>OrderItems(name='Test', category=Category(name='Test Cat'), unit='kg', quantity=10)
</code></pre>
<p>But I want the output like this:</p>
<pre class=""lang-py prettyprint-override""><code>OrderItems(name='Test', category='Test Cat', unit='kg', quantity=10)
</code></pre>
<p>How can I achieve this?</p>
"
"<p>Using setuptools, is it possible to list another editable package as a dependency for an editable package?</p>
<p>I'm trying to develop a collection of packages in order to use them across different production services, one of these packages (<code>my_pkg_1</code>) depends on a subset of my package collection (<code>my_pkg_2</code>, <code>my_pkg_x</code>, ...), so far, I've managed to put together this <code>pyproject.toml</code>:</p>
<pre class=""lang-ini prettyprint-override""><code>[build-system]
requires = [&quot;setuptools&quot;]
build-backend = &quot;setuptools.build_meta&quot;

[project]
name = &quot;my_pkg_1&quot;
version = &quot;0.0.1&quot;
dependencies = [
    &quot;my_pkg_2 @ file:///somewhere/in/mysystem/my_pkg_2&quot;
]
</code></pre>
<p>which <em>does</em> work when/for installing <code>my_pkg_1</code> in editable mode, and it does install <code>my_pkg_2</code> but not in editable mode. this is what I see when I <code>pip list</code>:</p>
<pre class=""lang-bash prettyprint-override""><code>Package         Version Editable project location
--------------- ------- -------------------------
my_pkg_2         0.0.1
my_pkg_1         0.0.1   /somewhere/in/mysystem/my_pkg_1
</code></pre>
<p>Is what I'm trying to do even possible? if so, how?</p>
"
"<p>I am trying to perform inference with the onnxruntime-gpu. Therefore, I installed CUDA, CUDNN and onnxruntime-gpu on my system, and checked that my GPU was compatible (versions listed below).</p>
<p>When I attempt to start an inference session, I receive the following warning:</p>
<pre class=""lang-bash prettyprint-override""><code>&gt;&gt;&gt; import onnxruntime as rt
&gt;&gt;&gt; rt.get_available_providers()
['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider']
&gt;&gt;&gt; rt.InferenceSession(&quot;[ PATH TO MODEL .onnx]&quot;, providers= ['CUDAExecutionProvider'])
2023-01-31 09:07:03.289984495 [W:onnxruntime:Default, onnxruntime_pybind_state.cc:578 CreateExecutionProviderInstance] Failed to create CUDAExecutionProvider. Please reference https://onnxruntime.ai/docs/reference/execution-providers/CUDA-ExecutionProvider.html#requirements to ensure all dependencies are met.
&lt;onnxruntime.capi.onnxruntime_inference_collection.InferenceSession object at 0x7f740b4af100&gt;
</code></pre>
<p>However, <a href=""https://stackoverflow.com/questions/75267445/why-does-onnxruntime-fail-to-create-cudaexecutionprovider-in-linuxubuntu-20/75267493#75267493"">if I import torch first</a>, inference runs on my GPU, and I see my python program listed under nvidia-smi as soon as I start the inference session:</p>
<pre class=""lang-bash prettyprint-override""><code>$ python
Python 3.8.16 (default, Dec  7 2022, 01:12:06)
[GCC 11.3.0] on linux
Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.
&gt;&gt;&gt; import torch
&gt;&gt;&gt; import onnxruntime as rt
&gt;&gt;&gt; sess = rt.InferenceSession(&quot;PATH TO MODEL . onnx&quot;, providers=['CUDAExecutionProvider'])
&gt;&gt;&gt;
</code></pre>
<p>Does anyone know why this is the case?
The import order is important; if I import torch after importing the onnxruntime, I receive the same warning as if I hadn't imported torch.</p>
<p>I checked the <code>__init__</code> of the torch package, and tracked down the helpful lines of code to loading <code>libtorch_global_deps.so</code>:</p>
<pre class=""lang-py prettyprint-override""><code>
import ctypes
lib_path = '[ path to my .venv38]/lib/python3.8/site-packages/torch/lib/libtorch_global_deps.so'
ctypes.CDLL(lib_path, mode=ctypes.RTLD_GLOBAL)
</code></pre>
<pre class=""lang-bash prettyprint-override""><code>$ python
Python 3.8.16 (default, Dec  7 2022, 01:12:06)
[GCC 11.3.0] on linux
Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.
&gt;&gt;&gt; import ctypes
&gt;&gt;&gt; lib_path = '[ path to my .venv38]/lib/python3.8/site-packages/torch/lib/libtorch_global_deps.so'
&gt;&gt;&gt; ctypes.CDLL(lib_path, mode=ctypes.RTLD_GLOBAL)
&gt;&gt;&gt; import onnxruntime as rt
&gt;&gt;&gt; sess = rt.InferenceSession(&quot;PATH TO MODEL . onnx&quot;, providers=['CUDAExecutionProvider'])
&gt;&gt;&gt;
</code></pre>
<p>also does the trick.</p>
<h3>Installed versions</h3>
<ul>
<li>NVIDIA-SMI 510.108.03</li>
<li>Driver Version: 510.108.03</li>
<li>CUDA Version: 11.6</li>
<li>CuDNN Version: cudnn-11.4-linux-x64-v8.2.4.15</li>
<li>onnx==1.12.0</li>
<li>onnxruntime-gpu==1.13.1</li>
<li>torch==1.12.1+cu116</li>
<li>torchvision==0.13.1+cu116</li>
<li>Python version 3.8</li>
<li>Ubuntu 22.04 5.19.3-051903-generic</li>
</ul>
<p>Python packages installed in a virtual environemnt.</p>
"
"<p>I have a poetry project that is not using setuptools</p>
<pre><code>[tool.poetry.dependencies]
python = &quot;&gt;=3.9,&lt;3.11&quot;
opencv-python = &quot;^4.7.0.68&quot;
tensorflow-macos = &quot;^2.11.0&quot;
tensorflow-metal = &quot;^0.7.0&quot;
</code></pre>
<p>but I keep getting this error in pycharm. Command from screenshot:</p>
<pre><code>/Users/mc/Library/Caches/pypoetry/virtualenvs/besafe-_8yAv-v6-py3.9/bin/Python /Users/mc/Library/Application Support/JetBrains/Toolbox/apps/PyCharm-P/ch-0/223.8214.51/PyCharm.app/Contents/plugins/python/helpers/packaging_tool.py list
</code></pre>
<p>It just pops up without any action from my side. It seems like PyCharm is doing some execution under the hood but I do not know what is it.</p>
<p>I do not understand how am I supposed to fix this?</p>
<p><a href=""https://i.sstatic.net/2TSGq.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/2TSGq.png"" alt=""enter image description here"" /></a></p>
"
"<p>I am trying to pick up the package <a href=""https://www.pola.rs/"" rel=""nofollow noreferrer"">polars</a> from Python.
I come from an R background so appreciate this might be an incredibly easy question.</p>
<p>I want to implement a case statement where if any of the conditions below are true, it will flag it to 1 otherwise it will be 0. My new column will be called 'my_new_column_flag'</p>
<p>I am however getting the error message</p>
<blockquote>
<p>Traceback (most recent call last):
File &quot;&quot;, line 2, in 
File &quot;C:\Users\foo\Miniconda3\envs\env\lib\site-packages\polars\internals\lazy_functions.py&quot;, line 204, in col
return pli.wrap_expr(pycol(name))
TypeError: argument 'name': 'int' object cannot be converted to 'PyString'</p>
</blockquote>
<pre class=""lang-py prettyprint-override""><code>import polars as pl
import numpy as np

np.random.seed(12)

df = pl.DataFrame(
    {
        &quot;nrs&quot;: [1, 2, 3, None, 5],
        &quot;names&quot;: [&quot;foo&quot;, &quot;ham&quot;, &quot;spam&quot;, &quot;egg&quot;, None],
        &quot;random&quot;: np.random.rand(5),
        &quot;groups&quot;: [&quot;A&quot;, &quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;B&quot;],
    }
)
print(df)

df.with_columns(
    pl.when(pl.col('nrs') == 1).then(pl.col(1))
    .when(pl.col('names') == 'ham').then(pl.col(1))
    .when(pl.col('random') == 0.014575).then(pl.col(1))
    .otherwise(pl.col(0))
    .alias('my_new_column_flag')
)

</code></pre>
<p>Can anyone help?</p>
"
"<p>First of all, I'm a totally new guys in the dev world
I'm currently taking courses in AI / Data Science and one of my work is to use a SQL Database to make prediction using Prophet, then use these predition to make a PowerBI
But currently, I'm stuck with the Python code, I'm not a developer initially, so I have no clue where the problem is:</p>
<pre class=""lang-py prettyprint-override""><code>import sqlalchemy
from sqlalchemy import create_engine
import pandas as pd
from prophet import Prophet
import pymysql


engine = create_engine(&quot;mysql+pymysql://root:Password@localhost:3306/data&quot;)
query = &quot;SELECT Cle_Produit, Date_Facturation, SUM(Quantite) AS Total_Quantite FROM ventes GROUP BY         Cle_Produit, Date_Facturation&quot;
df = pd.read_sql_query(query, engine)

df = df.pivot(index='Date_Facturation', columns='Cle_Produit', values='Total_Quantite')
df = df.reset_index()
df.rename(columns={'Date_Facturation': 'ds', 'Total_Quantite': 'y'}, inplace=True)


m = Prophet()
m.fit(df)
future = m.make_future_dataframe(periods=365)
forecast = m.predict(future)

forecast[['ds', 'yhat']].to_csv('forecast.csv', index=False)
</code></pre>
<p>It returns me this message:</p>
<blockquote>
<p>Importing plotly failed. Interactive plots will not work.
Traceback (most recent call last):
File &quot;f:\Backup\Cours\Cours\Explo Data\app3.py&quot;, line 9, in 
df = pd.read_sql_query(query, engine)
File &quot;F:\Programmes\Anaconda\envs\myenv\lib\site-packages\pandas\io\sql.py&quot;,
line 397, in    read_sql_query
return pandas_sql.read_query(
File &quot;F:\Programmes\Anaconda\envs\myenv\lib\site-packages\pandas\io\sql.py&quot;,
line 1560, in read_query
result = self.execute(*args)
File &quot;F:\Programmes\Anaconda\envs\myenv\lib\site-packages\pandas\io\sql.py&quot;,
line 1405, in execute
return self.connectable.execution_options().execute(*args, **kwargs)
AttributeError: 'OptionEngine' object has no attribute 'execute'</p>
</blockquote>
<p>Please, can somebody help me?</p>
<p>I want this python script to create a csv file with the prediction from prophet.
I want Prophet to use the table ventes from the DB data, and it should use the column <code>Cle_Produit</code>, <code>Quantite</code> and <code>Date_Facturation</code></p>
"
"<p>SQLAlchemy v2.0.0 works in a different way - they have changed some of the api.</p>
<p>Following investigation I found a solution.
My code was simply:</p>
<pre><code>s_settings_df = pd.read_sql_query(query, engine_cloud)
</code></pre>
<p>The error like the title, &quot;AttributeError: 'OptionEngine' object has no attribute 'execute'&quot;</p>
<p>I will answer my own post below.</p>
<p>I tried using various versions but did not like the idea of getting locked with historic components.</p>
"
"<p>I want to create BigInteger Identity column in SQLAlchemy ORM. <a href=""https://docs.sqlalchemy.org/en/20/dialects/postgresql.html"" rel=""noreferrer"">Documentation</a> does not have any example of either ORM Identity or BigInteger Identity.</p>
<ol>
<li>Is this possible at all? I don't see any parameter for Identity type that would allow specifying inner integer type</li>
<li>How to do this? Do I have to create custom type and pass it inside <code>Mapping[]</code> brackets?</li>
</ol>
"
"<p>I was creating a Python library, I needed to compile the pyproject.toml file.</p>
<p>I runned this command:</p>
<p><code>pip-compile pyproject.toml --resolver=backtracking</code></p>
<p>I got:</p>
<p>Backend subprocess exited when trying to invoke get_requires_for_build_wheel
Failed to parse .\pyproject.toml</p>
<p>My pyproject.toml:</p>
<pre><code>[build-system]
requires = [&quot;setuptools&gt;=61.0&quot;]
build-backend = &quot;setuptools.build_meta&quot;

[project]
name = &quot;filedump&quot;
version = &quot;1.0.0&quot;
description = &quot;Save multiple values to a .svf file (not encrypted)&quot;
readme = &quot;README.md&quot;
authors = [{ name = &quot;------&quot;, email = &quot;-------------------&quot; }]
license = { file = &quot;LICENSE&quot; }
classifiers = [
    &quot;License :: OSI Approved :: MIT License&quot;,
    &quot;Programming Language :: Python&quot;,
    &quot;Programming Language :: Python :: 3&quot;,
]
keywords = [&quot;file&quot;, &quot;encoding&quot;]
requires-python = &quot;&gt;=3.9&quot;

[project.optional-dependencies]
dev = [&quot;pip-tools&quot;]

[project.urls]
Homepage = &quot;https://github.com/-----------------------&quot;

[project.scripts]
file = &quot;filedump.FileOperation()&quot;
</code></pre>
<p>My project root:</p>
<pre><code>filedump\
|-- src\
|-- |___ __init__.py
|-- LICENSE
|-- MANIFEST.in
|-- pyproject.toml
|-- README.md
</code></pre>
"
"<p>I am trying to update my Python CI environment and am working on package management right now.  I have several reasons that I do not want to use Poetry; however, one nice feature of poetry is the fact that it automatically updates the <code>pyproject.toml</code> file.  I know that pip-tools can create a <code>requirements.txt</code> file from the <code>pyproject.toml</code> file; however, is there any feature within <code>virtualenv</code> or <code>pip-tools</code> that will enable an automatic update of the <code>pyproject.toml</code> file when you install a package with pip to your virtual environment?</p>
"
"<p>As raised in <a href=""https://github.com/python/cpython/issues/88306"" rel=""nofollow noreferrer"">cpython issue 88306</a>, python <a href=""https://docs.python.org/3/library/weakref.html#weakref.WeakKeyDictionary"" rel=""nofollow noreferrer"">WeakKeyDictionary</a> fails for non hashable types. According to the discussion in the python issue above, this is an unnecessary restriction, using <code>id</code>s of the keys instead of <code>hash</code> would work just fine: In this special case <code>id</code>s are unique identifiers for the keys in the WeakKeyDictionary, because the keys are automatically removed when the original object is deleted. It is important to be aware that using ids instead of hashes is only feasible in this very special case.</p>
<p>We can tweak <code>weakref.WeakKeyDictionary</code> (<a href=""https://gist.github.com/ptrba/b198f0cf6c22047df77483e8aa28f408"" rel=""nofollow noreferrer"">see gist</a>) to achieve the desired behaviour. In summary, this implementation wraps the <code>weakref</code> keys as follows:</p>
<pre class=""lang-py prettyprint-override""><code>class _IdKey:
    def __init__(self, key):
        self._id = id(key)

    def __hash__(self):
        return self._id

    def __eq__(self, other: typing_extensions.Self):
        return self._id == other._id

    def __repr__(self):
        return f&quot;&lt;_IdKey(_id={self._id})&gt;&quot;


class _IdWeakRef(_IdKey):
    def __init__(self, key, remove: typing.Callable[[typing.Any], None]):
        super().__init__(key)
        # hold weak ref to avoid garbage collection of the remove callback
        self._ref = weakref.ref(key, lambda _: remove(self))

    def __call__(self):
        # used in weakref.WeakKeyDictionary.__copy__
        return self._ref()

    def __repr__(self):
        return f&quot;&lt;_IdKey(_id={self._id},{self._ref})&gt;&quot;

class WeakKeyIdDictionary(weakref.WeakKeyDictionary):
   &quot;&quot;&quot;
   overrides all methods involving dictionary access key 
   &quot;&quot;&quot;
   ... https://gist.github.com/barmettl/b198f0cf6c22047df77483e8aa28f408
</code></pre>
<p>However, this depends on the details of the implementation of <code>weakref.WeakKeyDictionary</code> (using python3.10 here) and is likely to break in future (or even past) versions of python. Of course, alternatively one can just rewrite an entirely new class.</p>
<p>It is also possible to implement a custom <code>__hash__</code> method for all classes, but this won't work when dealing with external code and will give unreliable hashes for use cases beyond <code>weakref.WeakKeyDictionary</code>. We can also monkey patch <code>__hash__</code>, but this is not possible in particular for built in classes and will have unintended effects in other parts of the code.</p>
<p>Thus the following question: How should one store non hashable items in a WeakKeyDictionary?</p>
"
"<p>I am trying to store data retrieved from a website into MySQL database via a pandas data frame. However, when I make the function call <code>df.to_sql()</code>, the compiler give me an error message saying: <code>AttributeError: 'Connection' object has no attribute 'connect'</code>. I tested it couple times and I am sure that there is neither connection issue nor table existence issue involved. Is there anything wrong with the code itself? The code I am using is the following:</p>
<pre><code>    from sqlalchemy import create_engine, text
    import pandas as pd
    import mysql.connector

    
    config = configparser.ConfigParser()
    config.read('db_init.INI')
    password = config.get(&quot;section_a&quot;, &quot;Password&quot;)
    host = config.get(&quot;section_a&quot;, &quot;Port&quot;)
    database = config.get(&quot;section_a&quot;, &quot;Database&quot;)

    engine = create_engine('mysql+mysqlconnector://root:{0}@{1}/{2}'.
                           format(password, host, database),
                           pool_recycle=1, pool_timeout=57600, future=True)
    
    conn = engine.connect()
    df.to_sql(&quot;tableName&quot;, conn, if_exists='append', index = False)
</code></pre>
<p>The full stack trace looks like this:</p>
<pre><code>Traceback (most recent call last):
  File &quot;/Users/chent/Desktop/PFSDataParser/src/FetchPFS.py&quot;, line 304, in &lt;module&gt;
    main()
  File &quot;/Users/chent/Desktop/PFSDataParser/src/FetchPFS.py&quot;, line 287, in main
    insert_to_db(experimentDataSet, expName)
  File &quot;/Users/chent/Desktop/PFSDataParser/src/FetchPFS.py&quot;, line 89, in insert_to_db
    df.to_sql(tableName, conn, if_exists='append', index = False)
  File &quot;/Users/chent/opt/anaconda3/lib/python3.9/site-packages/pandas/core/generic.py&quot;, line 2951, in to_sql
    return sql.to_sql(
  File &quot;/Users/chent/opt/anaconda3/lib/python3.9/site-packages/pandas/io/sql.py&quot;, line 698, in to_sql
    return pandas_sql.to_sql(
  File &quot;/Users/chent/opt/anaconda3/lib/python3.9/site-packages/pandas/io/sql.py&quot;, line 1754, in to_sql
    self.check_case_sensitive(name=name, schema=schema)
  File &quot;/Users/chent/opt/anaconda3/lib/python3.9/site-packages/pandas/io/sql.py&quot;, line 1647, in check_case_sensitive
    with self.connectable.connect() as conn:

AttributeError: 'Connection' object has no attribute 'connect'
</code></pre>
<p>The version of pandas I am using is 1.4.4, sqlalchemy is 2.0</p>
<p>I tried to make a several execution of sql query, for example, <code>CREATE TABLE xxx IF NOT EXISTS</code> or <code>SELECT * FROM</code>, all of which have given me the result I wish to see.</p>
"
"<p>In TypeScript you can override type inferences with the <code>as</code> keyword</p>
<pre><code>const canvas = document.querySelector('canvas') as HTMLCanvasElement;
</code></pre>
<p>Are there similar techniques in Python3.x typing without involving runtime casting? I want to do something like the following:</p>
<pre class=""lang-py prettyprint-override""><code>class SpecificDict(TypedDict):
    foo: str
    bar: str

res = request(url).json() as SpecificDict
</code></pre>
"
"<p>I have the following line of code that keeps giving me an error that Engine object has no object execute. I think I have everything right but no idea what keeps happening. It seemed others had this issue and restarting their notebooks worked. I'm using Pycharm and have restarted that without any resolution. Any help is greatly appreciated!</p>
<pre><code>import pandas as pd
from sqlalchemy import create_engine, text
import sqlalchemy
import pymysql


masterlist = pd.read_excel('Masterlist.xlsx')

user = 'root'
pw = 'test!*'
db = 'hcftest'

engine = create_engine(&quot;mysql+pymysql://{user}:{pw}@localhost:3306/{db}&quot;
                           .format(user=user, pw=pw, db=db))


results = engine.execute(text(&quot;SELECT * FROM companyname;&quot;))

for row in results:
    print(row)
</code></pre>
"
"<p>I am trying to loop through a Polars recordset using the following code:</p>
<pre class=""lang-py prettyprint-override""><code>import polars as pl

df = pl.DataFrame({
    &quot;start_date&quot;: [&quot;2020-01-02&quot;, &quot;2020-01-03&quot;, &quot;2020-01-04&quot;],
    &quot;Name&quot;: [&quot;John&quot;, &quot;Joe&quot;, &quot;James&quot;]
})

for row in df.rows():
    print(row)
</code></pre>
<pre><code>('2020-01-02', 'John')
('2020-01-03', 'Joe')
('2020-01-04', 'James')
</code></pre>
<p>Is there a way to specifically reference 'Name' using the named column as opposed to the index? In Pandas this would look something like:</p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd

df = pd.DataFrame({
    &quot;start_date&quot;: [&quot;2020-01-02&quot;, &quot;2020-01-03&quot;, &quot;2020-01-04&quot;],
    &quot;Name&quot;: [&quot;John&quot;, &quot;Joe&quot;, &quot;James&quot;]
})

for index, row in df.iterrows():
    df['Name'][index]
</code></pre>
<pre><code>'John'
'Joe'
'James'
</code></pre>
"
"<p>I'm trying to find out if Pandas.read_json performs some level of autodetection. For example, I have the following data:</p>
<pre><code>data_records = [
    {
        &quot;device&quot;: &quot;rtr1&quot;,
        &quot;dc&quot;: &quot;London&quot;,
        &quot;vendor&quot;: &quot;Cisco&quot;,
    },
    {
        &quot;device&quot;: &quot;rtr2&quot;,
        &quot;dc&quot;: &quot;London&quot;,
        &quot;vendor&quot;: &quot;Cisco&quot;,
    },
    {
        &quot;device&quot;: &quot;rtr3&quot;,
        &quot;dc&quot;: &quot;London&quot;,
        &quot;vendor&quot;: &quot;Cisco&quot;,
    },
]

data_index = {
    &quot;rtr1&quot;: {&quot;dc&quot;: &quot;London&quot;, &quot;vendor&quot;: &quot;Cisco&quot;},
    &quot;rtr2&quot;: {&quot;dc&quot;: &quot;London&quot;, &quot;vendor&quot;: &quot;Cisco&quot;},
    &quot;rtr3&quot;: {&quot;dc&quot;: &quot;London&quot;, &quot;vendor&quot;: &quot;Cisco&quot;},
}
</code></pre>
<p>If I do the following:</p>
<pre><code>import pandas as pd
import json

pd.read_json(json.dumps(data_records))
---
  device      dc vendor
0   rtr1  London  Cisco
1   rtr2  London  Cisco
2   rtr3  London  Cisco
</code></pre>
<p>though I get the output that I desired, the data is record based. Being that the default <code>orient</code> is columns, I would have not thought this would have worked.</p>
<p>Therefore is there some level of autodetection going on? With index based inputs the behaviour seems more inline. As this shows appears to have parsed the data based on a column orient by default.</p>
<pre><code>pd.read_json(json.dumps(data_index))

          rtr1    rtr2    rtr3
dc      London  London  London
vendor   Cisco   Cisco   Cisco

pd.read_json(json.dumps(data_index), orient=&quot;index&quot;)

          dc vendor
rtr1  London  Cisco
rtr2  London  Cisco
rtr3  London  Cisco
</code></pre>
"
"<p>I want to integrate OpenCV with YOLOv8 from <code>ultralytics</code>, so I want to obtain the bounding box coordinates from the model prediction. How do I do this?</p>
<pre><code>from ultralytics import YOLO
import cv2

model = YOLO('yolov8n.pt')
cap = cv2.VideoCapture(0)
cap.set(3, 640)
cap.set(4, 480)

while True:
    _, frame = cap.read()
    
    img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

    results = model.predict(img)

    for r in results:
        for c in r.boxes.cls:
            print(model.names[int(c)])

    cv2.imshow('YOLO V8 Detection', frame)
    if cv2.waitKey(1) &amp; 0xFF == ord(' '):
        break

cap.release()
cv2.destroyAllWindows()
</code></pre>
<p>I want to display the YOLO annotated image in OpenCV. I know I can use the stream parameter in <code>model.predict(source='0', show=True)</code>. But I want to continuously monitor the predicted class names for my program, at the same time displaying the image output.</p>
"
"<p>I have a Python3 Poetry project with a <code>pyproject.toml</code> file specifying the dependencies:</p>
<pre><code>[tool.poetry.dependencies]
python = &quot;^3.10&quot;
nltk = &quot;^3.7&quot;
numpy = &quot;^1.23.4&quot;
scipy = &quot;^1.9.3&quot;
scikit-learn = &quot;^1.1.3&quot;
joblib = &quot;^1.2.0&quot;

[tool.poetry.dev-dependencies]
pytest = &quot;^5.2&quot;
</code></pre>
<p>I export those dependencies to a <code>requirements.txt</code> file using the command <code>poetry export --without-hashes -f requirements.txt --output requirements.txt</code> resulting in the following file <code>requirements.txt</code>:</p>
<pre><code>click==8.1.3 ; python_version &gt;= &quot;3.10&quot; and python_version &lt; &quot;4.0&quot;
colorama==0.4.6 ; python_version &gt;= &quot;3.10&quot; and python_version &lt; &quot;4.0&quot; and platform_system == &quot;Windows&quot;
joblib==1.2.0 ; python_version &gt;= &quot;3.10&quot; and python_version &lt; &quot;4.0&quot;
nltk==3.8.1 ; python_version &gt;= &quot;3.10&quot; and python_version &lt; &quot;4.0&quot;
numpy==1.24.1 ; python_version &gt;= &quot;3.10&quot; and python_version &lt; &quot;4.0&quot;
regex==2022.10.31 ; python_version &gt;= &quot;3.10&quot; and python_version &lt; &quot;4.0&quot;
scikit-learn==1.2.1 ; python_version &gt;= &quot;3.10&quot; and python_version &lt; &quot;4.0&quot;
scipy==1.9.3 ; python_version &gt;= &quot;3.10&quot; and python_version &lt; &quot;4.0&quot;
threadpoolctl==3.1.0 ; python_version &gt;= &quot;3.10&quot; and python_version &lt; &quot;4.0&quot;
tqdm==4.64.1 ; python_version &gt;= &quot;3.10&quot; and python_version &lt; &quot;4.0&quot;
</code></pre>
<p>that I use to install the dependencies when building a Docker image.</p>
<p><strong>My question:</strong> How can I omit the the <code>colorama</code> dependency in the above list of requirements when calling <code>poetry export --without-hashes -f requirements.txt --output requirements.txt</code>?</p>
<p><strong>Possible solution:</strong> I could filter out the line with <code>colorama</code> by producing the <code>requirements.txt</code> file using <code>poetry export --without-hashes -f requirements.txt | grep -v colorama &gt; requirements.txt</code>. But that seems hacky and may break things in case the Colorama requirement is expressed across multiple lines in that file. Is there a better and less hacky way?</p>
<p><strong>Background:</strong> When installing this list of requirements while building the Docker image using <code>pip install -r requirements.txt</code> I get the message</p>
<pre><code>Ignoring colorama: markers 'python_version &gt;= &quot;3.10&quot; and python_version &lt; &quot;4.0&quot; and platform_system == &quot;Windows&quot;' don't match your environment
</code></pre>
<p>A coworker thinks that message looks ugly and would like it not to be visible (but personally I don't care). A call to <code>poetry show --tree</code> reveals that the Colorama dependency is required by <code>pytest</code> and is used to make terminal colors work on Windows. Omitting the library as a requirement when installing on Linux is not likely a problem in this context.</p>
"
"<p>I am trying to run a Jupyter Notebook in VS Code. However, I'm getting the following error message whenever I try to execute a cell:</p>
<pre class=""lang-none prettyprint-override""><code>Failed to start the Kernel. 
Jupyter server crashed. Unable to connect. 
Error code from Jupyter: 1
usage: jupyter.py [-h] [--version] [--config-dir] [--data-dir] [--runtime-dir]
                  [--paths] [--json] [--debug]
                  [subcommand]

Jupyter: Interactive Computing

positional arguments:
  subcommand     the subcommand to launch

options:
  -h, --help     show this help message and exit
  --version      show the versions of core jupyter packages and exit
  --config-dir   show Jupyter config dir
  --data-dir     show Jupyter data dir
  --runtime-dir  show Jupyter runtime dir
  --paths        show all Jupyter paths. Add --json for machine-readable
                 format.
  --json         output paths as machine-readable json
  --debug        output debug information about paths

Available subcommands:

Jupyter command `jupyter-notebook` not found. 
View Jupyter log for further details.
</code></pre>
<p>The Jupyter log referred to by the diagnostic message just contains the same text as the above diagnostic message repeated multiple times.</p>
<p>I believe <a href=""https://stackoverflow.com/questions/57983475/jupyter-server-crashed-unable-to-connect-error-code-from-jupyter-1"">this post</a> refers to the same issue. Unfortunately, the accepted answer does not work for me because I do not have <em>Python: Select Interpreter to Start Jupyter server</em> in my Command Palette.</p>
<p>The file was working normally this morning. I also tried uninstalling and reinstalling the extensions.</p>
<p>How can I get the Kernel to start?</p>
"
"<p>Similar questions are <a href=""https://stackoverflow.com/questions/27322804/partition-of-a-set-into-k-disjoint-subsets-with-equal-sum"">1</a> and <a href=""https://stackoverflow.com/questions/47557812/partition-a-set-into-k-subsets-with-equal-sum"">2</a> but the answers didn't help.
Assume we have a list of integers. We want to find <code>K</code> disjoint lists such that they completely cover the given list and all have the same sum. For example, if <code>A = [4, 3, 5, 6, 4, 3, 1]</code> and <code>K = 2</code> then the answer should be:</p>
<pre><code>[[3, 4, 6], [1, 3, 4, 5]]
or
[[4, 4, 5], [1, 3, 3, 6]]
</code></pre>
<p>I have written a code that only works when <code>K = 2</code> and it works fine with small lists as input but with very larger lists, because of the code's high complexity, OS terminates the task. My code is:</p>
<pre><code>def subarrays_equal_sum(l):
    from itertools import combinations

    if len(l) &lt; 2 or sum(l) % 2 != 0:
        return []
    l = sorted(l)
    list_sum = sum(l)
    all_combinations = []
    for i in range(1, len(l)):
        all_combinations += (list(combinations(l, i)))

    combinations_list = [i for i in all_combinations if sum(i) == list_sum / 2]
    if not combinations_list:
        return []
    final_result = []
    for i in range(len(combinations_list)):
        for j in range(i + 1, len(combinations_list)):
            first = combinations_list[i]
            second = combinations_list[j]
            concat = sorted(first + second)
            if concat == l and [list(first), list(second)] not in final_result:
                final_result.append([list(first), list(second)])

    return final_result
</code></pre>
<p>An answer for any value of <code>K</code> is available <a href=""https://www.techiedelight.com/k-partition-problem-print-all-subsets/"" rel=""nofollow noreferrer"">here</a>. But if we pass the arguments <code>A = [4, 3, 5, 6, 4, 3, 1]</code> and <code>K = 2</code>, their code only returns <code>[[5, 4, 3, 1],[4, 3, 6]]</code> whereas my code returns all possible lists i.e.,</p>
<p><code>[[[3, 4, 6], [1, 3, 4, 5]], [[4, 4, 5], [1, 3, 3, 6]]]</code></p>
<p>My questions are:</p>
<ol>
<li>How to improve the complexity and cost of my code?</li>
<li>How to make my code work with any value of <code>k</code>?</li>
</ol>
"
"<p>I'm attempting to run a very simple tqdm script:</p>
<pre><code>from tqdm.notebook import tqdm
for i in tqdm(range(10)):
    time.sleep(1)
</code></pre>
<p>but am met with:</p>
<pre><code>IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html
</code></pre>
<p>My ipywidgets is v8.0.4 and jupyter v1.0.0... does tqdm not work anymore with VS Code Jupyter Notebook?</p>
"
"<p>I am using Duckdb to insert data by Batch Insert</p>
<p>While using following code</p>
<pre><code>conn.execute('INSERT INTO Main SELECT * FROM df')
</code></pre>
<p>I am getting following error</p>
<pre><code>Invalid Input Error: Failed to cast value: Unimplemented type for cast (VARCHAR -&gt; NULL)
</code></pre>
<p>I tried using</p>
<pre><code>df.fillna('N/A')
</code></pre>
<p>to fill any null values to avoid the error but still I am getting the same error. How to fix this?</p>
"
"<p>Letting them compete three times (a million pops/dels each time):</p>
<pre><code>from timeit import timeit

for _ in range(3):
    t1 = timeit('b.pop(0)', 'b = bytearray(1000000)')
    t2 = timeit('del b[0]', 'b = bytearray(1000000)')
    print(t1 / t2)
</code></pre>
<p>Time ratios (<a href=""https://tio.run/##hZDBasMwDIbvfoqfXpxA6NyWwij0tvNeYIzhMCUxc@JMVjb89JnS9bDbdBGW@b5f9lxkSNPpceZ17TiNkDBSEIRxTiz3kzEzh0kqa@1zErpgt@TFx1h2kIHAXkJCyCZ9EePoHAZiatAugpxG2iTZ@IyYvqHt7JqtbWgmVsaEjC76j7LHU6I8WQHT5sfoRfS@S4yx4HOhrFFTo2xa@mHTmLPTaOg8RqirIHruaa@71uYGviFMuuPUU3WqLwZacsD1/rbKtvs5zZWrbQPb6rwtQp7Zl@rgblWr6UYd/1DvFNG@uNd/qd@v08QHFdTr@gM"" rel=""noreferrer"" title=""Python 3.8 (pre-release) – Try It Online"">Try it online!</a>):</p>
<pre><code>274.6037053753368
219.38099365582403
252.08691226683823
</code></pre>
<p>Why is <code>pop</code> that much slower at doing the same thing?</p>
"
"<p>When I do pip install dotenv it says this -</p>
<p>`Collecting dotenv
Using cached dotenv-0.0.5.tar.gz (2.4 kB)
Preparing metadata (setup.py) ... error
error: subprocess-exited-with-error</p>
<p>× python setup.py egg_info did not run successfully.
│ exit code: 1
╰─&gt; [72 lines of output]
C:\Users\Anju Tiwari\AppData\Local\Programs\Python\Python311\Lib\site-packages\setuptools\installer.py:27: SetuptoolsDeprecationWarning: setuptools.installer is deprecated. Requirements should be satisfied by
a PEP 517 installer.
warnings.warn(
error: subprocess-exited-with-error</p>
<pre><code>    python setup.py egg_info did not run successfully.
    exit code: 1
 
    [17 lines of output]
    Traceback (most recent call last):
      File &quot;&lt;string&gt;&quot;, line 2, in &lt;module&gt;
      File &quot;&lt;pip-setuptools-caller&gt;&quot;, line 14, in &lt;module&gt;
      File &quot;C:\Users\Anju Tiwari\AppData\Local\Temp\pip-wheel-xv3lcsr9\distribute_009ecda977a04fb699d5559aac28b737\setuptools\__init__.py&quot;, line 2, in &lt;module&gt;
        from setuptools.extension import Extension, Library
      File &quot;C:\Users\Anju Tiwari\AppData\Local\Temp\pip-wheel-xv3lcsr9\distribute_009ecda977a04fb699d5559aac28b737\setuptools\extension.py&quot;, line 5, in &lt;module&gt;
        from setuptools.dist import _get_unpatched
      File &quot;C:\Users\Anju Tiwari\AppData\Local\Temp\pip-wheel-xv3lcsr9\distribute_009ecda977a04fb699d5559aac28b737\setuptools\dist.py&quot;, line 7, in &lt;module&gt;
        from setuptools.command.install import install
      File &quot;C:\Users\Anju Tiwari\AppData\Local\Temp\pip-wheel-xv3lcsr9\distribute_009ecda977a04fb699d5559aac28b737\setuptools\command\__init__.py&quot;, line 8, in &lt;module&gt;
        from setuptools.command import install_scripts
      File &quot;C:\Users\Anju Tiwari\AppData\Local\Temp\pip-wheel-xv3lcsr9\distribute_009ecda977a04fb699d5559aac28b737\setuptools\command\install_scripts.py&quot;, line 3, in &lt;module&gt;
        from pkg_resources import Distribution, PathMetadata, ensure_directory
      File &quot;C:\Users\Anju Tiwari\AppData\Local\Temp\pip-wheel-xv3lcsr9\distribute_009ecda977a04fb699d5559aac28b737\pkg_resources.py&quot;, line 1518, in &lt;module&gt;
        register_loader_type(importlib_bootstrap.SourceFileLoader, DefaultProvider)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    AttributeError: module 'importlib._bootstrap' has no attribute 'SourceFileLoader'
    [end of output]
 
    note: This error originates from a subprocess, and is likely not a problem with pip.
  error: metadata-generation-failed
 
  Encountered error while generating package metadata.
 
  See above for output.
 
  note: This is an issue with the package mentioned above, not pip.
  hint: See above for details.
  Traceback (most recent call last):
    File &quot;C:\Users\Anju Tiwari\AppData\Local\Programs\Python\Python311\Lib\site-packages\setuptools\installer.py&quot;, line 82, in fetch_build_egg
      subprocess.check_call(cmd)
    File &quot;C:\Users\Anju Tiwari\AppData\Local\Programs\Python\Python311\Lib\subprocess.py&quot;, line 413, in check_call
      raise CalledProcessError(retcode, cmd)
  subprocess.CalledProcessError: Command '['C:\\Users\\Anju Tiwari\\AppData\\Local\\Programs\\Python\\Python311\\python.exe', '-m', 'pip', '--disable-pip-version-check', 'wheel', '--no-deps', '-w', 'C:\\Users\\ANJUTI~1\\AppData\\Local\\Temp\\tmpcq62ekpo', '--quiet', 'distribute']' returned non-zero exit status 1.
 
  The above exception was the direct cause of the following exception:
 
  Traceback (most recent call last):
    File &quot;&lt;string&gt;&quot;, line 2, in &lt;module&gt;
    File &quot;&lt;pip-setuptools-caller&gt;&quot;, line 34, in &lt;module&gt;
    File &quot;C:\Users\Anju Tiwari\AppData\Local\Temp\pip-install-j7w9rs9u\dotenv_0f4daa500bef4242bb24b3d9366608eb\setup.py&quot;, line 13, in &lt;module&gt;
      setup(name='dotenv',
    File &quot;C:\Users\Anju Tiwari\AppData\Local\Programs\Python\Python311\Lib\site-packages\setuptools\__init__.py&quot;, line 86, in setup
      _install_setup_requires(attrs)
    File &quot;C:\Users\Anju Tiwari\AppData\Local\Programs\Python\Python311\Lib\site-packages\setuptools\__init__.py&quot;, line 80, in _install_setup_requires
      dist.fetch_build_eggs(dist.setup_requires)
    File &quot;C:\Users\Anju Tiwari\AppData\Local\Programs\Python\Python311\Lib\site-packages\setuptools\dist.py&quot;, line 875, in fetch_build_eggs
      resolved_dists = pkg_resources.working_set.resolve(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File &quot;C:\Users\Anju Tiwari\AppData\Local\Programs\Python\Python311\Lib\site-packages\pkg_resources\__init__.py&quot;, line 789, in resolve
      dist = best[req.key] = env.best_match(
                             ^^^^^^^^^^^^^^^
    File &quot;C:\Users\Anju Tiwari\AppData\Local\Programs\Python\Python311\Lib\site-packages\pkg_resources\__init__.py&quot;, line 1075, in best_match
      return self.obtain(req, installer)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File &quot;C:\Users\Anju Tiwari\AppData\Local\Programs\Python\Python311\Lib\site-packages\pkg_resources\__init__.py&quot;, line 1087, in obtain
      return installer(requirement)
             ^^^^^^^^^^^^^^^^^^^^^^
    File &quot;C:\Users\Anju Tiwari\AppData\Local\Programs\Python\Python311\Lib\site-packages\setuptools\dist.py&quot;, line 945, in fetch_build_egg
      return fetch_build_egg(self, req)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
    File &quot;C:\Users\Anju Tiwari\AppData\Local\Programs\Python\Python311\Lib\site-packages\setuptools\installer.py&quot;, line 84, in fetch_build_egg
      raise DistutilsError(str(e)) from e
  distutils.errors.DistutilsError: Command '['C:\\Users\\Anju Tiwari\\AppData\\Local\\Programs\\Python\\Python311\\python.exe', '-m', 'pip', '--disable-pip-version-check', 'wheel', '--no-deps', '-w', 'C:\\Users\\ANJUTI~1\\AppData\\Local\\Temp\\tmpcq62ekpo', '--quiet', 'distribute']' returned non-zero exit status 1.
  [end of output]
</code></pre>
<p>note: This error originates from a subprocess, and is likely not a problem with pip.
error: metadata-generation-failed</p>
<p>× Encountered error while generating package metadata.
╰─&gt; See above for output.</p>
<p>note: This is an issue with the package mentioned above, not pip.
hint: See above for details.`</p>
<p>I tried doing <code>pip install dotenv </code>but then that error come shown above.<br />
I also tried doing <code>pip install -U dotenv </code>but it didn't work and the same error came. Can someone please help me fix this?</p>
"
"<p>I want to create a Telegram bot that checks for a new post on a website (currently every 15s for testing purposes). If so, it should send a message with content from the post into the Telegram channel.</p>
<p>For this I already have the following &quot;code skeleton&quot;:
(The fine work in terms of formatting and additions comes later)</p>
<pre><code>import requests
import asyncio
from bs4 import BeautifulSoup
from telegram import InputMediaPhoto
from telegram.ext import Updater

# Telegram Bot API Token
API_TOKEN = 'XXXXXXXXXXXXXXXXX'

# URL of the website
URL = 'https://chemiehalle.de'

# List for storing seen posts
seen_posts = []

# Function for fetching posts
def get_posts():
    # Send request to the website
    res = requests.get(URL)
    # Parse HTML content
    soup = BeautifulSoup(res.content, 'html.parser')
    # Find all posts on the website
    posts = soup.find_all('article')
    # Iterate over each post
    for post in posts:
        # Get title of the post
        title = post.find('h2', class_='entry-title').text
        # Check if post has already been seen
        if title not in seen_posts:
            # Get image URL
            image_src = post.find('img')['src']
            # Get short text of the post
            text = post.find('div', class_='entry-content clearfix').find('p').text
            # Send image, title, and text as message
            bot.bot.send_media_group(chat_id='@chemiehalleBot', media=[InputMediaPhoto(media=image_src, caption=title + '\n\n' + text)])
            # Add title of the post to the list of seen posts
            seen_posts.append(title)

# Main loop
async def main():
    while True:
        # Call get_posts function every 15s
        get_posts()
        print(&quot;Check for new posts&quot;)
        await asyncio.sleep(15)

# Initialize Telegram Bot
updater = Updater(API_TOKEN)
bot = updater.bot

# Start main loop
asyncio.run(main())

</code></pre>
<p>So far I have found out that <code>updater = Updater(API_TOKEN, use_context=True)</code> produces errors and so I have removed <code>use_context=True</code> following the instructions from other posts on this site.</p>
<p>Since that I am encountering the error <code>TypeError: __init__() missing 1 required positional argument: 'update_queue'</code> in the <code>updater = Updater(API_TOKEN)</code> line.</p>
<p>But unfortunately I don't know what to change. According to this, the constructor of Updater needs an additional argument update_queue. But I have no idea which one this should be and where I should get it from.</p>
<p>Can you help me please?</p>
<p>Thank you very much for the support!</p>
"
"<p>I am having some real trouble getting a <code>created_date</code> column working with SQLAlchemy 2.0 with the ORM model.  The best answer so far I've found is at this comment: <a href=""https://stackoverflow.com/a/33532154"">https://stackoverflow.com/a/33532154</a> however I haven't been able to make that function work.  In my (simplified) <code>models.py</code> file I have:</p>
<pre><code>import datetime
from sqlalchemy import Integer, String, DateTime
from sqlalchemy.sql import func
from sqlalchemy.orm import DeclarativeBase
from sqlalchemy.orm import Mapped
from sqlalchemy.orm import mapped_column

class Base(DeclarativeBase):
    pass

class MyTable(Base):
    __tablename__ = &quot;my_table&quot;
    id: Mapped[int] = mapped_column(primary_key=True)
    name: Mapped[str] = mapped_column(String, nullable=False)
    created_date: Mapped[datetime.datetime] = mapped_column(DateTime(timezone=True), server_default=func.now())
</code></pre>
<p>So far, so good, thinks I.  In the simplified <code>engine.py</code> I have:</p>
<pre><code>from sqlalchemy import create_engine
from sqlalchemy import select
from sqlalchemy.orm import Session

import models

def add_entry(engine, name_str):
    this_row = models.MyTable()
    this_row.name = name_str
    with Session(engine) as session:
        session.add(this_row)
        session.commit()
</code></pre>
<p>If I'm understanding correctly, the default value for the <code>created_date</code> to be a SQL function, and SQLAlchemy maps <code>now()</code> to SQLite3's <code>datetime()</code>.  With the engine set to <code>echo=True</code>, I get the following result when it tries to run this insert command (Please note, this is data from the non-simplified form but it's still pretty simple, had 3 strings instead of the one I described)</p>
<pre><code>2023-02-06 09:47:07,080 INFO sqlalchemy.engine.Engine BEGIN (implicit)
2023-02-06 09:47:07,080 INFO sqlalchemy.engine.Engine INSERT INTO coaches (d_name, bb2_name, bb3_name) VALUES (?, ?, ?) RETURNING id, created_date
2023-02-06 09:47:07,081 INFO sqlalchemy.engine.Engine [generated in 0.00016s] ('andy#1111', 'AndyAnderson', 'Killer Andy')
2023-02-06 09:47:07,081 INFO sqlalchemy.engine.Engine ROLLBACK
</code></pre>
<p>This causes an exception when it gets to the time function: <code>IntegrityError: NOT NULL constraint failed: coaches.created_date</code></p>
<p>Some additional data (I have been using the <code>rich</code> library which produces an enormous amount of debug information so I'm trying to get the best bits:</p>
<pre><code>│ ╭─────────────────────────────────────────── locals ───────────────────────────────────────────╮ │
│ │    exc_tb = &lt;traceback object at 0x00000108BD2565C0&gt;                                         │ │
│ │  exc_type = &lt;class 'sqlalchemy.exc.IntegrityError'&gt;                                          │ │
│ │ exc_value = IntegrityError('(sqlite3.IntegrityError) NOT NULL constraint failed:             │ │
│ │             coaches.created_date')                                                           │ │
│ │      self = &lt;sqlalchemy.util.langhelpers.safe_reraise object at 0x00000108BD1B79A0&gt;          │ │
│ │ traceback = None                                                                             │ │
│ │     type_ = None                                                                             │ │
│ │     value = None                                                                             │ │
│ ╰──────────────────────────────────────────────────────────────────────────────────────────────╯
</code></pre>
<p>In any event, I feel like I've gotten the wrong end of the stick on the way to make a table column automatically execute a SQL command with the <code>func</code> call. Any notions on this one?  I haven't found any direct example in the SQLAlchemy 2.0 docs, and aside from the pretty awesome comment to a similar question, I haven't found any working solutions.</p>
<p>Thanks for considering!</p>
<p>I implemented a SQLAlchemy 2.0 mapped_column with a server_default of func.now() expecting the column to automatically fill during an INSERT operation.  During the insert operation, SQLAlchemy threw an exception claiming the column NOT NULLABLE constraint was violated -- thus it was not automatically filling.</p>
"
"<p>How can I configure a Docker container to use a custom pip.conf file?</p>
<p>This does not (seem to) work for me:</p>
<pre><code>from python:3.9

COPY pip.conf ~/.config/pip/pip.conf
</code></pre>
<p>where <code>pip.conf</code> is a copy of the pip configuration that points to a proprietary package repository.</p>
"
"<p>I've just seen that my web applications docker image is enormous. A 600-MB reason is the packages I install for it. The biggest single offender is botocore with 77.7 MB.</p>
<p>Apparently this is known behavior: <a href=""https://github.com/boto/botocore/issues/1629"" rel=""noreferrer"">https://github.com/boto/botocore/issues/1629</a></p>
<p>Is it possible to redue that size?</p>
<h2>Analysis</h2>
<ul>
<li>The <code>tar.gz</code> distribution is just 10.8 MB: <a href=""https://pypi.org/project/botocore/#files"" rel=""noreferrer"">https://pypi.org/project/botocore/#files</a></li>
<li>75MB are in <a href=""https://github.com/boto/botocore/tree/develop/botocore/data"" rel=""noreferrer"">the <code>data</code> directory</a></li>
<li>For every single AWS service, there seem to be <a href=""https://github.com/boto/botocore/tree/develop/botocore/data/ec2"" rel=""noreferrer"">multiple folders</a> (some kind of versioning?) and <a href=""https://github.com/boto/botocore/blob/develop/botocore/data/ec2/2014-09-01/service-2.json"" rel=""noreferrer"">a <code>service-2.json</code></a></li>
<li>The <code>service-2.json</code> files probably use most of the space. They are not minified and they contain a lot of information that seems not to be necessary for running a production system (e.g. <code>description</code>).</li>
</ul>
<p>Is there a way to either completely avoid botocore or in any other way reduce botocores size for the Docker image? (I'm only using S3)</p>
"
"<p>I am trying to execute a basic program using <em><strong>Selenium 4.8.0</strong></em> Python clients in <em><strong>headless</strong></em> mode:</p>
<pre><code>from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service

options = Options()
options.headless = True
s = Service('C:\\BrowserDrivers\\chromedriver.exe')
driver = webdriver.Chrome(service=s, options=options)
driver.get('https://www.google.com/')
driver.quit()
</code></pre>
<p>With the following configuration:</p>
<ul>
<li>Selenium 4.8.0 Python</li>
<li>Chrome _Version 109.0.5414.120 (Official Build) (64-bit)</li>
<li>ChromeDriver 109.0.5414.25</li>
</ul>
<p>Though the program gets executed successfully there seems to a DeprecationWarning as:</p>
<pre><code>DeprecationWarning: headless property is deprecated, instead use add_argument('--headless') or add_argument('--headless=new')
</code></pre>
<p>Can anyone explain the DeprecationWarning and the required changes?</p>
"
"<p>The subject contains the whole idea. I came accross code sample where it shows something like:</p>
<pre class=""lang-py prettyprint-override""><code>async for item in getItems():
    await item.process()
</code></pre>
<p>And others where the code is:</p>
<pre class=""lang-py prettyprint-override""><code>for item in await getItems():
    await item.process()
</code></pre>
<p>Is there a notable difference in these two approaches?</p>
"
"<p>I create a fresh environment, install numpy, then install GDAL. GDAL imports successfully and I can open images using <code>gdal.Open(</code>, but I get the <code>ImportError: cannot import name '_gdal_array' from 'osgeo'</code> error when trying to use <code>ReadAsRaster</code>.</p>
<p><code>pip list</code> returns:</p>
<pre><code>GDAL       3.6.2
numpy      1.24.2
pip        23.0
setuptools 65.6.3
wheel      0.38.4
</code></pre>
<p>Completely stumped, has anyone come across this? Google tells me that installing numpy first is the solution (but that doesn't help). Help would be much appreciated.</p>
"
"<h1>Question</h1>
<p>I came across pyscript hoping to use it to document python code with mkdocs. I have looked into importing my own module. Individual files work. <strong>How do I import my own module using pyscript instead?</strong></p>
<ul>
<li>Requirements for running the example:
<ul>
<li>python package <code>numpy</code> (<code>$ pip install numpy</code>)</li>
<li>python package <code>matplotlib</code> (<code>$ pip install matplotlib</code>)</li>
<li>local webserver for live preview on localhost (eq. <code>$ npm install -g live-server</code>)</li>
</ul>
</li>
</ul>
<hr />
<ul>
<li><p>Below is an example that works with the 'just import a python file' approach, see line <code>from example import myplot</code>.</p>
</li>
<li><p>When I change the line to <code>from package.example import myplot</code> it is not working and I get the following error in firefox/chromium:</p>
<pre><code>JsException(PythonError: Traceback (most recent call last): File &quot;/lib/python3.10/site-packages/_pyodide/_base.py&quot;, line 429, in eval_code .run(globals, locals) File &quot;/lib/python3.10/site-packages/_pyodide/_base.py&quot;, line 300, in run coroutine = eval(self.code, globals, locals) File &quot;&quot;, line 1, in ModuleNotFoundError: No module named 'package' )
</code></pre>
</li>
</ul>
<p>Any help is appreciated. I found this discussion on <a href=""https://github.com/pyscript/pyscript/issues/519#issuecomment-1208881190"" rel=""noreferrer"">github</a>, but I am lost when trying to follow.</p>
<h1>Example</h1>
<p><strong>Folder structure</strong></p>
<pre><code>├── index.html
└── pycode/
    ├── example.py
    └── package/
        ├── example.py
        └── __init__.py
</code></pre>
<p><strong><code>index.html</code></strong></p>
<pre class=""lang-html prettyprint-override""><code>&lt;!doctype html&gt;
&lt;html&gt;
&lt;head&gt;
&lt;script defer src=&quot;https://pyscript.net/alpha/pyscript.js&quot;&gt;&lt;/script&gt;
&lt;link rel=&quot;stylesheet&quot; href=&quot;https://pyscript.net/alpha/pyscript.css&quot;/&gt;
&lt;py-env&gt;
- numpy
- matplotlib
- paths:
  - ./pycode/example.py
  - ./pycode/package
&lt;/py-env&gt;
&lt;/head&gt;
  
&lt;body&gt;    
    
&lt;div id=&quot;lineplot&quot;&gt;&lt;/div&gt;

&lt;py-script&gt;
from example import myplot
import matplotlib.pyplot as plt

theta,r = myplot(4)

fig, ax = plt.subplots(
  subplot_kw = {'projection': 'polar'} 
)
ax.plot(theta, r)
ax.set_rticks([0.5, 1, 1.5, 2])
ax.grid(True)

pyscript.write('lineplot',fig)

&lt;/py-script&gt;

&lt;/body&gt;
&lt;/html&gt;
</code></pre>
<p><strong><code>example.py</code></strong></p>
<pre class=""lang-py prettyprint-override""><code>import numpy as np

def myplot(val:int):
    r = np.arange(0, 2, 0.01)
    theta = val * np.pi * r
    return theta,r
</code></pre>
<p><strong><code>__init__.py</code></strong></p>
<pre class=""lang-py prettyprint-override""><code>__all__ = [ 'example.py' ]
</code></pre>
<p><strong>Intended result in the webbrowser</strong></p>
<p><a href=""https://i.sstatic.net/kBTJe.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/kBTJe.png"" alt=""enter image description here"" /></a></p>
"
"<p>The new version of SQLAlchemy introduced the ability to use type annotations using Mapped[Type] (<a href=""https://docs.sqlalchemy.org/en/20/orm/declarative_tables.html#using-annotated-declarative-table-type-annotated-forms-for-mapped-column"" rel=""noreferrer"">link</a>).</p>
<p>My question is, what should we use as an annotation for <code>sqlalchemy.types.JSON</code>? Is just <code>dict</code> will be ok?</p>
<p>I'm just using dict, but I want to understand the correct option for this.</p>
"
"<p>I dont know why this error occurs.</p>
<pre><code>pd.read_excel('data/A.xlsx', usecols=[&quot;B&quot;, &quot;C&quot;])
</code></pre>
<p>Then I get this error:</p>
<pre><code>&quot;Value must be either numerical or a string containing a wild card&quot;
</code></pre>
<p>So i change my code use nrows all data</p>
<pre><code>pd.read_excel('data/A.xlsx', usecols=[&quot;B&quot;,&quot;C&quot;], nrows=172033)
</code></pre>
<p>Then there is no error and a dataframe is created.</p>
<p>my excel file has 172034 rows, 1st is column name.</p>
"
"<p>We have a poetry project with a pyproject.toml file like this:</p>
<pre class=""lang-ini prettyprint-override""><code>[tool.poetry]
name = &quot;daisy&quot;
version = &quot;0.0.2&quot;
description = &quot;&quot;
authors = [&quot;&quot;]

[tool.poetry.dependencies]
python = &quot;^3.9&quot;
pandas = &quot;^1.5.2&quot;
DateTime = &quot;^4.9&quot;
names = &quot;^0.3.0&quot;
uuid = &quot;^1.30&quot;
pyyaml = &quot;^6.0&quot;
psycopg2-binary = &quot;^2.9.5&quot;
sqlalchemy = &quot;^2.0.1&quot;
pytest = &quot;^7.2.0&quot;

[tool.poetry.dev-dependencies]
jupyterlab = &quot;^3.5.2&quot;
line_profiler = &quot;^4.0.2&quot;
matplotlib = &quot;^3.6.2&quot;
seaborn = &quot;^0.12.1&quot;

[build-system]
requires = [&quot;poetry-core&gt;=1.0.0&quot;]
build-backend = &quot;poetry.core.masonry.api&quot;
</code></pre>
<p>When I change the file to use Python 3.11 and run <code>poetry update</code> we get the following error:</p>
<pre class=""lang-bash prettyprint-override""><code>Current Python version (3.9.7) is not allowed by the project (^3.11).
Please change python executable via the &quot;env use&quot; command.
</code></pre>
<p>I only have one env:</p>
<pre class=""lang-bash prettyprint-override""><code>&gt; poetry env list
daisy-Z0c0FuMJ-py3.9 (Activated)
</code></pre>
<p>Strangely this issue does not occur on my Macbook, only on our Linux machine.</p>
"
"<p>I try my best to move from a <code>setup.py</code> managed lib to a pure <code>pyproject.toml</code> one.
I have the following folder structure:</p>
<pre><code>tests
└── &lt;files&gt;
docs
└── &lt;files&gt;
sepal_ui
└── &lt;files&gt;
pyproject.toml
</code></pre>
<p>and in my <code>pyproject.toml</code> the following setup for file and packages discovery:</p>
<pre class=""lang-ini prettyprint-override""><code>[build-system]
requires = [&quot;setuptools&gt;=61.2&quot;, &quot;wheel&quot;]

[tool.setuptools]
include-package-data = false

[tool.setuptools.packages.find]
include = [&quot;sepal_ui*&quot;]
exclude = [&quot;docs*&quot;, &quot;tests*&quot;]
</code></pre>
<p>and in the produce wheel, I get the following:</p>
<pre><code>tests
└── &lt;files&gt;
docs
└── &lt;files&gt;
sepal_ui
└── &lt;files&gt;
sepal_ui.egg-info
└── top-level.txt
</code></pre>
<p>looking at the top-level.txt, I see that only sepal_ui is included so my question is simple why do the extra &quot;docs&quot; and &quot;tests&quot; folder are still included even if they are not used? how to get rid of them ?</p>
<p>PS: I'm aware of the MANIFEST.in solution that I will accept if it's really the only one but I found it redundant to specify in 2 files.</p>
"
"<p>I need a 'for dummies' answer to this question that I know has been asked before.</p>
<p>We're using the Serverless framework for an AWS-hosted application. <code>Runtime=python3.8</code>  Got a nice big yml file that includes 16 functions, 2 of which include layers for Cryptography and for PyNaCl, which we bring in from here - <a href=""https://github.com/keithrozario/Klayers"" rel=""noreferrer"">https://github.com/keithrozario/Klayers</a> and have used successfully for quite a while.</p>
<p>Last week, I needed to update a different function, which meant re-testing, which meant finding there's a newer version of the cryptography layer, so I updated it to have Cyptography v.39. Now I have a function that fails with the error, <code>/lib64/libc.so.6: version `GLIBC_2.28' not found (required by /var/task/cryptography/hazmat/bindings/_rust.abi3.so)</code> This function hasn't been used since 07/2022, at which time it was just fine. Apparently it's also been that long since we redeployed from Serverless.</p>
<p>Attempts to fix:</p>
<ul>
<li>I reverted to the previous Cryptography layer; no difference</li>
<li>I found an SO answer with this link <a href=""https://aws.amazon.com/premiumsupport/knowledge-center/lambda-python-package-compatible/"" rel=""noreferrer"">https://aws.amazon.com/premiumsupport/knowledge-center/lambda-python-package-compatible/</a>, followed that guide to change my local install and also to make my own layer and attach it in the console; no difference</li>
<li>Another SO answer led me here - <a href=""https://github.com/pyca/cryptography/issues/6390"" rel=""noreferrer"">https://github.com/pyca/cryptography/issues/6390</a>, which then goes to <a href=""https://github.com/pyca/cryptography/issues/6391"" rel=""noreferrer"">https://github.com/pyca/cryptography/issues/6391</a>, which also didn't help</li>
<li>Today, I found this link <a href=""https://repost.aws/questions/QU85KE-2hPQ4KDQyByKV_WIw/creating-a-lambda-zip-package-that-runs-python-package-cryptography"" rel=""noreferrer"">https://repost.aws/questions/QU85KE-2hPQ4KDQyByKV_WIw/creating-a-lambda-zip-package-that-runs-python-package-cryptography</a> and the OP says they had to make all of their lambdas from x86_64 to arm64, even if those functions weren't using Cryptography.  That's seems insane, and besides.... how??</li>
</ul>
<p>This question <a href=""https://stackoverflow.com/questions/69475140/lambda-function-failing-with-lib64-libc-so-6-version-glibc-2-18-not-found"">Lambda function failing with /lib64/libc.so.6: version `GLIBC_2.18&#39; not found</a> includes the advice to move from Cryptography v.39 all the way back to v.3.4.7 (from 03/2021), which seems like bad advice.  Surely the 14 updates between those 2 versions include some important changes.</p>
<p>I'm at a loss. I feel like I'm just running in circles, and meanwhile can't make progress on the actual function I'm trying to update because this is such a block.</p>
"
"<p>I'm not sure where to begin, so looking for some guidance.  I'm looking for a way to create some arrays/tables in one process, and have it accessible (read-only) from another.</p>
<p>So I create a <code>pyarrow.Table</code> like this:</p>
<pre class=""lang-py prettyprint-override""><code>a1 = pa.array(list(range(3)))
a2 = pa.array([&quot;foo&quot;, &quot;bar&quot;, &quot;baz&quot;])

a1
# &lt;pyarrow.lib.Int64Array object at 0x7fd7c4510100&gt;
# [
#   0,
#   1,
#   2
# ]

a2
# &lt;pyarrow.lib.StringArray object at 0x7fd7c5d6fa00&gt;
# [
#   &quot;foo&quot;,
#   &quot;bar&quot;,
#   &quot;baz&quot;
# ]

tbl = pa.Table.from_arrays([a1, a2], names=[&quot;num&quot;, &quot;name&quot;])

tbl
# pyarrow.Table
# num: int64
# name: string
# ----
# num: [[0,1,2]]
# name: [[&quot;foo&quot;,&quot;bar&quot;,&quot;baz&quot;]]
</code></pre>
<p>Now how do I read this from a different process?  I thought I would use <a href=""https://docs.python.org/3/library/multiprocessing.shared_memory.html"" rel=""noreferrer""><code>multiprocessing.shared_memory.SharedMemory</code></a>, but that didn't quite work:</p>
<pre class=""lang-py prettyprint-override""><code>shm = shared_memory.SharedMemory(name='pa_test', create=True, size=tbl.nbytes)
with pa.ipc.new_stream(shm.buf, tbl.schema) as out:
    for batch in tbl.to_batches():
        out.write(batch)

# TypeError: Unable to read from object of type: &lt;class 'memoryview'&gt;
</code></pre>
<p>Do I need to wrap the <code>shm.buf</code> with something?</p>
<p>Even if I get this to work, it seems very fiddly.  How would I do this in a robust manner?  Do I need something like zmq?</p>
<p>I'm not clear how this is zero copy though.  When I write the record batches, isn't that serialisation?  What am I missing?</p>
<p>In my real use case, I also want to talk to Julia, but maybe that should be a separate question when I come to it.</p>
<p>PS: I have gone through the <a href=""https://arrow.apache.org/docs/python/ipc.html"" rel=""noreferrer"">docs</a>, it didn't clarify this part for me.</p>
"
"<p>I want to use <code>openai.embeddings_utils import get_embeddings</code>
So already install openai</p>
<pre><code>Name: openai
Version: 0.26.5
Summary: Python client library for the OpenAI API
Home-page: https://github.com/openai/openai-python
Author: OpenAI
Author-email: support@openai.com
License: 
Location: /Users/lima/Desktop/Paprika/Openai/.venv/lib/python3.9/site-packages
Requires: aiohttp, requests, tqdm
Required-by: 
</code></pre>
<p>This is my openai
But why not use openai.embeddings_utils??</p>
"
"<p>I am using Poetry for the first time.
I have a very simple project. Basically</p>
<pre><code>a_project
|
|--test
|    |---test_something.py
|
|-script_to_test.py
</code></pre>
<p>From a project I do <code>poetry init</code> and then <code>poetry install</code></p>
<p>I get the following</p>
<pre><code> poetry install
Updating dependencies
Resolving dependencies... (0.5s)

Writing lock file

Package operations: 7 installs, 0 updates, 0 removals

  • Installing attrs (22.2.0)
  • Installing exceptiongroup (1.1.0)
  • Installing iniconfig (2.0.0)
  • Installing packaging (23.0)
  • Installing pluggy (1.0.0)
  • Installing tomli (2.0.1)
  • Installing pytest (7.2.1)

/home/me/MyStudy/2023/pyenv_practice/dos/a_project/a_project does not contain any element
</code></pre>
<p>after this I can run <code>poetry run pytest</code> without problem but what does that error message mean?</p>
"
"<p>I am using a XGBClassifier and try to do a grid search in order to tune some parameters, and I get this warning : WARNING: ../src/learner.cc:1517: Empty dataset at worker: 0 whenever I launch the command xgb.cv(). It does not work either when I try this on default parameters.</p>
<p>Can anybody help? I am kind of lost there!</p>
<p>Here is an example code that produces the warning when I launch it:</p>
<pre><code>import pandas as pd
import xgboost as xgb
from xgboost import cv
import numpy as np

seed = 10
nfold = 10

X = pd.DataFrame(np.random.randint(0,100,size=(6, 8)), columns=list(&quot;ABCDEFGH&quot;))
y = pd.Series(np.array(\[0,0,0,1,1,1\]), index = \[0,1,2,3,4,5\])
X_test = pd.DataFrame(np.random.randint(0,100,size=(2, 8)), columns=list(&quot;ABCDEFGH&quot;))
X_test.index= \[6,7\]
y_test = pd.Series(np.array(\[0,1\]), index = \[6,7\])

    dtrain = xgb.DMatrix(X, label=y)
    dtest = xgb.DMatrix(X_test, label = y_test)
    
    # Here, this part is just to confirm we get a better model: this model will probably overfit.
    params = {'objective':'multi:softmax'}
    params['eval_metric'] = &quot;mlogloss&quot;
    params['num_class'] = np.unique(y).size # Count how many levels in the response variable.
    num_boost_round = 999
    
    default_model = xgb.train(
        params,
        dtrain,
        num_boost_round=num_boost_round,
        evals=[(dtest, &quot;Test&quot;)],
        early_stopping_rounds=10)
    
    cv_results = xgb.cv(
        params,
        dtrain,
        num_boost_round=num_boost_round,
        seed=seed,
        nfold=nfold,
        metrics={'mlogloss'},
        early_stopping_rounds=10)
</code></pre>
"
"<p>Python supports defining functions that take a variable-length sequence of arguments with <code>*args</code>. The type annotation in those cases is expected to describe the type of the individual elements in the sequence. But how can one describe the return type of the function if its length is supposed to depend on the length of <code>args</code> and the type of the elements is arbitrary?</p>
<p>Let's consider how we could annotate the output type in the following dubiously useful function:</p>
<pre class=""lang-py prettyprint-override""><code>def add_one_and_to_str(*args):
    return tuple(str(i+1) for i in args)
</code></pre>
<h4>Attempt 1: <code>Sequence</code></h4>
<p>One option might be to use <code>Sequence</code>. For example, one can do</p>
<pre class=""lang-py prettyprint-override""><code>from typing import Sequence

def add_one_and_to_str(*args: int) -&gt; Sequence[str]:
    return tuple(str(i+1) for i in args)
</code></pre>
<p>This kind of works, but it is loosing information: we know that return type will not be just any sequence of strings but specifically a tuple, and precisely of the same length than the number of arguments provided.</p>
<h4>Attempt 2: <code>TypeVarTuple</code></h4>
<p>As an alternative, Python 3.11 introduces <a href=""https://docs.python.org/3/library/typing.html#typing.TypeVarTuple"" rel=""noreferrer"">TypeVarTuple</a>, which conceptually, can be thought of as a tuple of type variables (T1, T2, ...).</p>
<p>So you can use it like</p>
<pre class=""lang-py prettyprint-override""><code>from typing import Sequence, TypeVarTuple

Ts = TypeVarTuple('Ts')

def make_tuple(*args: *Ts) -&gt; Tuple[*Ts]:
    return args
</code></pre>
<p>This correctly propagates the information about the length of the tuple. However, you cannot restrict the input types because <a href=""https://peps.python.org/pep-0646/#variance-type-constraints-and-type-bounds-not-yet-supported"" rel=""noreferrer"">type bounds are still not supported</a> for <code>TypeVarTuple</code> (so you would accept <code>Any</code>). But more importantly this seems to be useful in this case only to use the exact same types in the outputs than in the input.</p>
<p>Thus, it doesn't seem to be useful in the case discussed her.</p>
<h4>Open question</h4>
<p>How could the first function be correctly annotated specifying that the output will be a tuple of strings with the same number of elements than integers were provided as function arguments? Any idea?</p>
<p>Thanks!</p>
"
"<p>I am learning about context managers and was trying to build one myself. The following is a dummy context manager that opens a file in read mode (I know I can just do <code>with open(...): ...</code>. this is just an example I built to help me understand how to make my own context managers):</p>
<pre class=""lang-py prettyprint-override""><code>@contextmanager
def open_read(path: str):
    f = open(path, 'r')
    print('open')
    yield f
    f.close()
    print('closed')


def foo():
    try:
        with open_read('main.py') as f:
            print(f.readline())
            raise Exception('oopsie')
    except Exception:
        pass
    print(f.readline())


foo()

</code></pre>
<p>I expect this code to print:</p>
<pre><code>open
&lt;line 1 of a.txt&gt;
closed
ValueError: I/O operation on closed file.
</code></pre>
<p>But instead it prints:</p>
<pre><code>open
&lt;line 1 of a.txt&gt;
&lt;line 2 of a.txt&gt;
</code></pre>
<p>It didn't close the file!</p>
<p>This seems to contradict python's docs which state that <code>__exit__</code> will be called whether the <code>with</code> statement exited successfully or with an exception:</p>
<blockquote>
<p>object.<strong>exit</strong>(self, exc_type, exc_value, traceback)</p>
<p>Exit the runtime context related to this object. The parameters describe the exception that caused the context to be exited. If the context was exited without an exception, all three arguments will be None.</p>
</blockquote>
<p>Interestingly, when I reimplemented the context manager as shown below, it worked as expected:</p>
<pre class=""lang-py prettyprint-override""><code>class open_read(ContextDecorator):
    def __init__(self, path: str):
        self.path = path
        self.f = None

    def __enter__(self):
        self.f = open(self.path, 'r')
        print('open')
        return self.f

    def __exit__(self, exc_type, exc_val, exc_tb):
        self.f.close()
        print('closed')
</code></pre>
<p>Why didn't my original implementation work?</p>
"
"<p>I am currently running python 3.9.13 on my mac. I wanted to update my version to 3.10.10</p>
<p>I tried running</p>
<pre><code>brew install python
</code></pre>
<p>However it says that &quot;python 3.10.10 is already installed&quot;!</p>
<p>When i run</p>
<pre><code>python3 --version
</code></pre>
<p>in the terminal it says that i am still on &quot;python 3.9.13&quot;</p>
<p>So my question is, how do i change the python version from 3.9.13 to 3.10.10? I already deleted python 3.9 from my applications and python 3.10 is the only one that is still there.</p>
<p>I also tried to install python 3.10.10 from the website and installing it. However it does not work. Python 3.10.10 is being installed successfully but the version is still the same when i check it.</p>
"
"<p>When I try
<code>pip install torch</code>, I get</p>
<p>ERROR: Could not find a version that satisfies the requirement torch (from versions: none)</p>
<p>ERROR: No matching distribution found for torch</p>
<p>Searching on here stackoverflow I find that the issue is I need an older verson of python, currently I'm using 3.11. That post said 3.8 but was written some time ago, so how do I find the latest version of python that will run pytorch? I couldn't find it easily on the PyTorch pages.</p>
"
"<p>I have a setup.py like this:</p>
<pre><code>#!/usr/bin/env python

from setuptools import setup, find_packages

setup(
    name=&quot;myproject&quot;,
    package_dir={&quot;&quot;: &quot;src&quot;},
    packages=find_packages(&quot;src&quot;),
    entry_points={
        &quot;console_scripts&quot;: [
            &quot;my-script = myproject.myscript:entrypoint&quot;,
        ],
    },
)
</code></pre>
<p>How can I write that <code>entry_points</code> configuration in pyproject.toml using setuptools?</p>
<p>I'm guessing something like this, going on <a href=""https://setuptools.pypa.io/en/latest/userguide/pyproject_config.html#dynamic-metadata"" rel=""noreferrer"">setuptools' pyproject.toml docs</a>, which says I need to use &quot;INI format&quot; following <a href=""https://packaging.python.org/en/latest/specifications/entry-points/"" rel=""noreferrer"">the docs that references for entry-points</a> but it doesn't seem to give an example, and my guess at how to combine the setuptools syntax with the pyproject.toml syntax is wrong (I get a traceback from <code>pip install -e .</code> that reports <code>pip._vendor.tomli.TOMLDecodeError: Invalid value</code>, pointing at the <code>entry-points</code> line in pyproject.toml):</p>
<pre><code>[build-system]
requires = [&quot;setuptools&quot;, &quot;setuptools-scm&quot;]
build-backend = &quot;setuptools.build_meta&quot;

[metadata]
name = &quot;myproject&quot;

[tool.setuptools]
package-dir = {&quot;&quot; = &quot;src&quot;}

[tool.setuptools.packages.find]
where = [&quot;src&quot;]

[tool.setuptools.dynamic]
entry-points =
    my-script = myproject.myscript:entrypoint
</code></pre>
<p>Note I have a stub setup.py alongside that pyproject.toml, like this (which I read I need to support <code>pip install -e .</code> i.e. &quot;editable installation&quot;):</p>
<pre><code>from setuptools import setup

if __name__ == &quot;__main__&quot;:
    setup()
</code></pre>
"
"<p>Is there a way to be able to pause/kill the optuna study, then resume it either by running the incomplete trials from the beginning, or resuming the incomplete trials from the latest checkpoint?</p>
<pre><code>study = optuna.create_study()
study.optimize(objective)
</code></pre>
"
"<p>Hi I'm struggling to get Tensorflow V2.11 to find my eGPU (RTX 3060 Ti)</p>
<p>I am currently on Windows 11
CUDA version is 12
I am currently downloading CUDA 11 as well as CUDnn as I've heard it is recommended</p>
<p>I have tried the following code:</p>
<pre><code>import tensorflow as tf
tf.config.list_physical_devices('GPU')
</code></pre>
<p>which outputs:</p>
<blockquote>
<p>[]</p>
</blockquote>
<p>any help would be great</p>
"
"<p>I am working on google colab with the segmentation_models library. It worked perfectly the first week using it, but now it seems that I can't import the library anymore. Here is the error message, when I execute import segmentation_models as sm :</p>
<pre><code>---------------------------------------------------------------------------

AttributeError                            Traceback (most recent call last)

&lt;ipython-input-3-6f48ce46383f&gt; in &lt;module&gt;
      1 import tensorflow as tf
----&gt; 2 import segmentation_models as sm

            3 frames

/usr/local/lib/python3.8/dist-packages/efficientnet/__init__.py in init_keras_custom_objects()
     69     }
     70 
---&gt; 71     keras.utils.generic_utils.get_custom_objects().update(custom_objects)
     72 
     73 

AttributeError: module 'keras.utils.generic_utils' has no attribute 'get_custom_objects'
</code></pre>
<p>Colab uses tensorflow version 2.11.0.</p>
<p>I did not find any information about this particular error message. Does anyone know where the problem may come from ?</p>
"
"<p>Is there any built-in function in <code>polars</code> or a better way to convert time durations to numeric by defining the time resolution (e.g.: days, hours, minutes)?</p>
<pre class=""lang-py prettyprint-override""><code>import polars as pl

df = pl.DataFrame({
    &quot;from&quot;: [&quot;2023-01-01&quot;, &quot;2023-01-02&quot;, &quot;2023-01-03&quot;],
    &quot;to&quot;: [&quot;2023-01-04&quot;, &quot;2023-01-05&quot;, &quot;2023-01-06&quot;],
})
</code></pre>
<p>My current approach:</p>
<pre><code># Convert to date and calculate the time difference
df = (
    df.with_columns(
        pl.col(&quot;to&quot;, &quot;from&quot;).str.to_date().name.suffix(&quot;_date&quot;)
    )
    .with_columns((pl.col(&quot;to_date&quot;) - pl.col(&quot;from_date&quot;)).alias(&quot;time_diff&quot;))
)

# Convert the time difference to int (in days)
df = df.with_columns(
    ((pl.col(&quot;time_diff&quot;) / (24 * 60 * 60 * 1000)).cast(pl.Int8)).alias(&quot;time_diff_int&quot;)
)
</code></pre>
<p>Output:</p>
<pre><code>shape: (3, 6)
┌────────────┬────────────┬────────────┬────────────┬──────────────┬───────────────┐
│ from       ┆ to         ┆ to_date    ┆ from_date  ┆ time_diff    ┆ time_diff_int │
│ ---        ┆ ---        ┆ ---        ┆ ---        ┆ ---          ┆ ---           │
│ str        ┆ str        ┆ date       ┆ date       ┆ duration[ms] ┆ i8            │
╞════════════╪════════════╪════════════╪════════════╪══════════════╪═══════════════╡
│ 2023-01-01 ┆ 2023-01-04 ┆ 2023-01-04 ┆ 2023-01-01 ┆ 3d           ┆ 3             │
│ 2023-01-02 ┆ 2023-01-05 ┆ 2023-01-05 ┆ 2023-01-02 ┆ 3d           ┆ 3             │
│ 2023-01-03 ┆ 2023-01-06 ┆ 2023-01-06 ┆ 2023-01-03 ┆ 3d           ┆ 3             │
└────────────┴────────────┴────────────┴────────────┴──────────────┴───────────────┘
</code></pre>
"
"<p>This bug suddenly came up literally today after read_excel previously was working fine.  Fails no matter which version of python3 I use - either 10 or 11.</p>
<p>Do folks know the fix?</p>
<pre><code>  File &quot;/Users/aizenman/My Drive/code/daily_new_clients/code/run_daily_housekeeping.py&quot;, line 38, in &lt;module&gt;
    main()
  File &quot;/Users/aizenman/My Drive/code/daily_new_clients/code/run_daily_housekeeping.py&quot;, line 25, in main
    sb = diana.superbills.load_superbills_births(args.site, ath)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/Users/aizenman/My Drive/code/daily_new_clients/code/diana/superbills.py&quot;, line 148, in load_superbills_births
    sb = pd.read_excel(SUPERBILLS_EXCEL, sheet_name=&quot;Births&quot;, parse_dates=[&quot;DOS&quot;, &quot;DOB&quot;])
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/util/_decorators.py&quot;, line 211, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File &quot;/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/util/_decorators.py&quot;, line 331, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File &quot;/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/io/excel/_base.py&quot;, line 482, in read_excel
    io = ExcelFile(io, storage_options=storage_options, engine=engine)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/io/excel/_base.py&quot;, line 1695, in __init__
    self._reader = self._engines[engine](self._io, storage_options=storage_options)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/io/excel/_openpyxl.py&quot;, line 557, in __init__
    super().__init__(filepath_or_buffer, storage_options=storage_options)
  File &quot;/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/io/excel/_base.py&quot;, line 545, in __init__
    self.book = self.load_workbook(self.handles.handle)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/io/excel/_openpyxl.py&quot;, line 568, in load_workbook
    return load_workbook(
           ^^^^^^^^^^^^^^
  File &quot;/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openpyxl/reader/excel.py&quot;, line 346, in load_workbook
    reader.read()
  File &quot;/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openpyxl/reader/excel.py&quot;, line 303, in read
    self.parser.assign_names()
  File &quot;/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openpyxl/reader/workbook.py&quot;, line 109, in assign_names
    sheet.defined_names[name] = defn
    ^^^^^^^^^^^^^^^^^^^
AttributeError: 'ReadOnlyWorksheet' object has no attribute 'defined_names'
</code></pre>
"
"<p>I want to select columns in a Polars DataFrame based on a condition. In my case, I want to select all string columns that have less than 100 unique values. Naively I tried:</p>
<pre><code>df.select((pl.col(pl.String)) &amp; (pl.all().n_unique() &lt; 100))
</code></pre>
<p>which gave me an error, which is probably due to the second part of the expression.</p>
<pre><code>df.select(pl.all().n_unique() &lt; 100)
</code></pre>
<p>This doesn't select columns but instead returns a single row DataFrame of bool values. I'm new to polars and still can't quite wrap my head around the expression API, I guess. What am I doing wrong?</p>
"
"<p>It was working perfectly earlier but for some reason now I am getting strange errors.</p>
<p>pandas version: <code>1.2.3</code></p>
<p>matplotlib version: <code>3.7.0</code></p>
<p>sample dataframe:</p>
<pre><code>df
    cap       Date
0    1     2022-01-04
1    2     2022-01-06
2    3     2022-01-07
3    4     2022-01-08
</code></pre>
<pre><code>df.plot(x='cap', y='Date')
plt.show()
</code></pre>
<pre><code>df.dtypes
cap              int64
Date    datetime64[ns]
dtype: object
</code></pre>
<p>I get a traceback:</p>
<pre class=""lang-none prettyprint-override""><code>Traceback (most recent call last):
  File &quot;/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/code.py&quot;, line 90, in runcode
    exec(code, self.locals)
  File &quot;&lt;input&gt;&quot;, line 1, in &lt;module&gt;
  File &quot;/Volumes/coding/venv/lib/python3.8/site-packages/pandas/plotting/_core.py&quot;, line 955, in __call__
    return plot_backend.plot(data, kind=kind, **kwargs)
  File &quot;/Volumes/coding/venv/lib/python3.8/site-packages/pandas/plotting/_matplotlib/__init__.py&quot;, line 61, in plot
    plot_obj.generate()
  File &quot;/Volumes/coding/venv/lib/python3.8/site-packages/pandas/plotting/_matplotlib/core.py&quot;, line 279, in generate
    self._setup_subplots()
  File &quot;/Volumes/coding/venv/lib/python3.8/site-packages/pandas/plotting/_matplotlib/core.py&quot;, line 337, in _setup_subplots
    fig = self.plt.figure(figsize=self.figsize)
  File &quot;/Volumes/coding/venv/lib/python3.8/site-packages/matplotlib/_api/deprecation.py&quot;, line 454, in wrapper
    return func(*args, **kwargs)
  File &quot;/Volumes/coding/venv/lib/python3.8/site-packages/matplotlib/pyplot.py&quot;, line 813, in figure
    manager = new_figure_manager(
  File &quot;/Volumes/coding/venv/lib/python3.8/site-packages/matplotlib/pyplot.py&quot;, line 382, in new_figure_manager
    _warn_if_gui_out_of_main_thread()
  File &quot;/Volumes/coding/venv/lib/python3.8/site-packages/matplotlib/pyplot.py&quot;, line 360, in _warn_if_gui_out_of_main_thread
    if _get_required_interactive_framework(_get_backend_mod()):
  File &quot;/Volumes/coding/venv/lib/python3.8/site-packages/matplotlib/pyplot.py&quot;, line 208, in _get_backend_mod
    switch_backend(rcParams._get(&quot;backend&quot;))
  File &quot;/Volumes/coding/venv/lib/python3.8/site-packages/matplotlib/pyplot.py&quot;, line 331, in switch_backend
    manager_pyplot_show = vars(manager_class).get(&quot;pyplot_show&quot;)
TypeError: vars() argument must have __dict__ attribute
</code></pre>
"
"<p>I have access to six 24GB GPUs.
When I try to load some HuggingFace models, for example the following</p>
<pre><code>from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
tokenizer = AutoTokenizer.from_pretrained(&quot;google/ul2&quot;)
model = AutoModelForSeq2SeqLM.from_pretrained(&quot;google/ul2&quot;)
</code></pre>
<p>I get an out of memory error, as the model only seems to be able to load on a single GPU. However, while the whole model cannot fit into a single 24GB GPU card, I have 6 of these and would like to know if there is a way to distribute the model loading across multiple cards, to perform inference.</p>
<p>HuggingFace seems to have <a href=""https://huggingface.co/docs/transformers/perf_infer_gpu_many"" rel=""noreferrer"">a webpage</a> where they explain how to do this but it has no useful content as of today.</p>
"
"<p>I have two scripts:</p>
<pre><code>from fastapi import FastAPI
import asyncio

app = FastAPI()

@app.get(&quot;/&quot;)
async def root():
    a = await asyncio.sleep(10)
    return {'Hello': 'World',}
</code></pre>
<p>And second one:</p>
<pre><code>from fastapi import FastAPI
import time
  
app = FastAPI()

@app.get(&quot;/&quot;)
def root():
    a = time.sleep(10)
    return {'Hello': 'World',}
</code></pre>
<p>Please note the second script doesn't use <code>async</code>. Both scripts do the same, at first I thought, the benefit of an <code>async</code> script is that it allows multiple connections at once, but when testing the second code, I was able to run multiple connections as well. The results are the same, performance is the same and I don't understand why would we use <code>async</code> method. Would appreciate your explanation.</p>
"
"<p>I have a problem with following code:</p>
<pre><code>from pandasql import sqldf
import pandas as pd

df = pd.DataFrame({'column1': [1, 2, 3], 'column2': [4, 5, 6]})

query = &quot;SELECT * FROM df WHERE column1 &gt; 1&quot;

new_dataframe = sqldf(query)

print(new_dataframe)
</code></pre>
<p>When I submit, I have this error:</p>
<pre><code>Traceback (most recent call last):

  File ~\AppData\Local\Programs\Spyder\Python\lib\site-packages\sqlalchemy\engine\base.py:1410 in execute
    meth = statement._execute_on_connection

AttributeError: 'str' object has no attribute '_execute_on_connection'


The above exception was the direct cause of the following exception:

Traceback (most recent call last):

  File ~\AppData\Local\Programs\Spyder\pkgs\spyder_kernels\py3compat.py:356 in compat_exec
    exec(code, globals, locals)

  File c:\users\yv663dz\downloads\untitled1.py:18
    new_dataframe = sqldf(query)

  File ~\AppData\Local\Programs\Spyder\Python\lib\site-packages\pandasql\sqldf.py:156 in sqldf
    return PandaSQL(db_uri)(query, env)

  File ~\AppData\Local\Programs\Spyder\Python\lib\site-packages\pandasql\sqldf.py:61 in __call__
    result = read_sql(query, conn)

  File ~\AppData\Local\Programs\Spyder\Python\lib\site-packages\pandas\io\sql.py:592 in read_sql
    return pandas_sql.read_query(

  File ~\AppData\Local\Programs\Spyder\Python\lib\site-packages\pandas\io\sql.py:1557 in read_query
    result = self.execute(*args)

  File ~\AppData\Local\Programs\Spyder\Python\lib\site-packages\pandas\io\sql.py:1402 in execute
    return self.connectable.execution_options().execute(*args, **kwargs)

  File ~\AppData\Local\Programs\Spyder\Python\lib\site-packages\sqlalchemy\engine\base.py:1412 in execute
    raise exc.ObjectNotExecutableError(statement) from err

ObjectNotExecutableError: Not an executable object: 'SELECT * FROM df WHERE column1 &gt; 1'
</code></pre>
<p>I installed the latest versions of pandas, pandasql and sqlalchemy and I use Spyder as IDE. Could someone help me please?</p>
"
"<p>I want to use the database by creating it as a dataframe, and I've used sqlalchemy for importing create_engine, but I'm stuck with the not an executable object: 'SELECT * FROM LoanParcel', where LoanParcel is the name of the database I want to create as a dataframe, how should I fix it?</p>
<p>views.py</p>
<pre class=""lang-bash prettyprint-override""><code>from sqlalchemy import create_engine

engine = create_engine(&quot;mysql+pymysql://mariadb:mariadb@localhost:9051/mariadb&quot;)

def user_detail(req, id):
    conn = engine.connect()
    QLoanParcel = &quot;SELECT * FROM LoanParcel&quot;
    dfParcel = pd.read_sql(QLoanParcel, conn)
    conn.close()
    df = dfParcel.drop([&quot;id&quot;, &quot;date_add&quot;, &quot;start_date&quot;], axis = 1)
    return render(req,'pages/user_detail.html')
</code></pre>
"
"<p>Whenever I try to read Excel using</p>
<pre><code>part=pd.read_excel(path,sheet_name = mto_sheet)
</code></pre>
<p>I get this exception:</p>
<blockquote>
<p>&lt;class 'Exception'&gt; 'ReadOnlyWorksheet' object has no attribute 'defined_names'</p>
</blockquote>
<p>This is if I use Visual Studio Code and Python 3.11. However, I don't have this problem when using Anaconda. Any reason for that?</p>
"
"<p>I am encountering the following error when attempting to install matplotlib in an alpine Docker image:</p>
<pre class=""lang-none prettyprint-override""><code> error: Failed to download any of the following: ['http://www.qhull.org/download/qhull-2020-src-8.0.2.tgz'].  Please download one of these urls and extract it into 'build/' at the top-level of the source repository.
      [end of output]

  note: This error originates from a subprocess, and is likely not a problem with pip.
  ERROR: Failed building wheel for matplotlib
Failed to build matplotlib
ERROR: Could not build wheels for matplotlib, which is required to install pyproject.toml-based projects
</code></pre>
<p>My Python version is 3.9.12. How can I resolve this error?</p>
"
"<p>TA-Lib windows distribution from here <a href=""https://www.lfd.uci.edu/%7Egohlke/pythonlibs/#ta-lib"" rel=""nofollow noreferrer"">https://www.lfd.uci.edu/~gohlke/pythonlibs/#ta-lib</a>, but only support to 3.10.
How to install Ta-lib with python 3.11 in Windows?</p>
<p><a href=""https://github.com/ta-lib/ta-lib-python#windows"" rel=""nofollow noreferrer"">Instructions from the README for installing on Windows</a>:</p>
<blockquote>
<p>Download <a href=""http://prdownloads.sourceforge.net/ta-lib/ta-lib-0.4.0-msvc.zip"" rel=""nofollow noreferrer"">ta-lib-0.4.0-msvc.zip</a> and unzip to <code>C:\ta-lib</code>.</p>
<p>This is a 32-bit binary release. If you want to use 64-bit Python, you
will need to build a 64-bit version of the library. Some unofficial
(and unsupported) instructions for building on 64-bit Windows 10, here
for reference:</p>
<ol>
<li>Download and Unzip <code>ta-lib-0.4.0-msvc.zip</code></li>
<li>Move the Unzipped Folder <code>ta-lib</code> to <code>C:\</code></li>
<li>Download and Install Visual Studio Community (2015 or
later)
<ul>
<li>Remember to Select <code>[Visual C++]</code> Feature</li>
</ul>
</li>
<li>Build TA-Lib Library
<ul>
<li>From Windows Start Menu, Start <code>[VS2015 x64 Native Tools Command Prompt]</code></li>
<li>Move <code>to C:\ta-lib\c\make\cdr\win32\msvc</code></li>
<li>Build the Library <code>nmake</code></li>
</ul>
</li>
</ol>
</blockquote>
<p>But then:</p>
<pre><code>ERROR: Failed building wheel for TA-Lib
Failed to build TA-Lib
ERROR: Could not build wheels for TA-Lib, which is required to install pyproject.toml-based projects
</code></pre>
"
"<p>EDIT: attaching some code to help generate similar results (appended at end)</p>
<p>I have a really small model with architecture <code>[2, 3, 6]</code> where the hidden layer uses ReLU and it's a softmax activation for multiclass classification. Trained offline and statically quantized later to qint8. What I would like to do now is extract the weights so I can use them on other hardware via matrix multiplication/addition. The problem I'm encountering is it doesn't seem to behave as expected. Take for instance this GraphModule output of state_dict():</p>
<pre class=""lang-py prettyprint-override""><code>OrderedDict([('input_layer_input_scale_0', tensor(0.0039)),
             ('input_layer_input_zero_point_0', tensor(0)),
             ('input_layer.scale', tensor(0.0297)),
             ('input_layer.zero_point', tensor(0)),
             ('input_layer._packed_params.dtype', torch.qint8),
             ('input_layer._packed_params._packed_params',
              (tensor([[-0.1180,  0.1180],
                       [-0.2949, -0.5308],
                       [-3.3029, -7.5496]], size=(3, 2), dtype=torch.qint8,
                      quantization_scheme=torch.per_tensor_affine, scale=0.05898105353116989,
                      zero_point=0),
               Parameter containing:
               tensor([-0.4747, -0.3563,  7.7603], requires_grad=True))),
             ('out.scale', tensor(1.5963)),
             ('out.zero_point', tensor(243)),
             ('out._packed_params.dtype', torch.qint8),
             ('out._packed_params._packed_params',
              (tensor([[  0.4365,   0.4365, -55.4356],
                       [  0.4365,   0.0000,   1.3095],
                       [  0.4365,   0.0000, -13.9680],
                       [  0.4365,  -0.4365,   4.3650],
                       [  0.4365,   0.4365,  -3.0555],
                       [  0.4365,   0.0000,  -1.3095],
                       [  0.4365,   0.0000,   3.0555]], size=(7, 3), dtype=torch.qint8,
                      quantization_scheme=torch.per_tensor_affine, scale=0.43650051951408386,
                      zero_point=0),
               Parameter containing:
               tensor([ 19.2761,  -1.0785,  14.2602, -22.3171,  10.1059,   7.2197, -11.7253],
                      requires_grad=True)))])
</code></pre>
<p>If I directly access the weights the way I think I should like so:</p>
<pre class=""lang-py prettyprint-override""><code>input_weights = np.array(
[[-0.1180,  0.1180],
 [-0.2949, -0.5308],
 [-3.3029, -7.5496]])
inputs_scale = 0.05898105353116989
inputs_zero_point = 0

W1=np.clip(np.round(input_weights/inputs_scale+ inputs_zero_scale), -127, 128)
b1=np.clip(np.round(np.array([-0.4747, -0.3563,  7.7603])/inputs_scale + inputs_zer_scale), -127, 128)

output_weights = np.array(
[[  0.4365,   0.4365, -55.4356],
 [  0.4365,   0.0000,   1.3095],
 [  0.4365,   0.0000, -13.9680],
 [  0.4365,  -0.4365,   4.3650],
 [  0.4365,   0.4365,  -3.0555],
 [  0.4365,   0.0000,  -1.3095],
 [  0.4365,   0.0000,   3.0555]])

outputs_scale=0.43650051951408386
outputs_zero_point=0
W1=np.clip(np.round(output_weights/outputs_scale+ outputs_zero_scale), -127, 128)
W2=np.clip(np.round(np.array([ 19.2761,  -1.0785,  14.2602, -22.3171,  10.1059,   7.2197, -11.7253])/outputs_scale + outputs_zero_scale), -127, 128)
</code></pre>
<p>And then I give it some data:</p>
<pre><code>inputs = np.array(
       [[1.  , 1.  ], # class 0 example
       [1.  , 0.  ], # class 1 example
       [0.  , 1.  ],
       [0.  , 0.  ],
       [0.  , 0.9 ],
       [0.  , 0.75],
       [0.  , 0.25]]) # class 6 example
</code></pre>
<p>Where each row is an example, then I would expect to be able to do matrix multiplication and argmax over the rows to get the result. However, doing that gives me this:</p>
<pre><code>&gt;&gt;&gt; (ReLU((inputs @ W1.T) + b1) @ W2.T + b2).argmax(axis=0)
array([0, 3, 0, 3, 0, 0, 3])
</code></pre>
<p>which is not right.
And when I test accuracy of the quantized model in pytorch it's high enough that it should get all examples correct here. So what am I misunderstanding in terms of accessing these weights/bias?</p>
<p>EDIT: adding code to help people mess around with quantization. Now technically it doesn't matter how this code is generated - an OrderedDict of the quantized model will remain similar. If you want to mess around with it, here is some code to generate a model and quantize it on the XOR problem. Note that I'm using a multiclass classification still to help stick to my original model. Anyway.... here you go...</p>
<pre class=""lang-py prettyprint-override""><code>import torch
import torch.nn as nn
import random
import copy
import numpy as np
import tensorflow as tf
import torch.nn.functional as F
from torch.ao.quantization.quantize_fx import prepare_fx, convert_fx
from torch.utils.data import DataLoader, TensorDataset
from pytorch_lightning.callbacks.progress import RichProgressBar
from pytorch_lightning.callbacks.early_stopping import EarlyStopping
import pytorch_lightning as pl

class XORModel(nn.Module):
    def __init__(self, h: int):
        super().__init__()
        self.input_layer = nn.Linear(2, h)
        self.out = nn.Linear(h, 2)
    
    def forward(self, x):
        out = self.input_layer(x)
        out = F.relu(out)
        out = self.out(out)
        return out

class LitModel(pl.LightningModule):
    def __init__(self, model: XORModel):
        super().__init__()
        self.model = model
    
    def forward(self, x):
        return self.model(x)
    
    def _generic_step(self, batch, batch_idx, calc_metric: bool = False):
        x, y = batch
        out = self.model(x)
        if calc_metric:
            with torch.no_grad():
                soft = F.softmax(out, dim=-1)
                metric = (soft.argmax(-1).ravel() == y.ravel()).float().mean()
                self.log('Accuracy', metric, prog_bar=True)
        
        loss = F.cross_entropy(out, y)
        return loss
    
    def training_step(self, batch, batch_idx):
        loss = self._generic_step(batch, batch_idx)
        self.log('train_loss', loss, prog_bar=True)
        return loss
    
    def validation_step(self, batch, batch_idx):
        loss = self._generic_step(batch, batch_idx, calc_metric=True)
        self.log('val_loss', loss, prog_bar=True)
        return loss
    
    def configure_optimizers(self):
        return torch.optim.Adam(self.model.parameters())

def get_accuracy(model: XORModel, seed: int):
    dataset = make_dataset(1000, 1000, False, seed)
    
    model.eval()
    ret = []
    with torch.no_grad():
        for X, y in dataset:
            out = F.softmax(model(X), dim=-1).argmax(-1)
            ret.append((out.cpu().numpy() == y.numpy()).mean())
    model.train()
    return np.array(ret).mean()

def make_dataset(samples: int, batch_size: int, shuffle: bool, seed: int):
    inputs, outputs = [], []
    rng = random.Random(seed)

    for _ in range(samples):
        x0 = rng.randint(0, 1)
        x1 = rng.randint(0, 1)
        y = x0 ^ x1
        inputs.append((x0, x1))
        outputs.append(y)
    
    dataset = TensorDataset(torch.tensor(inputs, dtype=torch.float), torch.tensor(outputs, dtype=torch.long))
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)
    return dataloader

def quantize_model(model: XORModel):
    model_to_quantize = copy.deepcopy(model)
    model_to_quantize.eval()
    def calibrate(m, data_loader):
        m.eval()
        with torch.no_grad():
            for x in data_loader:
                m(x)

    loader = make_dataset(1000, 1000, False, 0x42)
    sample_inputs = next(iter(loader))[0]
    qconfig_dict = {'': torch.quantization.get_default_qconfig('fbgemm')}
    prepared_model = prepare_fx(model, qconfig_dict)
    calibrate(prepared_model, sample_inputs)
    quantized_model = convert_fx(prepared_model)

    return quantized_model

if __name__ == '__main__':
    train_dataset = make_dataset(10_000, 256, True, 123456)
    val_dataset = make_dataset(500, 64, True, 0xabcd)
    test_dataset = make_dataset(1000, 1000, False, 0x1122)

    model = XORModel(3)
    lit_model = LitModel(model)
    trainer = pl.Trainer(accelerator='cpu', max_epochs=100,
                         callbacks=[
                            RichProgressBar(refresh_rate=50),
                            EarlyStopping(monitor='val_loss', mode='min', patience=3)
                         ])
    
    trainer.fit(lit_model, train_dataset, val_dataset)
    qmodel = quantize_model(lit_model.model)
    print('accuracy of model', get_accuracy(model, 0xbeef))  # prints 1
    print('accuray of qmodel', get_accuracy(qmodel, 0xbeef)) # prints 1
    
</code></pre>
<p>Now assuming you save off the qmodel for later, you can look at the parameters similar to how I do by calling <code>qmodel.state_dict()</code></p>
"
"<p>I am using the latest version of pip, <code>23.01</code>. I have a <code>pyproject.toml</code> file with dependencies and optional dependency groups (aka &quot;extras&quot;). To avoid redundancies and make managing optional dependency groups easier, I would like to know how to have optional dependency groups require other optional dependency groups.</p>
<p>I have a <code>pyproject.toml</code> where the optional dependency groups have redundant overlaps in dependencies. I guess they could described as &quot;hierarchical&quot;. It looks like this:</p>
<pre class=""lang-ini prettyprint-override""><code>[project]
name = 'my-package'
dependencies = [
    'pandas',
    'numpy&gt;=1.22.0',
    # ...
]

[project.optional-dependencies]
# development dependency groups
test = [
    'my-package[chem]',
    'pytest&gt;=4.6',
    'pytest-cov',
    # ...
    # Redundant overlap with chem and torch dependencies
    'rdkit',
    # ...
    'torch&gt;=1.9',
    # ...
]

# feature dependency groups
chem = [
    'rdkit',
    # ...
    # Redundant overlap with torch dependencies
    'torch&gt;=1.9',
    # ...

]
torch = [
    'torch&gt;=1.9',
    # ...
]
</code></pre>
<p>In the above example, <code>pip install .[test]</code> will include all of <code>chem</code> and <code>torch</code> groups' packages, and <code>pip install .[chem]</code> will include <code>torch</code> group's packages.</p>
<p>Removing overlaps and references from one group to another, a user can still get packages required for <code>chem</code> by doing <code>pip install .[chem,torch]</code>, but I work with data scientists who may not realize immediately that the <code>torch</code> group is a requirement for the <code>chem</code> group, etc.</p>
<p>Therefore, I want a file that's something like this:</p>
<pre class=""lang-ini prettyprint-override""><code>[project]
name = 'my-package'
dependencies = [
    'pandas',
    'numpy&gt;=1.22.0',
    # ...
]

[project.optional-dependencies]
# development dependency groups
test = [
    'my-package[chem]',
    'pytest&gt;=4.6',
    'pytest-cov',
    # ...
]

# feature dependency groups
chem = [
    'my-package[torch]',
    'rdkit',
    # ...
]
torch = [
    'torch&gt;=1.9',
    # ...
]
</code></pre>
<p>This approach can't work because <code>my-package</code> is hosted in our private pip repository, so having<code>'my-package[chem]'</code> like the above example fetches the previously built version's <code>chem</code> group packages.</p>
<p>It appears that using Poetry and its <code>pyproject.toml</code> format/features can make this possible, but I would prefer not to switch our build system around too much. Is this possible with pip?</p>
"
"<p>I want to be able to create a python class like the following programmatically:</p>
<pre class=""lang-py prettyprint-override""><code>class Foo(BaseModel):
    bar: str = &quot;baz&quot;
</code></pre>
<p>The following almost works:</p>
<pre class=""lang-py prettyprint-override""><code>Foo = type(&quot;Foo&quot;, (BaseModel,), {&quot;bar&quot;:&quot;baz&quot;})
</code></pre>
<p>But doesn't include the annotation, <code>Foo.__annotations__</code> is set in first example but not the second.</p>
<p>Is there any way to achieve this?</p>
<p>My motivation is to create a class decorator that creates a clone of the decorated class with modified type annotations. The annotations have to be set during class creation (not after the fact) to that the metaclass of BaseModel will see them.</p>
"
"<p>I have a very powerful bot in discord (discord.py, PYTHON) and it can play music in voice channels. It gets the music from youtube (youtube_dl). It <strong>worked perfectly before</strong> but now it doesn't want to work with any video.
I tried updating youtube_dl but it still doesn't work
I searched everywhere but I still can't find a answer that might help me.</p>
<p>This is the Error: <code>Error: Unable to extract uploader id</code></p>
<p>After and before the error log there is no more information.
Can anyone help?</p>
<p>I will leave some of the code that I use for my bot...
The youtube setup settings:</p>
<pre><code>youtube_dl.utils.bug_reports_message = lambda: ''


ytdl_format_options = {
    'format': 'bestaudio/best',
    'outtmpl': '%(extractor)s-%(id)s-%(title)s.%(ext)s',
    'restrictfilenames': True,
    'noplaylist': True,
    'nocheckcertificate': True,
    'ignoreerrors': False,
    'logtostderr': False,
    'quiet': True,
    'no_warnings': True,
    'default_search': 'auto',
    'source_address': '0.0.0.0',  # bind to ipv4 since ipv6 addresses cause issues sometimes
}

ffmpeg_options = {
    'options': '-vn',
}

ytdl = youtube_dl.YoutubeDL(ytdl_format_options)


class YTDLSource(discord.PCMVolumeTransformer):
    def __init__(self, source, *, data, volume=0.5):
        super().__init__(source, volume)

        self.data = data

        self.title = data.get('title')
        self.url = data.get('url')
        self.duration = data.get('duration')
        self.image = data.get(&quot;thumbnails&quot;)[0][&quot;url&quot;]
    @classmethod
    async def from_url(cls, url, *, loop=None, stream=False):
        loop = loop or asyncio.get_event_loop()
        data = await loop.run_in_executor(None, lambda: ytdl.extract_info(url, download=not stream))
        #print(data)

        if 'entries' in data:
            # take first item from a playlist
            data = data['entries'][0]
        #print(data[&quot;thumbnails&quot;][0][&quot;url&quot;])
        #print(data[&quot;duration&quot;])
        filename = data['url'] if stream else ytdl.prepare_filename(data)
        return cls(discord.FFmpegPCMAudio(filename, **ffmpeg_options), data=data)

</code></pre>
<p>Approximately the command to run the audio (from my bot):</p>
<pre><code>sessionChanel = message.author.voice.channel
await sessionChannel.connect()
url = matched.group(1)
player = await YTDLSource.from_url(url, loop=client.loop, stream=True)
sessionChannel.guild.voice_client.play(player, after=lambda e: print(
                                       f'Player error: {e}') if e else None)
</code></pre>
"
"<p>Similar to <a href=""https://stackoverflow.com/questions/57838797/why-use-true-is-slower-than-use-1-in-python3"">why use True is slower than use 1 in Python3</a> but I'm using pypy3 and not using the sum function.</p>
<pre><code>def sieve_num(n):
    nums = [0] * n
    for i in range(2, n):
        if i * i &gt;= n: break
        if nums[i] == 0:
            for j in range(i*i, n, i):
                nums[j] = 1

    return [i for i in range(2, n) if nums[i] == 0]


def sieve_bool(n):
    nums = [False] * n
    for i in range(2, n):
        if i * i &gt;= n: break
        if nums[i] == False:
            for j in range(i*i, n, i):
                nums[j] = True

    return [i for i in range(2, n) if nums[i] == False]
</code></pre>
<p><code>sieve_num(10**8)</code> takes 2.55 s, but <code>sieve_bool(10**8)</code> takes 4.45 s, which is a noticeable difference.</p>
<p>My suspicion was that <code>[0]*n</code> is somehow smaller than <code>[False]*n</code> and fits into cache better, but <code>sys.getsizeof</code> and vmprof line profiling are unsupported for PyPy. The only info I could get is that <code>&lt;listcomp&gt;</code> for <code>sieve_num</code> took 116 ms (19% of total execution time) while <code>&lt;listcomp&gt;</code> for <code>sieve_bool</code> tool 450 ms (40% of total execution time).</p>
<p>Using PyPy 7.3.1 implementing Python 3.6.9 on Intel i7-7700HQ with 24 GB RAM on Ubuntu 20.04. With Python 3.8.10 <code>sieve_bool</code> is only slightly slower.</p>
"
"<p>Installing wxpython with pip gives the error <code>ModuleNotFoundError: No module named 'attrdict'</code></p>
<h2>Details:</h2>
<p>py -3.10-64 -m pip install -U wxpython</p>
<pre><code>Collecting wxpython
  Using cached wxPython-4.2.0.tar.gz (71.0 MB)
  Preparing metadata (setup.py) ... error
  error: subprocess-exited-with-error

  × python setup.py egg_info did not run successfully.
  │ exit code: 1
  ╰─&gt; [8 lines of output]
      Traceback (most recent call last):
        File &quot;&lt;string&gt;&quot;, line 2, in &lt;module&gt;
        File &quot;&lt;pip-setuptools-caller&gt;&quot;, line 34, in &lt;module&gt;
        File &quot;C:\Users\Bernard\AppData\Local\Temp\pip-install-dokcizpt\wxpython_662eefb4314c47eba7b194b4d07a8e18\setup.py&quot;, line 27, in &lt;module&gt;
          from buildtools.config import Config, msg, opj, runcmd, canGetSOName, getSOName
        File &quot;C:\Users\Bernard\AppData\Local\Temp\pip-install-dokcizpt\wxpython_662eefb4314c47eba7b194b4d07a8e18\buildtools\config.py&quot;, line 30, in &lt;module&gt;
          from attrdict import AttrDict
      ModuleNotFoundError: No module named 'attrdict'
      [end of output]

  note: This error originates from a subprocess, and is likely not a problem with pip.
error: metadata-generation-failed

× Encountered error while generating package metadata.
╰─&gt; See above for output.

note: This is an issue with the package mentioned above, not pip.
hint: See above for details.
</code></pre>
<h2>What works</h2>
<p>installing other packages works, e.g.</p>
<p>py -3.10-64 -m pip install -U mido
Requirement already satisfied: mido in c:\python311\lib\site-packages (1.2.10)</p>
<h2>Version info</h2>
<p>Windows 10 22H2
pip 23.0.1 from C:\Python311\Lib\site-packages\pip (python 3.11)</p>
<h2>Context</h2>
<p>This is used in the fluidpatcher installer, I logged a bug <a href=""https://github.com/albedozero/fluidpatcher/issues/74"" rel=""noreferrer"">here</a>.</p>
<h1>Update 1</h1>
<p>Seems to be a known issue reported here: <a href=""https://github.com/wxWidgets/Phoenix/issues/2296"" rel=""noreferrer"">https://github.com/wxWidgets/Phoenix/issues/2296</a></p>
<p>Tried workaround of manually installing</p>
<pre><code>py -3.10-64 -m pip install -U attrdict3
</code></pre>
<p>Which installs.</p>
<p>Then retried the wxpython install</p>
<pre><code>py -3.10-64 -m pip install -U wxpython
</code></pre>
<p>Which fails, this time with a different error message</p>
<pre><code>Collecting wxpython
  Using cached wxPython-4.2.0.tar.gz (71.0 MB)
  Preparing metadata (setup.py) ... done
Requirement already satisfied: pillow in c:\python311\lib\site-packages (from wxpython) (9.4.0)
Requirement already satisfied: six in c:\python311\lib\site-packages (from wxpython) (1.16.0)
Requirement already satisfied: numpy in c:\python311\lib\site-packages (from wxpython) (1.24.2)
Installing collected packages: wxpython
  DEPRECATION: wxpython is being installed using the legacy 'setup.py install' method, because it does not have a 'pyproject.toml' and the 'wheel' package is not installed. pip 23.1 will enforce this behaviour change. A possible replacement is to enable the '--use-pep517' option. Discussion can be found at https://github.com/pypa/pip/issues/8559
  Running setup.py install for wxpython ... error
  error: subprocess-exited-with-error

  × Running setup.py install for wxpython did not run successfully.
  │ exit code: 1
  ╰─&gt; [49 lines of output]
      C:\Python311\Lib\site-packages\setuptools\dist.py:771: UserWarning: Usage of dash-separated 'license-file' will not be supported in future versions. Please use the underscore name 'license_file' instead
        warnings.warn(
      C:\Python311\Lib\site-packages\setuptools\config\setupcfg.py:508: SetuptoolsDeprecationWarning: The license_file parameter is deprecated, use license_files instead.
        warnings.warn(msg, warning_class)
      C:\Python311\Lib\site-packages\setuptools\dist.py:317: DistDeprecationWarning: use_2to3 is ignored.
        warnings.warn(f&quot;{attr} is ignored.&quot;, DistDeprecationWarning)
      running install
      C:\Python311\Lib\site-packages\setuptools\command\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.
        warnings.warn(
      running build
      Will build using: &quot;C:\Python311\python.exe&quot;
      3.11.0 (main, Oct 24 2022, 18:26:48) [MSC v.1933 64 bit (AMD64)]
      Python's architecture is 64bit
      cfg.VERSION: 4.2.0

      Running command: build
      Running command: build_wx
      Command '&quot;C:\Python311\python.exe&quot; -c &quot;import os, sys, setuptools.msvc; setuptools.msvc.isfile = lambda path: path is not None and os.path.isfile(path); ei = setuptools.msvc.EnvironmentInfo('x64', vc_min_ver=14.0); env = ei.return_env(); env['vc_ver'] = ei.vc_ver; env['vs_ver'] = ei.vs_ver; env['arch'] = ei.pi.arch; env['py_ver'] = sys.version_info[:2]; print(env)&quot;' failed with exit code 1.
      Traceback (most recent call last):

        File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt;

        File &quot;C:\Python311\Lib\site-packages\setuptools\msvc.py&quot;, line 1120, in __init__

          self.si = SystemInfo(self.ri, vc_ver)

                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^

        File &quot;C:\Python311\Lib\site-packages\setuptools\msvc.py&quot;, line 596, in __init__

          vc_ver or self._find_latest_available_vs_ver())

                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

        File &quot;C:\Python311\Lib\site-packages\setuptools\msvc.py&quot;, line 610, in _find_latest_available_vs_ver

          raise distutils.errors.DistutilsPlatformError(

      distutils.errors.DistutilsPlatformError: No Microsoft Visual C++ version found
      Finished command: build_wx (0m1.80s)
      Finished command: build (0m1.80s)
      WARNING: Building this way assumes that all generated files have been
      generated already.  If that is not the case then use build.py directly
      to generate the source and perform the build stage.  You can use
      --skip-build with the bdist_* or install commands to avoid this
      message and the wxWidgets and Phoenix build steps in the future.

      &quot;C:\Python311\python.exe&quot; -u build.py build
      Command '&quot;C:\Python311\python.exe&quot; -u build.py build' failed with exit code 1.
      [end of output]

  note: This error originates from a subprocess, and is likely not a problem with pip.
error: legacy-install-failure

× Encountered error while trying to install package.
╰─&gt; wxpython
</code></pre>
<h1>Update 2</h1>
<p>Workaround: install Python 3.10.</p>
"
"<p>I am a beginner and getting familiar with pandas .
It is throwing an error , When I was trying to create a new column this way :</p>
<p>drinks['total_servings'] = drinks.loc[: ,'beer_servings':'wine_servings'].apply(calculate,axis=1)</p>
<p>Below is my code, and I get the following error for line number 9:</p>
<p>&quot;<code>Cannot set a DataFrame with multiple columns to the single column total_servings</code>&quot;</p>
<p>Any help or suggestion would be appreciated :)</p>
<pre><code>import pandas as pd
drinks = pd.read_csv('drinks.csv')

def calculate(drinks):
    return drinks['beer_servings']+drinks['spirit_servings']+drinks['wine_servings']
print(drinks)
drinks['total_servings'] = drinks.loc[:, 'beer_servings':'wine_servings'].apply(calculate,axis=1)

drinks['beer_sales'] = drinks['beer_servings'].apply(lambda x: x*2)
drinks['spirit_sales'] = drinks['spirit_servings'].apply(lambda x: x*4)
drinks['wine_sales'] = drinks['wine_servings'].apply(lambda x: x*6)
drinks
</code></pre>
"
"<p>I'm trying to scrape some info regarding different agencies from <a href=""https://clutch.co/"" rel=""noreferrer"">clutch.co</a>. When I look up the urls in my browser everything is fine, but using scrapy it gives me 403 response. From all I read on the related issues, I suppose it's coming from Cloudflare. Is there anyway I can bypass these security measures?
Here's my scrapy code:</p>
<pre><code>class ClutchSpider(scrapy.Spider):
    name = &quot;clutch&quot;
    allowed_domains = [&quot;clutch.co&quot;]
    start_urls = [&quot;http://clutch.co/&quot;]
    
    custom_settings = {
        'DOWNLOAD_DELAY': 1,
        'CONCURRENT_REQUESTS': 5,
        'RETRY_ENABLED': True,
        'RETRY_TIMES': 5,
        'ROBOTSTXT_OBEY': False,
        'FEED_URL': f'output/output{datetime.timestamp(datetime.now())}.json',
        'FEED_FORMAT': 'json',
    }


    def __init__(self, *args, **kwargs) -&gt; None:
        super().__init__(*args, **kwargs)
        self.input_urls = ['https://clutch.co/directory/mobile-application-developers']
        self.headers = {
                        'accept': '*/*', 
                        'accept-encoding': 'gzip, deflate, br', 
                        'accept-language': 'en-US,en;q=0.9,fa;q=0.8', 
                        # 'cookie': 'shortlist_prompts=true; FPID=FPID2.2.iqvavTK2dqTJ7yLsgWqoL8fYmkFoX3pzUlG6mTVjfi0%3D.1673247154; CookieConsent={stamp:%27zejzt8TIN2JRypvuDr+oPX/PjYUsuVCNii4qWhJvCxxtOxEXcb5hMg==%27%2Cnecessary:true%2Cpreferences:true%2Cstatistics:true%2Cmarketing:true%2Cmethod:%27explicit%27%2Cver:1%2Cutc:1673247163647%2Cregion:%27nl%27}; _gcl_au=1.1.1124048711.1676796982; _gid=GA1.2.316079371.1676796983; ab.storage.deviceId.c7739970-c490-4772-aa67-2b5c1403137e=%7B%22g%22%3A%22d2822ae5-4bac-73ae-cfc0-86adeaeb1add%22%2C%22c%22%3A1676797005041%2C%22l%22%3A1676797005041%7D; ln_or=eyIyMTU0NjAyIjoiZCJ9; hubspotutk=f019384cf677064ee212b1891e67181c; FPLC=o62q7Cwf0JP12iF73tjxOelgvID3ocGZrxnLxzHlB%2F9In25%2BL7oYAwvSxOTnaZWDYH7G2iMkQ03VUW%2BJgWsv7i7StDXSdFnQr6Dpj6VC%2F2Ya4ZptNbWzzRcJUv00JA%3D%3D; __hssrc=1; shortlist_prompts=true; __hstc=238368351.f019384cf677064ee212b1891e67181c.1676798584729.1676873409297.1676879456609.3; __cf_bm=Pn4xsZ2pgyFdB0bdi9t0xTpqxVzY9t5vhySYN6uRpAQ-1676881063-0-AT8uJ+ux6Tmu0WU+bsJovJ1CubUhs+C0JBulUr1i2aQLY28rn7T23PVuGWffSrCaNjeeYPzSDN42NJ46j10jKEPjPO3mS4P8uMx9dDmA7wTqz5NCdil5W5uGQJs2pMbcjbQSfNTjQLh5umYER6hhhLx8qrRFHDnTTJ1vkORfc0eSqBe0rjqaHeR4HFINZOp1UQ==; _ga=GA1.2.298895719.1676796981; _gat_gtag_UA_2814589_5=1; __hssc=238368351.3.1676879456609; _ga_D0WFGX8X3V=GS1.1.1676879405.3.1.1676881074.46.0.0', 
                        'referer': 'https://google.com', 
                        'sec-ch-ua': '&quot;Chromium&quot;;v=&quot;110&quot;, &quot;Not A(Brand&quot;;v=&quot;24&quot;, &quot;Microsoft Edge&quot;;v=&quot;110&quot;', 
                        'sec-ch-ua-mobile': '?0', 
                        'sec-ch-ua-platform': '&quot;Windows&quot;', 
                        'sec-fetch-dest': 'empty', 
                        'sec-fetch-mode': 'cors', 
                        'sec-fetch-site': 'same-origin', 
                        'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36 Edg/110.0.1587.50'
                    }

    def start_requests(self) -&gt; scrapy.Request:
        for url in self.input_urls:
            yield scrapy.Request(url=url, callback=self.parse, headers=self.headers)


    def parse(self, response) -&gt; scrapy.Request:
        agencies = response.xpath(&quot;.//div[@class='company col-md-12 prompt-target sponsor']/a/@href&quot;).extract()
        for agency in agencies:
            return response.follow(agency, callback=self.parse_agency, headers=self.headers)


</code></pre>
<p><strong>PS</strong>: I'm not willing to use tools such as selenium, as they make everything too slow. But if there are no other ways to go around this issue, how can I benefit from selenium? (though it also gave me 403)</p>
"
"<p>I am seriously stumped with this one and can't find any questions that match.</p>
<p>If I run <code>pip3 show setuptools</code> or <code>pip2 list</code>, both say that <code>setuptools 40.8.0</code> is installed, but when I try to install a module locally, from a local source code directory, I get the error that says <code>No matching distribution found for setuptools&gt;=40.8.0</code></p>
<p>Due to access and firewall restrictions, I have to install the module into my home directory and using what's already installed on the system.</p>
<p>This worked with the previous version of the module, but now it's failing.</p>
"
"<p>I am failing to get a minimal working example running with the following setup:</p>
<ul>
<li>azure function in docker container</li>
<li>python as language, specifically the <a href=""https://learn.microsoft.com/en-us/azure/azure-functions/functions-reference-python?tabs=asgi%2Capplication-level&amp;pivots=python-mode-decorators#programming-model"" rel=""noreferrer"">&quot;new Python programming model V2&quot;</a></li>
</ul>
<p>I followed the instructions from <a href=""https://learn.microsoft.com/en-us/azure/azure-functions/functions-create-function-linux-custom-image?tabs=in-process%2Cbash%2Cazure-cli&amp;pivots=programming-language-python#create-supporting-azure-resources-for-your-function"" rel=""noreferrer"">here</a> but added the V2 flag, specifically:</p>
<pre><code> # init directory
 func init --worker-runtime python --docker -m V2
 # build docker image
 docker build -t foo .
 # run functions locally
 docker run -p 80:80 foo
</code></pre>
<p>Whatever I tried, the runtime seems to not pick up the auto generated http trigger function</p>
<pre class=""lang-py prettyprint-override""><code># function_app.py (autogenerated by func init ...) 

import azure.functions as func

app = func.FunctionApp()

@app.function_name(name=&quot;HttpTrigger1&quot;)
@app.route(route=&quot;hello&quot;) # HTTP Trigger
def test_function(req: func.HttpRequest) -&gt; func.HttpResponse:
    return func.HttpResponse(&quot;HttpTrigger1 function processed a request!!!&quot;)
</code></pre>
<p>I think the relevant part of the logs is:</p>
<pre><code>info: Host.Startup[327]
      1 functions found
info: Host.Startup[315]
      0 functions loaded
info: Host.Startup[0]
      Generating 0 job function(s)
warn: Host.Startup[0]
      No job functions found. Try making your job classes and methods public. If you're using binding extensions (e.g. Azure Storage, ServiceBus, Timers, etc.) make sure you've called the registration method for the extension(s) in your startup code (e.g. builder.AddAzureStorage(), builder.AddServiceBus(), builder.AddTimers(), etc.).
info: Microsoft.Azure.WebJobs.Script.WebHost.WebScriptHostHttpRoutesManager[0]
      Initializing function HTTP routes
      No HTTP routes mapped
</code></pre>
<p>because when I use the &quot;programming model V1&quot;, then the <code>Microsoft.Azure.WebJobs.Script.WebHost.WebScriptHostHttpRoutesManager</code> actually prints some info about the mapped routes.</p>
<p>How can I fix this? Is this not supported at the moment?</p>
"
"<p>I want to use Scalene to profile my Pytest test suit</p>
<p>Typically I run the test suit by running</p>
<pre><code>pytest
</code></pre>
<p>So I tried</p>
<pre><code>scalene pytest
</code></pre>
<p>which doesn't work as I expect.</p>
<p>What is the correct way to run my test suit through scalene?</p>
"
"<p>My <code>GET</code> endpoint receives a query parameter that needs to meet the following criteria:</p>
<ol>
<li>be an <code>int</code> between 0 and 10</li>
<li>be even number</li>
</ol>
<p><code>1.</code> is straight forward using <code>Query(gt=0, lt=10)</code>. However, it is not quiet clear to me how to extend <code>Query</code> to do extra custom validation such as <code>2.</code>. The documentation ultimately leads to pydantic. But, my application runs into internal server error when the second validation <code>2.</code> fails.</p>
<p>Below is a minimal scoped example</p>
<pre class=""lang-py prettyprint-override""><code>from fastapi import FastAPI, Depends, Query
from pydantic import BaseModel, ValidationError, validator

app = FastAPI()

class CommonParams(BaseModel):
    n: int = Query(default=..., gt=0, lt=10)

    @validator('n')
    def validate(cls, v):
        if v%2 != 0:
            raise ValueError(&quot;Number is not even :( &quot;)
        return v


@app.get(&quot;/&quot;)
async def root(common: CommonParams = Depends()):
    return {&quot;n&quot;: common.n}
</code></pre>
<p>Below are requests that work as expected and ones that break:</p>
<pre><code># requsts that work as expected
localhost:8000?n=-4
localhost:8000?n=-3
localhost:8000?n=2
localhost:8000?n=8
localhost:8000?n=99

# request that break server
localhost:8000?n=1
localhost:8000?n=3
localhost:8000?n=5
</code></pre>
"
"<p>I have a DBT package named <code>dbt_helpers</code>, where i intend to override some of dbt's in built global macros. In this example i intend to override the macro <code>dbt_spark_validate_get_file_format</code>, which is present in the dbt spark adapter <a href=""https://github.com/dbt-labs/dbt-spark/blob/main/dbt/include/spark/macros/adapters.sql"" rel=""noreferrer"">here</a>.</p>
<p>I have referred the dbt docs specified <a href=""https://docs.getdbt.com/reference/dbt-jinja-functions/dispatch"" rel=""noreferrer"">here</a> to implement my use case. Here is how i have implemented the macro in my package under package's <code>macros</code> folder.</p>
<pre><code>{% macro dbt_spark_validate_get_file_format(raw_file_format) -%}
    {{ return(adapter.dispatch('dbt_spark_validate_get_file_format','dbt_helpers')(raw_file_format)) }}
{%- endmacro %}


{% macro default__dbt_spark_validate_get_file_format(raw_file_format) %}
    {% do log('overriding global macro', info=true) %}
    {#  Custom implementation here  #}
    
{% endmacro %}
</code></pre>
<p>I have used the macro namespace <code>dbt_helpers</code> same as my package name. I have specified this in my main DBT project as a package in the <code>packages.yml</code> and I am able to see the macros defined in the <code>dbt_packages</code> directory after running the command <code>dbt deps</code>. In my main dbt project's <code>dbt_project.yml</code> I have included the project level dispatch config to take the macro from my package as shown, as directed in <a href=""https://docs.getdbt.com/reference/dbt-jinja-functions/dispatch#overriding-global-macros"" rel=""noreferrer"">this</a> section of the dbt docs.</p>
<pre><code>dispatch:
  - macro_namespace: dbt
    search_order: ['dbt_helpers','dbt']
</code></pre>
<p>However when I run my dbt model the macro defined in my package is not being called, rather the inbuilt global macro is still being called. I am able to override the macro by placing it directly inside my projects macros folder, but i need to override the macro from my <code>dbt_helpers</code> package. How can i manage to do this?</p>
"
"<p>I switched from a zip-based deployment to a docker-based deployment of two lambda functions (which are used in an API Gateway). Both functions where in the same zip file and I want to have both functions in the same docker-based container (meaning I can't use the <code>cmd</code> setting in my Dockerfile (or to be precise need to overwrite it anyway). Previously, I used the handler attribute in the cloudformation template for specifying which handler function to call in which module, e.g.</p>
<pre><code>...
  ConfigLambda:
    Type: 'AWS::Serverless::Function'
    Properties:
      Handler: config.handler
      ...
...
  LogLambda:
    Type: 'AWS::Serverless::Function'
    Properties:
      Handler: logs.handler
      ...
</code></pre>
<p>but with a docker-based build one has to define an <code>ImageConfig</code>, i.e.</p>
<pre><code>...
  LogLambda:
    Type: 'AWS::Serverless::Function'
    Properties:
      PackageType: Image
      ImageUri: !Ref EcrImageUri
      FunctionName: !Sub &quot;${AWS::StackName}-Logs&quot;
      ImageConfig:
        WorkingDirectory: /var/task
        Command: ['logs.py']
        EntryPoint: ['/var/lang/bin/python3']
...
  ConfigLambda:
    Type: 'AWS::Serverless::Function'
    Properties:
      PackageType: Image
      ImageUri: !Ref EcrImageUri
      FunctionName: !Sub &quot;${AWS::StackName}-Config&quot;
      ImageConfig:
        WorkingDirectory: /var/task
        Command: ['config.py']
        EntryPoint: ['/var/lang/bin/python3']
</code></pre>
<p>I'm a bit stuck because this does not work, no matter what combination I pass to the command array. If I fire a test event in the AWS console, I get the following error</p>
<pre><code>RequestId: &lt;uuid&gt; Error: Runtime exited without providing a reason
Runtime.ExitError
</code></pre>
<p>Judging from the full output, the file is loaded and executed, but the handler function is not invoked (there is some output from a logging setup function which is called right after the module imports). The section in the AWS documentation on python3 based lambdas state that naming for handlers should be file_name.function (e.g. function_lambda.lambda_handler), but this doesn't give any clues on how do to this for command array in a ImageConfig.</p>
<p>How do I set the Command section correctly for my lambda function in my cloudformation template?</p>
"
"<p>I have a <code>logs.txt</code> file at certain location, in a <a href=""https://cloud.google.com/compute/docs/instances#:%7E:text=An%20instance%20is%20a%20virtual,or%20the%20Compute%20Engine%20API."" rel=""noreferrer"">Compute Engine VM Instance</a>. I want to periodically backup (i.e. <strong>overwrite</strong>) <code>logs.txt</code> in a <a href=""https://cloud.google.com/storage/docs/json_api/v1/buckets"" rel=""noreferrer"">Google Cloud Storage bucket</a>. Since <code>logs.txt</code> is the result of some preprocessing made inside a Python script, I want to also use that script to upload / copy that file, into the Google Cloud Storage bucket (therefore, <strong>the use of <a href=""https://cloud.google.com/storage/docs/gsutil/commands/cp"" rel=""noreferrer""><code>cp</code></a> cannot be considered an option</strong>). Both the Compute Engine VM instance, and the Cloud Storage bucket, stay at the same GCP project, so &quot;they see each other&quot;. What I am attempting right now, based on <a href=""https://cloud.google.com/storage/docs/uploading-objects#uploading-an-object"" rel=""noreferrer"">this sample code</a>, looks like:</p>
<pre class=""lang-py prettyprint-override""><code>from google.cloud import storage

bucket_name = &quot;my-bucket&quot;
destination_blob_name = &quot;logs.txt&quot;
source_file_name = &quot;logs.txt&quot;  # accessible from this script

storage_client = storage.Client()
bucket = storage_client.bucket(bucket_name)
blob = bucket.blob(destination_blob_name)

generation_match_precondition = 0
blob.upload_from_filename(source_file_name, if_generation_match=generation_match_precondition)

print(f&quot;File {source_file_name} uploaded to {destination_blob_name}.&quot;)
</code></pre>
<p>If <code>gs://my-bucket/logs.txt</code> does not exist, the script works correctly, but if I try to <strong>overwrite</strong>, I get the following error:</p>
<pre><code>Traceback (most recent call last):
  File &quot;/usr/local/lib/python3.8/dist-packages/google/cloud/storage/blob.py&quot;, line 2571, in upload_from_file
    created_json = self._do_upload(
  File &quot;/usr/local/lib/python3.8/dist-packages/google/cloud/storage/blob.py&quot;, line 2372, in _do_upload
    response = self._do_multipart_upload(
  File &quot;/usr/local/lib/python3.8/dist-packages/google/cloud/storage/blob.py&quot;, line 1907, in _do_multipart_upload
    response = upload.transmit(
  File &quot;/usr/local/lib/python3.8/dist-packages/google/resumable_media/requests/upload.py&quot;, line 153, in transmit
    return _request_helpers.wait_and_retry(
  File &quot;/usr/local/lib/python3.8/dist-packages/google/resumable_media/requests/_request_helpers.py&quot;, line 147, in wait_and_retry
    response = func()
  File &quot;/usr/local/lib/python3.8/dist-packages/google/resumable_media/requests/upload.py&quot;, line 149, in retriable_request
    self._process_response(result)
  File &quot;/usr/local/lib/python3.8/dist-packages/google/resumable_media/_upload.py&quot;, line 114, in _process_response
    _helpers.require_status_code(response, (http.client.OK,), self._get_status_code)
  File &quot;/usr/local/lib/python3.8/dist-packages/google/resumable_media/_helpers.py&quot;, line 105, in require_status_code
    raise common.InvalidResponse(
google.resumable_media.common.InvalidResponse: ('Request failed with status code', 412, 'Expected one of', &lt;HTTPStatus.OK: 200&gt;)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File &quot;/home/my_folder/upload_to_gcs.py&quot;, line 76, in &lt;module&gt;
    blob.upload_from_filename(source_file_name, if_generation_match=generation_match_precondition)
  File &quot;/usr/local/lib/python3.8/dist-packages/google/cloud/storage/blob.py&quot;, line 2712, in upload_from_filename
    self.upload_from_file(
  File &quot;/usr/local/lib/python3.8/dist-packages/google/cloud/storage/blob.py&quot;, line 2588, in upload_from_file
    _raise_from_invalid_response(exc)
  File &quot;/usr/local/lib/python3.8/dist-packages/google/cloud/storage/blob.py&quot;, line 4455, in _raise_from_invalid_response
    raise exceptions.from_http_status(response.status_code, message, response=response)
google.api_core.exceptions.PreconditionFailed: 412 POST https://storage.googleapis.com/upload/storage/v1/b/production-onementor-dt-data/o?uploadType=multipart&amp;ifGenerationMatch=0: {
  &quot;error&quot;: {
    &quot;code&quot;: 412,
    &quot;message&quot;: &quot;At least one of the pre-conditions you specified did not hold.&quot;,
    &quot;errors&quot;: [
      {
        &quot;message&quot;: &quot;At least one of the pre-conditions you specified did not hold.&quot;,
        &quot;domain&quot;: &quot;global&quot;,
        &quot;reason&quot;: &quot;conditionNotMet&quot;,
        &quot;locationType&quot;: &quot;header&quot;,
        &quot;location&quot;: &quot;If-Match&quot;
      }
    ]
  }
}
: ('Request failed with status code', 412, 'Expected one of', &lt;HTTPStatus.OK: 200&gt;)
</code></pre>
<p>I have checked the documentation for <a href=""https://cloud.google.com/python/docs/reference/storage/latest/google.cloud.storage.blob.Blob#google_cloud_storage_blob_Blob_upload_from_filename"" rel=""noreferrer""><code>upload_from_filename</code></a>, but it seems there is no flag to &quot;enable overwritting&quot;.</p>
<p>How to properly overwrite a file existing in a Google Cloud Storage Bucket, using Python language?</p>
"
"<p>I need to drop rows that have a nan value in any column. As for null values with <code>drop_nulls()</code></p>
<pre><code>df.drop_nulls()
</code></pre>
<p>but for nans. I have found that the method <code>drop_nans</code> exist for Series but not for DataFrames</p>
<pre><code>df['A'].drop_nans()
</code></pre>
<p>Pandas code that I'm using:</p>
<pre><code>df = pd.DataFrame(
    {
        'A': [0, 0, 0, 1,None, 1],
        'B': [1, 2, 2, 1,1, np.nan]
    }
)
df.dropna()
</code></pre>
"
"<p>I am trying to use LangChain Agents and am unable to import load_tools.
Version: <code>langchain==0.0.27</code></p>
<p>I tried these:</p>
<pre><code>from langchain.agents import initialize_agent
from langchain.llms import OpenAI
from langchain.agents import load_tools
</code></pre>
<p>shows output</p>
<pre><code>---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
&lt;ipython-input-36-8eb0012265d0&gt; in &lt;module&gt;
      1 from langchain.agents import initialize_agent
      2 from langchain.llms import OpenAI
----&gt; 3 from langchain.agents import load_tools

ImportError: cannot import name 'load_tools' from 'langchain.agents' (C:\ProgramData\Anaconda3\lib\site-packages\langchain\agents\__init__.py)
</code></pre>
"
"<p>I'm hoping to include a dropdown bar with a callback function that allows the user to display specific points within smaller areas. Initially, I want to use all point geometry data as a default. I'm then aiming to include a dropdown bar and callback function that returns smaller subsets from this main df. This is accomplished by merging the point data within a specific polygon area.</p>
<p>Using below, the default df is labelled <code>gdf_all</code>. This contains point data across a large region. The smaller polygon files are subset from <code>gdf_poly</code>. These include African and European continents. These are used within a function to only return point data if it intersects within the polygon shape.</p>
<p>I've hard-coded the outputs below. 1) uses <code>gdf_all</code> and 2) uses a subset from African contintent.</p>
<p><strong>Ideally, the dropdown bar will be used to input the desired point data to be visualised within the figures.</strong></p>
<pre><code>import geopandas as gpd
import plotly.express as px
import dash
from dash import dcc
from dash import html
import dash_bootstrap_components as dbc

# point data
gdf_all = gpd.read_file(gpd.datasets.get_path(&quot;naturalearth_cities&quot;))

# polygon data
gdf_poly = gpd.read_file(gpd.datasets.get_path(&quot;naturalearth_lowres&quot;))
gdf_poly = gdf_poly.drop('name', axis = 1)

gdf_all['LON'] = gdf_all['geometry'].x
gdf_all['LAT'] = gdf_all['geometry'].y

# subset African continent
Afr_gdf_area = gdf_poly[gdf_poly['continent'] == 'Africa'].reset_index(drop = True)

# subset European continent
Eur_gdf_area = gdf_poly[gdf_poly['continent'] == 'Europe'].reset_index(drop = True)

# function to merge point data within selected polygon area
def merge_withinboundary(gdf1, gdf2):

    # spatial join data within larger boundary
    gdf_out = gpd.sjoin(gdf1, gdf2, predicate = 'within', how = 'left').reset_index(drop = True)

    return gdf_out

gdf_Africa = merge_withinboundary(gdf_all, Afr_gdf_area)
gdf_Europe = merge_withinboundary(gdf_all, Eur_gdf_area)


external_stylesheets = [dbc.themes.SPACELAB, dbc.icons.BOOTSTRAP]

app = dash.Dash(__name__, external_stylesheets = external_stylesheets)

# function to return selected df for plotting
def update_dataset(df):

    if df == 'gdf_Africa':
        gdf = gdf_Africa

    elif df == 'gdf_Europe':
        gdf = gdf_Europe

    else:
        gdf = gdf_all

    return gdf


nav_bar =  html.Div([
     html.P(&quot;area-dropdown:&quot;),
     dcc.Dropdown(
       id='data', 
       value='data', 
       options=[{'value': 'gdf_all', 'label': 'gdf_all'},
            {'value': 'gdf_Africa', 'label': 'gdf_Africa'},
            {'value': 'gdf_Europe', 'label': 'gdf_Europe'}
            ],
       clearable=False
  ),
])

# output 1
df = gdf_all

# output 2
#df = gdf_Africa

scatter = px.scatter_mapbox(data_frame = df, 
                                   lat = 'LAT', 
                                   lon = 'LON',
                                   zoom = 2,
                                   mapbox_style = 'carto-positron', 
                                   )


count = df['name'].value_counts()

bar = px.bar(x = count.index, 
              y = count.values, 
              color = count.index, 
              )

app.layout = dbc.Container([
    dbc.Row([
        dbc.Col(html.Div(nav_bar), width=2),
        dbc.Col([
            dbc.Row([
                dbc.Col(dcc.Graph(figure = scatter))
            ]),
            dbc.Row([
                dbc.Col(dcc.Graph(figure = bar))
            ]),
        ], width=5),
        dbc.Col([
        ], width=5),
    ])
], fluid=True)


if __name__ == '__main__':
    app.run_server(debug=True, port = 8051)
</code></pre>
<p>Output 1:</p>
<p><a href=""https://i.sstatic.net/Ey0hB.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Ey0hB.png"" alt=""enter image description here"" /></a></p>
<p>Output 2:</p>
<p><a href=""https://i.sstatic.net/eQzRj.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/eQzRj.png"" alt=""enter image description here"" /></a></p>
"
"<p>I am new to Tensorflow, and am trying to train a specific deep learning neural network. I am using Tensorflow (2.11.0) to get a deep neural network model which is described below. The data which I use is also given below:</p>
<p><strong>Data:</strong></p>
<p>Here is some example data. For sake of ease we can consider 10 samples in data. Here, each sample has shape: <code>(128,128)</code>.</p>
<p>One can consider the below code as example training data.</p>
<pre><code>x_train = np.random.rand(10, 128, 128, 1)
</code></pre>
<p><strong>Normalization layer:</strong></p>
<pre><code>normalizer = tf.keras.layers.Normalization(axis=-1)
normalizer.adapt(x_train)
</code></pre>
<p><strong>Build model:</strong></p>
<pre><code>def build_and_compile_model(norm):
    model = tf.keras.Sequential([
      norm,
      layers.Conv2D(128, 128, activation='relu'),
      layers.Conv2D(3, 3, activation='relu'),
      layers.Flatten(),
      layers.Dense(units=32, activation='relu'),
      layers.Dense(units=1)
    ])

    model.compile(loss='mean_absolute_error', optimizer=tf.keras.optimizers.Adam(0.001))
    
    return model
</code></pre>
<p>When I do</p>
<pre><code>dnn_model = build_and_compile_model(normalizer)
dnn_model.summary()
</code></pre>
<p>I get the below error:</p>
<pre><code>ValueError: The channel dimension of the inputs should be defined. The input_shape received is (None, None, None, None), where axis -1 (0-based) is the channel dimension, which found to be `None`.
</code></pre>
<p><strong>What am I doing wrong here?</strong></p>
<p>I have tried to get insights from <a href=""https://stackoverflow.com/questions/48264676/tensorflow-valueerror-the-channel-dimension-of-the-inputs-should-be-defined-fo"">this</a>, <a href=""https://stackoverflow.com/questions/66013918/valueerror-the-channel-dimension-of-the-inputs-should-be-defined-found-none"">this</a>, <a href=""https://stackoverflow.com/questions/68978375/the-channel-dimension-of-the-inputs-should-be-defined-found-none"">this</a> and <a href=""https://stackoverflow.com/questions/64881851/tensorflow-keras-model-load-error-valueerror-the-last-dimension-of-the-inputs"">this</a>. But, I have not found a workable solution yet.</p>
<p>What should I do to remove the error and get the model to work?</p>
<p>I will appreciate any help.</p>
"
"<p>Hello I am getting this error I tried different torch and torchvision versions and nothing worked. Any suggestion please?</p>
<pre><code>env/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory
warn(f&quot;Failed to load image Python extension: {e}&quot;)
</code></pre>
<p>I tried different torch and torchvision versions</p>
"
"<p>Why is np.dot so much faster than np.sum?  Following this <a href=""https://stackoverflow.com/questions/61945412/which-method-is-faster-and-why-np-sumarr-vs-arr-sum/61945719#61945719"">answer</a> we know that np.sum is slow and has faster alternatives.</p>
<p>For example:</p>
<pre><code>In [20]: A = np.random.rand(1000)

In [21]: B = np.random.rand(1000)

In [22]: %timeit np.sum(A)
3.21 µs ± 270 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)

In [23]: %timeit A.sum()
1.7 µs ± 11.5 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)

In [24]: %timeit np.add.reduce(A)
1.61 µs ± 19.6 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)
</code></pre>
<p>But all of them are slower than:</p>
<pre><code>In [25]: %timeit np.dot(A,B)
1.18 µs ± 43.9 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)
</code></pre>
<p>Given that np.dot is both multiplying two arrays elementwise and then summing them, how can this be faster than just summing one array?  If B were set to the all ones array then np.dot would simply be summing A.</p>
<p>So it seems the  fastest option to sum A is:</p>
<pre><code>In [26]: O = np.ones(1000)
In [27]: %timeit np.dot(A,O)
1.16 µs ± 6.37 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)
</code></pre>
<p>This can't be right, can it?</p>
<p>This is on Ubuntu with numpy 1.24.2 using openblas64 on Python 3.10.6.</p>
<p>Supported SIMD extensions in this NumPy install:</p>
<pre><code>baseline = SSE,SSE2,SSE3
found = SSSE3,SSE41,POPCNT,SSE42,AVX,F16C,FMA3,AVX2
</code></pre>
<p><strong>Update</strong></p>
<p>The order of the timings reverses if the array is much longer.  That is:</p>
<pre><code>In [28]: A = np.random.rand(1000000)
In [29]: O = np.ones(1000000)
In [30]: %timeit np.dot(A,O)
545 µs ± 8.87 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)
In [31]: %timeit np.sum(A)
429 µs ± 11 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)    
In [32]: %timeit A.sum()
404 µs ± 2.95 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)
In [33]: %timeit np.add.reduce(A)
401 µs ± 4.21 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)
</code></pre>
<p>This implies to me that there is some fixed sized overhead when calling np.sum(A), A.sum(), np.add.reduce(A) that doesn't exist when calling np.dot() but the part of the code that does the summation is in fact faster.</p>
<p>——————————-</p>
<p>Any speed ups using cython, numba, python etc would be great to see.</p>
"
"<p>I downloaded the mambaforge install file for Windows, ran it, and it successfully completed. Mamba has a quickstart guide with CLI commands here: <a href=""https://mamba.readthedocs.io/en/latest/user_guide/mamba.html"" rel=""noreferrer"">https://mamba.readthedocs.io/en/latest/user_guide/mamba.html</a></p>
<p>But I can't figure out WHERE to enter the commands (e.g. mamba create -n envname). Is there supposed to be a Start Menu shortcut for Mambaforge or something similar? I checked the option to create Start Menu shortcuts during the install, but I don't see any even though the install completed without errors.</p>
<p>I tried running <code>mamba create -n envname</code> from the cmd prompt and it returns:</p>
<pre><code>'mamba' is not recognized as an internal or external command,
operable program or batch file.
</code></pre>
<p>Clearly I missed a step somewhere but I can't for the life of me figure out what.</p>
<p><em>What I tried already:</em>
I tried running the <code>Mambaforge-Windows-x86_64.exe</code> and checked the option to create Start Menu shortcuts. The install completed successfully. I found the mambaforge directory (which has a _conda.exe and python.exe among others)</p>
<p>I was expecting it to create a &quot;mambaforge&quot; start menu shortcut. As far as I can tell no start menu shortcuts were created.</p>
<p>The Windows installation instructions are literally a single line:
&quot;Download the installer and double click it on the file browser.&quot;
<a href=""https://github.com/conda-forge/miniforge"" rel=""noreferrer"">https://github.com/conda-forge/miniforge</a></p>
"
"<p>I want to do a network visualisation using pyvis in the latest version and the python version 3.9.6:</p>
<pre class=""lang-py prettyprint-override""><code>from pyvis.network import Network
g = Network()
g.add_node(0)
g.add_node(1)
g.add_edge(0, 1)
g.show('test.html')
</code></pre>
<p>every time I execute <code>g.show()</code> i get this error:</p>
<pre><code>Traceback (most recent call last):
  File &quot;/Users/tom/Library/Mobile Documents/com~apple~CloudDocs/Projekte/Coding_/f1 standings/test2.py&quot;, line 3, in &lt;module&gt;
    g.show('nx.html')
  File &quot;/Users/tom/Library/Python/3.9/lib/python/site-packages/pyvis/network.py&quot;, line 546, in show
    self.write_html(name, open_browser=False,notebook=True)
  File &quot;/Users/tom/Library/Python/3.9/lib/python/site-packages/pyvis/network.py&quot;, line 515, in write_html
    self.html = self.generate_html(notebook=notebook)
  File &quot;/Users/tom/Library/Python/3.9/lib/python/site-packages/pyvis/network.py&quot;, line 479, in generate_html
    self.html = template.render(height=height,
AttributeError: 'NoneType' object has no attribute 'render'
</code></pre>
<p>I tried updating pyvis, I changed all sorts of details in my code and I imported all of pyvis.network without any results.</p>
"
"<p>I'm working on simple flask app, and I received this error</p>
<pre><code>    from click.core import ParameterSource
ImportError: cannot import name 'ParameterSource' from 'click.core' (/usr/local/lib/python3.10/dist-packages/click/core.py)
</code></pre>
<p>I don't know why it's appearing, because everything was fine and then just...</p>
<p>Here are versions I use:</p>
<pre><code>black 23.1.0
click 8.1.3
Flask 2.2.3
Python 3.10.6
pip 22.0.2
</code></pre>
<p>I've been searching for solution and found that many people can't deal with this problem, and the only advice I found, is that I have to update Click and black to the latest version, but I'm already using the latest version.</p>
<p>What should I do? I there any any way to not use Click at all?</p>
<p>UPDATE</p>
<p>Here is how full error looks like</p>
<pre><code>Traceback (most recent call last):
  File &quot;/usr/lib/python3.10/runpy.py&quot;, line 187, in _run_module_as_main
    mod_name, mod_spec, code = _get_module_details(mod_name, _Error)
  File &quot;/usr/lib/python3.10/runpy.py&quot;, line 146, in _get_module_details
    return _get_module_details(pkg_main_name, error)
  File &quot;/usr/lib/python3.10/runpy.py&quot;, line 110, in _get_module_details
    __import__(pkg_name)
  File &quot;/home/diametr/.local/lib/python3.10/site-packages/flask/__init__.py&quot;, line 5, in &lt;module&gt;
    from .app import Flask as Flask
  File &quot;/home/diametr/.local/lib/python3.10/site-packages/flask/app.py&quot;, line 34, in &lt;module&gt;
    from . import cli
  File &quot;/home/diametr/.local/lib/python3.10/site-packages/flask/cli.py&quot;, line 15, in &lt;module&gt;
    from click.core import ParameterSource
ImportError: cannot import name 'ParameterSource' from 'click.core' (/usr/local/lib/python3.10/dist-packages/click/core.py)
</code></pre>
"
"<p>I'm using the <code>Jupyter</code> extension (v2022.9.1303220346) in <code>Visual Studio Code</code> (v1.73.1).</p>
<p>To reproduce this issue, make any modification to the notebook and check it into git. You'll observe that you get an extra difference for <code>execution_count</code>. For example (display from <code>Git Gui</code>):</p>
<pre><code>-   &quot;execution_count&quot;: 7,
+   &quot;execution_count&quot;: 9,
</code></pre>
<p>The execution count doesn't appear to be useful and is noise in the git history. Can Jupyter or VS Code be configured to stop updating this value or (better) ignore it altogether?</p>
"
"<p>Accordingo to the python <a href=""https://docs.python.org/3/library/sys.html"" rel=""nofollow noreferrer"">python documentation concerning <code>sys.stdout</code> and <code>sys.stderr</code></a>:</p>
<blockquote>
<p>stdout is used for the output of print() and expression statements and for the prompts of input();</p>
</blockquote>
<blockquote>
<p>The interpreter’s own prompts and its error messages go to stderr.</p>
</blockquote>
<p>Nevertheless, according to the documentation ot <a href=""https://tqdm.github.io/docs/tqdm/"" rel=""nofollow noreferrer"">tqdm</a> the default output is sys.stderr.</p>
<p>I am confused on why this would be the case given that it does not seem to be related to the interpreter own prompts or error messages. What am I missing? Why is tqdm output directed to sys.stderr and not to sys.stdout?</p>
<p>Edit: I think that the discussion here sort of answers this:
<a href=""https://stackoverflow.com/questions/35461687/when-to-use-sys-stdout-instead-of-sys-stderr"">When to use sys.stdout instead of sys.stderr?</a></p>
"
"<p>I'm following a tutorial on Youtube for image classification with machine learning. I keep getting the error as shown in the title from trying to save a deep learning module with tensorflow.</p>
<p>I'm using PyCharm on Windows 11.</p>
<p>here is my code...</p>
<pre><code>import cv2 as cv
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow import keras as ker

(training_images, training_labels), (testing_images, testing_labels) = ker.datasets.cifar10.load_data()
testing_images, testing_images = training_images / 255, testing_images / 255

class_names = ['Plane', 'Car', 'Bird', 'Cat', 'Deer', 'Dog', 'Frog', 'Horse', 'Ship', 'Truck']

for i in range(16):
    plt.subplot(4, 4, i+1)
    plt.xticks([])
    plt.yticks([])
    plt.imshow(training_images[i], cmap=plt.cm.binary)
    plt.xlabel(class_names[training_labels[i][0]])

plt.show()

training_images = training_images[:20000]
training_labels = training_labels[:20000]
testing_images = testing_images[:4000]
testing_labels = testing_labels[:4000]

#Model
model = ker.models.Sequential()
model.add(ker.layers.Conv2D(32, (3,3), activation='relu', input_shape=(32,32,3)))
model.add(ker.layers.MaxPooling2D((2,2)))
model.add(ker.layers.Conv2D(64, (3,3), activation='relu'))
model.add(ker.layers.MaxPooling2D((2,2)))
model.add(ker.layers.Conv2D(64, (3,3), activation='relu'))
model.add(ker.layers.Flatten())
model.add(ker.layers.Dense(64, activation='relu'))
model.add(ker.layers.Dense(10, activation='softmax'))

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

model.fit(training_images, training_labels, epochs=1, validation_data=(testing_images, testing_labels))

loss, accuracy = model.evaluate(testing_images, testing_labels)
print(f&quot;Loss: {loss}&quot;)
print(f&quot;Accuracy: {accuracy}&quot;)

model.save('img_classifier.model')

#model = ker.models.load_model()
</code></pre>
<p>the output...</p>
<pre><code>C:\img_classifier\venv\Scripts\python.exe 
C:\img_classifier\main.py 
625/625 [==============================] - 15s 23ms/step - loss: 2.1824 - accuracy: 0.2874 - val_loss: 2.3014 - val_accuracy: 0.1047
125/125 [==============================] - 1s 8ms/step - loss: 2.3014 - accuracy: 0.1047
Loss: 2.30141019821167
Accuracy: 0.10474999994039536
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla while saving (showing 4 of 4). These functions will not be directly callable after loading.
Traceback (most recent call last):
  File &quot;C:\Users\{user}\PycharmProjects\img_classifier\main.py&quot;, line 47, in &lt;module&gt;
    model.save('img_classifier.model')
  File &quot;C:\Users\{user}\AppData\Roaming\Python\Python311\site-packages\keras\utils\traceback_utils.py&quot;, line 70, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File &quot;C:\Users\{user}\AppData\Roaming\Python\Python311\site-packages\tensorflow\python\trackable\data_structures.py&quot;, line 823, in __getattribute__
    return super().__getattribute__(name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: this __dict__ descriptor does not support '_DictWrapper' objects

Process finished with exit code 1

</code></pre>
<p>I've tried changing IDE's, originally I was using VSCode but had way more errors, then I switched to PyCharm and this is the only error that persists. As you can see, the code for training the module has no issues, but I cannot save the module afterwards.</p>
<p>How can I fix this??</p>
"
"<p>I'm trying to run a Hugging Face model using the following code in Google Colab:</p>
<pre><code>!pip install transformers

from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(&quot;Helsinki-NLP/opus-mt-en-es&quot;)
inputs = tokenizer(text, return_tensors=&quot;pt&quot;).input_ids
</code></pre>
<p>And I'm having the following error:</p>
<pre><code>ValueError: This tokenizer cannot be instantiated. Please make sure you have `sentencepiece` installed in order to use this tokenizer.
</code></pre>
<p>How do I fix it?</p>
"
"<p>I am new to Polars and I am not sure whether I am using <code>.with_columns()</code> correctly.</p>
<p>Here's a situation I encounter frequently:
There's a dataframe and in <code>.with_columns()</code>, I apply some operation to a column. For example, I convert some dates from <code>str</code> to <code>date</code> type and then want to compute the duration between start and end date. I'd implement this as follows.</p>
<pre class=""lang-py prettyprint-override""><code>import polars as pl 

pl.DataFrame(
    {
        &quot;start&quot;: [&quot;01.01.2019&quot;, &quot;01.01.2020&quot;],
        &quot;end&quot;: [&quot;11.01.2019&quot;, &quot;01.05.2020&quot;],
    }
).with_columns(
    pl.col(&quot;start&quot;).str.to_date(),
    pl.col(&quot;end&quot;).str.to_date(),
).with_columns(
    (pl.col(&quot;end&quot;) - pl.col(&quot;start&quot;)).alias(&quot;duration&quot;),
)
</code></pre>
<p>First, I convert the two columns, next I call <code>.with_columns()</code> again.</p>
<p>Something shorter like this does not work:</p>
<pre class=""lang-py prettyprint-override""><code>pl.DataFrame(
    {
        &quot;start&quot;: [&quot;01.01.2019&quot;, &quot;01.01.2020&quot;],
        &quot;end&quot;: [&quot;11.01.2019&quot;, &quot;01.05.2020&quot;],
    }
).with_columns(
    pl.col(&quot;start&quot;).str.to_date(),
    pl.col(&quot;end&quot;).str.to_date(),
    (pl.col(&quot;end&quot;) - pl.col(&quot;start&quot;)).alias(&quot;duration&quot;),
)
</code></pre>
<pre><code># InvalidOperationError: sub operation not supported for dtypes `str` and `str`
</code></pre>
<p>Is there a way to avoid calling <code>.with_columns()</code> twice and to write this in a more compact way?</p>
"
"<p>Command:</p>
<pre class=""lang-none prettyprint-override""><code>pip install -r requirements.txt
</code></pre>
<p>Output:</p>
<pre class=""lang-none prettyprint-override""><code>error: externally-managed-environment

× This environment is externally managed
╰─&gt; To install Python packages system-wide, try apt install
python3-xyz, where xyz is the package you are trying to
install.

If you wish to install a non-Debian-packaged Python package,
create a virtual environment using python3 -m venv path/to/venv.
Then use path/to/venv/bin/python and path/to/venv/bin/pip. Make
sure you have python3-full installed.

If you wish to install a non-Debian packaged Python application,
it may be easiest to use pipx install xyz, which will manage a
virtual environment for you. Make sure you have pipx installed.

See /usr/share/doc/python3.11/README.venv for more information.

note: If you believe this is a mistake, please contact your Python installation or OS distribution provider. You can override this, at the risk of breaking your Python installation or OS, by passing --break-system-packages.
hint: See PEP 668 for the detailed specification.
</code></pre>
<p>I wish someone would explain to me what to do and how to solve it.</p>
"
"<p>When I run <code>pip install xyz</code> on a Linux machine (using Debian or Ubuntu or a derived distro), I get this error:</p>
<blockquote>
<pre class=""lang-none prettyprint-override""><code>error: externally-managed-environment

× This environment is externally managed
╰─&gt; To install Python packages system-wide, try apt install
    python3-xyz, where xyz is the package you are trying to
    install.

    If you wish to install a non-Debian-packaged Python package,
    create a virtual environment using python3 -m venv path/to/venv.
    Then use path/to/venv/bin/python and path/to/venv/bin/pip. Make
    sure you have python3-full installed.

    If you wish to install a non-Debian packaged Python application,
    it may be easiest to use pipx install xyz, which will manage a
    virtual environment for you. Make sure you have pipx installed.

    See /usr/share/doc/python3.11/README.venv for more information.

note: If you believe this is a mistake, please contact your Python installation or OS distribution provider. You can override this, at the risk of breaking your Python installation or OS, by passing --break-system-packages.
hint: See PEP 668 for the detailed specification.
</code></pre>
</blockquote>
<p>What does this error mean? How do I avoid it? Why doesn't <code>pip install xyz</code> work like it did before I upgraded my system using <code>sudo apt upgrade</code>?</p>
"
"<p>Prevously to use <a href=""https://mermaid.js.org/"" rel=""noreferrer"">Mermaid</a> in a Jupyter Notebook file, <a href=""https://pypi.org/project/nb-mermaid/"" rel=""noreferrer""><code>nb-mermaid</code></a> should be installed using <code>pip</code> and then it called using built-in magic commands <code>%%javascript</code> as instructed <a href=""https://bollwyvl.github.io/nb-mermaid/"" rel=""noreferrer"">here</a> or using <code>%%html</code>.</p>
<p>Unfortunately, the result, in a Jupyter Notebook file, <a href=""https://gist.github.com/bollwyvl/e51b4e724f0b82669c84"" rel=""noreferrer"">can not be displayed on GitHub</a>, but <a href=""https://nbviewer.org/gist/bollwyvl/e51b4e724f0b82669c84"" rel=""noreferrer"">will be displayed on nbviewer</a>. It works only in a GitHub page.</p>
<p>Then there is another way using <code>mermaid.ink</code> with IPython as guide in <a href=""https://mermaid.js.org/config/Tutorials.html#jupyter-integration-with-mermaid-js"" rel=""noreferrer"">here</a> as follows.</p>
<pre class=""lang-py prettyprint-override""><code>import base64
from IPython.display import Image, display
import matplotlib.pyplot as plt

def mm(graph):
  graphbytes = graph.encode(&quot;ascii&quot;)
  base64_bytes = base64.b64encode(graphbytes)
  base64_string = base64_bytes.decode(&quot;ascii&quot;)
  display(
    Image(
      url=&quot;https://mermaid.ink/img/&quot;
      + base64_string
    )
  )

mm(&quot;&quot;&quot;
graph LR;
    A--&gt; B &amp; C &amp; D;
    B--&gt; A &amp; E;
    C--&gt; A &amp; E;
    D--&gt; A &amp; E;
    E--&gt; B &amp; C &amp; D;
&quot;&quot;&quot;)
</code></pre>
<p>And it works fine and can be viewed on GitHub as in <a href=""https://github.com/dudung/py-jupyter-nb/blob/main/src/apply/flowchart/mermaid/begprocend.ipynb"" rel=""noreferrer"">here</a>.</p>
<p>But when it runs behind proxy the image, which is generated remotely on <code>https://mermaid.ink/</code> and display using <code>matplotlib</code>, can not be displayed in a Jupyter Notebook file. Is there any solution to this problem?</p>
"
"<ul>
<li><strong>tf-nightly version</strong> = 2.12.0-dev2023203</li>
<li><strong>Python version</strong> = 3.10.6</li>
<li><strong>CUDA drivers version</strong> = 525.85.12</li>
<li><strong>CUDA version</strong> = 12.0</li>
<li><strong>Cudnn version</strong> = 8.5.0</li>
<li>I am using <strong>Linux</strong> (x86_64, Ubuntu 22.04)</li>
<li>I am coding in <strong>Visual Studio Code</strong> on a <strong>venv</strong> virtual environment</li>
</ul>
<p>I am trying to run some models on the GPU (NVIDIA GeForce RTX 3050) using tensorflow nightly 2.12 (to be able to use Cuda 12.0). The problem that I have is that apparently every checking that I am making seems to be correct, but in the end the script is not able to detect the GPU. I've dedicated a lot of time trying to see what is happening and nothing seems to work, so any advice or solution will be more than welcomed. The GPU seems to be working for torch as you can see at the very end of the question.</p>
<p>I will show some of the most common checkings regarding CUDA that I did (Visual Studio Code terminal), I hope you find them useful:</p>
<ol>
<li><p><strong>Check CUDA version:</strong></p>
<p><code>$ nvcc --version</code></p>
<pre><code>nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2023 NVIDIA Corporation
Built on Fri_Jan__6_16:45:21_PST_2023
Cuda compilation tools, release 12.0, V12.0.140
Build cuda_12.0.r12.0/compiler.32267302_0
</code></pre>
</li>
<li><p><strong>Check if the connection with the CUDA libraries is correct:</strong></p>
<p><code>$ echo $LD_LIBRARY_PATH</code></p>
<pre><code>/usr/cuda/lib
</code></pre>
</li>
<li><p><strong>Check nvidia drivers for the GPU and check if GPU is readable for the venv:</strong></p>
<p><code>$ nvidia-smi</code></p>
<pre><code>+-----------------------------------------------------------------------------+
| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA GeForce ...  On   | 00000000:01:00.0  On |                  N/A |
| N/A   40C    P5     6W /  20W |     46MiB /  4096MiB |     22%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A      1356      G   /usr/lib/xorg/Xorg                 45MiB |
+-----------------------------------------------------------------------------+
</code></pre>
</li>
<li><p><strong>Add cuda/bin PATH and Check it:</strong></p>
<p><code>$ export PATH=&quot;/usr/local/cuda/bin:$PATH&quot;</code></p>
<p><code>$ echo $PATH</code></p>
<pre><code>/usr/local/cuda-12.0/bin:/home/victus-linux/Escritorio/MasterThesis_CODE/to_share/venv_master/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/snap/bin
</code></pre>
</li>
<li><p><strong>Custom function to check if CUDA is correctly installed: [<a href=""https://stackoverflow.com/questions/31326015/how-to-verify-cudnn-installation"">function by Sherlock</a>]</strong></p>
<pre class=""lang-bash prettyprint-override""><code>function lib_installed() { /sbin/ldconfig -N -v $(sed 's/:/ /' &lt;&lt;&lt; $LD_LIBRARY_PATH) 2&gt;/dev/null | grep $1; }
function check() { lib_installed $1 &amp;&amp; echo &quot;$1 is installed&quot; || echo &quot;ERROR: $1 is NOT installed&quot;; }
check libcuda
check libcudart
</code></pre>
<pre><code>libcudart.so.12 -&gt; libcudart.so.12.0.146
        libcuda.so.1 -&gt; libcuda.so.525.85.12
        libcuda.so.1 -&gt; libcuda.so.525.85.12
        libcudadebugger.so.1 -&gt; libcudadebugger.so.525.85.12
libcuda is installed
        libcudart.so.12 -&gt; libcudart.so.12.0.146
libcudart is installed
</code></pre>
</li>
<li><p><strong>Custom function to check if Cudnn is correctly installed: [<a href=""https://stackoverflow.com/questions/31326015/how-to-verify-cudnn-installation"">function by Sherlock</a>]</strong></p>
<pre class=""lang-bash prettyprint-override""><code>function lib_installed() { /sbin/ldconfig -N -v $(sed 's/:/ /' &lt;&lt;&lt; $LD_LIBRARY_PATH) 2&gt;/dev/null | grep $1; }
function check() { lib_installed $1 &amp;&amp; echo &quot;$1 is installed&quot; || echo &quot;ERROR: $1 is NOT installed&quot;; }
check libcudnn 
</code></pre>
<pre><code>        libcudnn_cnn_train.so.8 -&gt; libcudnn_cnn_train.so.8.8.0
        libcudnn_cnn_infer.so.8 -&gt; libcudnn_cnn_infer.so.8.8.0
        libcudnn_adv_train.so.8 -&gt; libcudnn_adv_train.so.8.8.0
        libcudnn.so.8 -&gt; libcudnn.so.8.8.0
        libcudnn_ops_train.so.8 -&gt; libcudnn_ops_train.so.8.8.0
        libcudnn_adv_infer.so.8 -&gt; libcudnn_adv_infer.so.8.8.0
        libcudnn_ops_infer.so.8 -&gt; libcudnn_ops_infer.so.8.8.0
libcudnn is installed
</code></pre>
</li>
</ol>
<p>So, once I did these previous checkings I used a script to evaluate if everything was finally ok and then the following error appeared:</p>
<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf

print(f'\nTensorflow version = {tf.__version__}\n')
print(f'\n{tf.config.list_physical_devices(&quot;GPU&quot;)}\n')
</code></pre>
<pre><code>2023-03-02 12:05:09.463343: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.
2023-03-02 12:05:09.489911: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.
2023-03-02 12:05:09.490522: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-03-02 12:05:10.066759: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT

Tensorflow version = 2.12.0-dev20230203

2023-03-02 12:05:10.748675: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-03-02 12:05:10.771263: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1956] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...

[]
</code></pre>
<p><strong>Extra check:</strong> I tried to run a checking script on torch and in here it worked so I guess the problem is related with tensorflow/tf-nightly</p>
<pre class=""lang-py prettyprint-override""><code>import torch

print(f'\nAvailable cuda = {torch.cuda.is_available()}')

print(f'\nGPUs availables = {torch.cuda.device_count()}')

print(f'\nCurrent device = {torch.cuda.current_device()}')

print(f'\nCurrent Device location = {torch.cuda.device(0)}')

print(f'\nName of the device = {torch.cuda.get_device_name(0)}')
</code></pre>
<pre><code>Available cuda = True

GPUs availables = 1

Current device = 0

Current Device location = &lt;torch.cuda.device object at 0x7fbe26fd2ec0&gt;

Name of the device = NVIDIA GeForce RTX 3050 Laptop GPU
</code></pre>
<p>Please, if you know something that might help solve this issue, don't hesitate on telling me.</p>
"
"<p>I am currently trying to use OpenAI's most recent model: <code>gpt-3.5-turbo</code>. I am following a very <a href=""https://www.youtube.com/watch?v=0l4UDn1p7gM&amp;ab_channel=TinkeringwithDeepLearning%26AI"" rel=""noreferrer"">basic tutorial</a>.</p>
<p>I am working from a Google Collab notebook. I have to make a request for each prompt in a list of prompts, which for sake of simplicity looks like this:</p>
<pre><code>prompts = ['What are your functionalities?', 'what is the best name for an ice-cream shop?', 'who won the premier league last year?']
</code></pre>
<p>I defined a function to do so:</p>
<pre><code>import openai

# Load your API key from an environment variable or secret management service
openai.api_key = 'my_API'

def get_response(prompts: list, model = &quot;gpt-3.5-turbo&quot;):
  responses = []

  
  restart_sequence = &quot;\n&quot;

  for item in prompts:

      response = openai.Completion.create(
      model=model,
      messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: prompt}],
      temperature=0,
      max_tokens=20,
      top_p=1,
      frequency_penalty=0,
      presence_penalty=0
    )

      responses.append(response['choices'][0]['message']['content'])

  return responses
</code></pre>
<p>However, when I call <code>responses = get_response(prompts=prompts[0:3])</code> I get the following error:</p>
<pre><code>InvalidRequestError: Unrecognized request argument supplied: messages
</code></pre>
<p>Any suggestions?</p>
<p>Replacing the <code>messages</code> argument with <code>prompt</code> leads to the following error:</p>
<pre><code>InvalidRequestError: [{'role': 'user', 'content': 'What are your functionalities?'}] is valid under each of {'type': 'array', 'minItems': 1, 'items': {'oneOf': [{'type': 'integer'}, {'type': 'object', 'properties': {'buffer': {'type': 'string', 'description': 'A serialized numpy buffer'}, 'shape': {'type': 'array', 'items': {'type': 'integer'}, 'description': 'Array shape'}, 'dtype': {'type': 'string', 'description': 'Stringified dtype'}, 'token': {'type': 'string'}}}]}, 'example': '[1, 1313, 451, {&quot;buffer&quot;: &quot;abcdefgh&quot;, &quot;shape&quot;: [1024], &quot;dtype&quot;: &quot;float16&quot;}]'}, {'type': 'array', 'minItems': 1, 'maxItems': 2048, 'items': {'oneOf': [{'type': 'string'}, {'type': 'object', 'properties': {'buffer': {'type': 'string', 'description': 'A serialized numpy buffer'}, 'shape': {'type': 'array', 'items': {'type': 'integer'}, 'description': 'Array shape'}, 'dtype': {'type': 'string', 'description': 'Stringified dtype'}, 'token': {'type': 'string'}}}], 'default': '', 'example': 'This is a test.', 'nullable': False}} - 'prompt'
</code></pre>
"
"<p>I am trying to load <a href=""https://huggingface.co/Carve/u2net-universal"" rel=""noreferrer"">this</a> semantic segmentation model from HF using the following code:</p>
<pre><code>from transformers import pipeline

model = pipeline(&quot;image-segmentation&quot;, model=&quot;Carve/u2net-universal&quot;, device=&quot;cpu&quot;)
</code></pre>
<p>But I get the following error:</p>
<pre><code>OSError: tamnvcc/isnet-general-use does not appear to have a file named config.json. Checkout 'https://huggingface.co/tamnvcc/isnet-general-use/main' for available files.
</code></pre>
<p>Is it even possible to load models from HuggingFace without config.json file provided?</p>
<p>I also tried loading the model via:</p>
<pre><code>id2label = {0: &quot;background&quot;, 1: &quot;target&quot;}
label2id = {&quot;background&quot;: 0, &quot;target&quot;: 1}
image_processor = AutoImageProcessor.from_pretrained(&quot;Carve/u2net-universal&quot;)
model = AutoModelForSemanticSegmentation(&quot;Carve/u2net-universal&quot;, id2label=id2label, label2id=label2id)
</code></pre>
<p>But got the same error.</p>
"
"<p>I am trying to organise DataFrame columns based on the specific rules, but I don't know the way.</p>
<p>For example, I have a DataFrame related to chemistry as shown below.
Each row shows the number of chemical bonds in a chemical compound.</p>
<pre><code>   OH  HO  CaO  OCa  OO  NaMg  MgNa
0   2   3    2    0   1     1     1
1   0   2    3    4   5     2     0
2   1   2    3    0   0     0     0
</code></pre>
<p>In chemistry, OH (Oxygen-Hydrogen) bond is equal to HO (Hydrogen-Oxygen) bond and CaO (Calcium-Oxygen) bond is equal to OCa (Oxygen-Calcium) bond in the meaning. Thus, I'd like to organise the DataFrame as shown below.</p>
<pre><code>   OH  CaO  OO  NaMg 
0   5    2   1     2
1   2    7   9     2
2   3    3   0     0
</code></pre>
<p>I’m struggling because:</p>
<ul>
<li>there are a variety of chemical bonds in my real DataFrame, so it is impossible to organise the information one by one (The number of columns is more than 3,000 and I don't know which kinds of chemical bonds exist and are duplicates.)</li>
<li>the number of letters depends on each element symbol and some symbols include lowercase
(e.g. Hydrogen: H (one letter and only uppercase), Calcium: Ca (Two letters and uppercase &amp; lowercase)</li>
</ul>
<p>I looked for the same question online and wrote codes by myself, but I was not able to find the way. I would like to know the codes which solve my problem.</p>
"
"<p>I've been using cpython forever, but I'm new to pypy.</p>
<p>In cpython, this is how I use virtual environments and pip.</p>
<pre class=""lang-bash prettyprint-override""><code>python3 -m venv venv
source venv/bin/activate
python3 -m pip install &lt;package&gt;
</code></pre>
<p>I recently started using pypy for a project, and noticed that the following works.</p>
<pre class=""lang-bash prettyprint-override""><code>pypy3 -m venv venv
source venv/bin/activate
pypy3 -m pip install &lt;package&gt;
</code></pre>
<p>Questions:</p>
<ul>
<li>Are there any differences between cpython venv/pip and pypy venv/pip?</li>
<li>Can I create a venv using cpython, and use it with pypy, or vice-versa?</li>
<li>Similarly, can I install packages using cpython's pip, and use them from pypy interpreter, or vice-versa?</li>
<li>Is what I'm doing &quot;correct&quot;, or are there any downsides/issues I'll face in future if I go down this road.</li>
</ul>
<p>Reasons why I prefer the <code>python3 -m ...</code> invocations:</p>
<ul>
<li>venv is present in std. lib, so I don't have to globally install virtualenv.</li>
<li>Less ambiguous than using <code>pip</code> and <code>pip3</code>.</li>
</ul>
<p>References:</p>
<ul>
<li><a href=""https://stackoverflow.com/questions/41573587"">What is the difference between venv, pyvenv, pyenv, virtualenv, virtualenvwrapper, pipenv, etc?</a></li>
<li><a href=""https://stackoverflow.com/questions/61664673"">Should I use pip or pip3?</a></li>
</ul>
<hr />
<p>EDIT:<br />
Tried to share venv's between cpython and venv doesn't work (seems obvious in hindsight). It's still possible to create two separate venv's like <code>python3 -m venv cpython_venv</code> and <code>pypy3 -m venv pypy_venv</code> and switch between them as needed. <code>python</code> will be bound to cpython or pypy based on which virtual env is active, and pypi packages need to be installed in every venv where it's needed.</p>
"
"<p>In my setting I have an abstract situation like the following, which shall only function as an example case:</p>
<pre class=""lang-py prettyprint-override""><code>base = trial.suggest_int(1, 3)
power = trial.suggest_int(1, 10)
# value = base ** power
</code></pre>
<p>As when the <code>base == 1</code> the power parameter becomes irrelevant and I would like to fix it to 1.</p>
<p>For example:</p>
<pre class=""lang-py prettyprint-override""><code>base = trial.suggest_int(&quot;base&quot;, 1, 3)
if base == 1:
   # Different distribution! But still inside the other.
   power = trial.suggest_int(&quot;power&quot;, 1, 1) 
else:
   power = trial.suggest_int(&quot;power&quot;, 1, 10)
</code></pre>
<p>While this works it creates later problems in the form of <code>ValueError</code>s because the underlying distributions are not the same.</p>
<hr />
<p>How can I suggest a fixed value <strong>with the same parameter name</strong> that depends on another value that is sampled within the trial?</p>
"
"<p>I have two models Product and Cart. Product model has  <code>maximum_order_quantity</code>. While updating quantity in cart, I'll have to check whether quantity is greater than <code>maximum_order_quantity</code>at database level. For that am comparing quantity with <code>maximum_order_quantity</code> in Cart Model But it throws an error when I try to migrate</p>
<p><code>cart.CartItems: (models.E041) 'constraints' refers to the joined field 'product__maximum_order_quantity'</code>.</p>
<p>Below are my models</p>
<pre><code>class Products(models.Model):
    category = models.ForeignKey(
        Category, on_delete=models.CASCADE, related_name=&quot;products&quot;
    )
    product_name = models.CharField(max_length=50, unique=True)
    base_price = models.IntegerField()
    product_image = models.ImageField(
        upload_to=&quot;photos/products&quot;, null=True, blank=True
    )
    stock = models.IntegerField(validators=[MinValueValidator(0)])
    maximum_order_quantity = models.IntegerField(null=True, blank=True)
)

</code></pre>
<pre><code>class CartItems(models.Model):
    cart = models.ForeignKey(Cart, on_delete=models.CASCADE)
    product = models.ForeignKey(Products, on_delete=models.CASCADE)
    quantity = models.IntegerField()

    class Meta:
        verbose_name_plural = &quot;Cart Items&quot;
        constraints = [
            models.CheckConstraint(
            check=models.Q(quantity__gt=models.F(&quot;product__maximum_order_quantity&quot;)),
            name=&quot;Quantity cannot be more than maximum order quantity&quot;
            )
        ]
</code></pre>
<h1>Error</h1>
<pre><code>SystemCheckError: System check identified some issues:

ERRORS:
cart.CartItems: (models.E041) 'constraints' refers to the joined field 'product__maximum_order_quantity'.
</code></pre>
"
"<p>I have two tensors:</p>
<pre><code># losses_q
tensor(0.0870, device='cuda:0', grad_fn=&lt;SumBackward0&gt;)
# this_loss_q
tensor([0.0874], device='cuda:0', grad_fn=&lt;AddBackward0&gt;)
</code></pre>
<p>When I am trying to concat them, PyTorch raises an error:</p>
<pre><code>losses_q = torch.cat((losses_q, this_loss_q), dim=0)

RuntimeError: zero-dimensional tensor (at position 0) cannot be concatenated
</code></pre>
<p>How to resolve this error?</p>
"
"<p>I wrote pytest fixture in fixtures.py file and using it in my main_test.py . But I am getting this error in flake8: <strong>F401 'utils_test.backup_path' imported but unused</strong> for this code:</p>
<pre><code>@pytest.fixture
def backup_path():
    ...
</code></pre>
<pre><code>from fixtures import backup_path


def test_filename(backup_path):
    ...
</code></pre>
<p>How can I resolve this?</p>
"
"<p>I have a problem with this code because it work perfectly on laptops and desktop devices but not on mobile devices , please help me and tell me what are the things to add so it will work for both.
I added the touch events but it didn't work
This is the my Js code :</p>
<pre><code>const draggableElements = document.querySelectorAll(&quot;.draggable&quot;);
const droppableElements = document.querySelectorAll(&quot;.droppable&quot;);


draggableElements.forEach(elem =&gt; {
  elem.addEventListener(&quot;dragstart&quot;, dragStart);
  elem.addEventListener(&quot;touchstart&quot;, dragStart);
});

droppableElements.forEach(elem =&gt; {
  elem.addEventListener(&quot;dragenter&quot;, dragEnter); 
  elem.addEventListener(&quot;dragover&quot;, dragOver ); 
  elem.addEventListener(&quot;dragleave&quot;, dragLeave);
  elem.addEventListener(&quot;drop&quot;, drop);
  elem.addEventListener(&quot;touchstart&quot;, dragEnter); 
  elem.addEventListener(&quot;touchmove&quot;, dragOver ); 
  elem.addEventListener(&quot;touchend&quot;, drop);
  elem.addEventListener(&quot;touchcancel&quot;, dragLeave);
});

function dragStart(event) {
  event.dataTransfer.setData('text/plain', event.target.id);
  setTimeout(() =&gt; {
      event.target.classList.add('hide');
  }, 0);
}


function dragEnter(event) {
  event.preventDefault();
  event.target.classList.add('droppable-hover');
}

function dragOver(event) {
  event.preventDefault();
  event.target.classList.add('droppable-hover');
}

function dragLeave(event) {
  event.target.classList.remove('droppable-hover');
}


function drop(event) {
  event.preventDefault();
  var id = event.dataTransfer.getData(&quot;text/plain&quot;);
  var draggedElement = document.getElementById(id);
  var container = event.target.closest(&quot;.droppable&quot;);
  if (container &amp;&amp; draggedElement) {
    var existingImage = container.querySelector(&quot;img&quot;);
    if (existingImage) {
      const draggableContainer = document.querySelector(&quot;.draggable&quot;);
      container.removeChild(existingImage);
      draggableContainer.appendChild(existingImage);
      container.appendChild(draggedElement);
    }
    existingImage = draggedElement;
  }
} 
</code></pre>
"
"<p><code>python 3.10</code> running in <code>venv</code> on <code>Windows 10 pro</code>.</p>
<p>I am trying to follow the tutorial for the <code>Celery</code> and <code>Flask</code> integration: <a href=""https://flask.palletsprojects.com/en/latest/patterns/celery/"" rel=""noreferrer"">https://flask.palletsprojects.com/en/latest/patterns/celery/</a></p>
<pre class=""lang-py prettyprint-override""><code># example.py

from celery import Celery, Task
from flask import Flask


def celery_init_app(app: Flask) -&gt; Celery:
    class FlaskTask(Task):
        def __call__(self, *args: object, **kwargs: object) -&gt; object:
            with app.app_context():
                return self.run(*args, **kwargs)

    celery_app = Celery(app.name, task_cls=FlaskTask)
    celery_app.config_from_object(app.config[&quot;CELERY&quot;])
    celery_app.set_default()
    app.extensions[&quot;celery&quot;] = celery_app
    return celery_app


def create_app() -&gt; Flask:
    app = Flask(__name__)
    app.config.from_mapping(
        CELERY=dict(
            # Redis Docker container connection string
            broker_url=&quot;redis://default:redispw@localhost:55000&quot;,
            result_backend=&quot;redis://default:redispw@localhost:55000&quot;,
            task_ignore_result=True,
        ),
    )
    app.config.from_prefixed_env()
    celery_init_app(app)
    return app
</code></pre>
<pre class=""lang-py prettyprint-override""><code># make_celery.py

from example import create_app

flask_app = create_app()
celery_app = flask_app.extensions[&quot;celery&quot;]
</code></pre>
<p>Command line call:
<code>celery -A make_celery worker --loglevel INFO</code></p>
<p>Runs into:</p>
<pre><code>Unrecoverable error: AttributeError(&quot;Can't pickle local object 'celery_init_app.&lt;locals&gt;.FlaskTask'&quot;)
[...]
.venv\lib\site-packages\billiard\reduction.py&quot;, line 123, in steal_handle
    return _winapi.DuplicateHandle(
PermissionError: [WinError 5] Access is denied
</code></pre>
<p>The same error and traceback occurs on the example repo:
<a href=""https://github.com/pallets/flask/tree/main/examples/celery"" rel=""noreferrer"">https://github.com/pallets/flask/tree/main/examples/celery</a></p>
<p>How can I resolve this?
What alternatives are available?</p>
<p>Tried moving the <code>class FlaskTask(Task)</code> out of the local scope of the <code>celery_init_app</code> function, but then I obviously lose access to the <code>app</code> variable.</p>
"
"<p><sub>There are a great many existing Q&amp;A on Stack Overflow on this general theme, but they are all either poor quality (typically, implied from a beginner's debugging problem) or miss the mark in some other way (generally by being insufficiently general). There are at least two extremely common ways to get the naive code wrong, and beginners would benefit more from a canonical about looping than from having their questions closed as typos or a canonical about what printing entails. So this is my attempt to put all the related information in the same place.</sub></p>
<p>Suppose I have some simple code that does a calculation with a value <code>x</code> and assigns it to <code>y</code>:</p>
<pre><code>y = x + 1

# Or it could be in a function:
def calc_y(an_x):
    return an_x + 1
</code></pre>
<p>Now I want to repeat the calculation for many possible values of <code>x</code>. I know that I can use a <code>for</code> loop if I already have a list (or other sequence) of values to use:</p>
<pre><code>xs = [1, 3, 5]
for x in xs:
    y = x + 1
</code></pre>
<p>Or I can use a <code>while</code> loop if there is some other logic to calculate the sequence of <code>x</code> values:</p>
<pre><code>def next_collatz(value):
    if value % 2 == 0:
        return value // 2
    else:
        return 3 * value + 1

def collatz_from_19():
    x = 19
    while x != 1:
        x = next_collatz(x)
</code></pre>
<p>The question is: <strong>how can I collect these values and use them after the loop</strong>? I tried <code>print</code>ing the value inside the loop, but it doesn't give me anything useful:</p>
<pre><code>xs = [1, 3, 5]
for x in xs:
    print(x + 1)
</code></pre>
<p>The results show up on the screen, but I can't find any way to use them in the next part of the code. So I think I should try to store the values in a container, like a list or a dictionary. But when I try that:</p>
<pre><code>xs = [1, 3, 5]
for x in xs:
    ys = []
    y = x + 1
    ys.append(y)
</code></pre>
<p>or</p>
<pre><code>xs = [1, 3, 5]
for x in xs:
    ys = {}
    y = x + 1
    ys[x] = y
</code></pre>
<p>After either of these attempts, <code>ys</code> only contains the last result.</p>
"
"<p>I am trying to edit an existing pdf file using <code>PyPDF</code> and <code>ReportLab</code>. When I try to position the red circle and red text, it appears to be hiding behind a white container or something. If I position it anywhere else, it works fine. What is causing this?</p>
<p>sample pdf = <a href=""https://www.puc.nh.gov/regulatory/CASEFILE/2010/10-246/INITIAL%20FILING%20-%20PETITION/10-246%202010-09-13%20BAYRING%20ATT%20TO%20PET%20FOR%20AUTH%20TO%20CONSTRUCT%20UTILITY%20CABLE%20OVER%20AND%20ACROSS%20SUNCOOK%20RIVER.PDF"" rel=""nofollow noreferrer"">https://www.puc.nh.gov/regulatory/CASEFILE/2010/10-246/INITIAL%20FILING%20-%20PETITION/10-246%202010-09-13%20BAYRING%20ATT%20TO%20PET%20FOR%20AUTH%20TO%20CONSTRUCT%20UTILITY%20CABLE%20OVER%20AND%20ACROSS%20SUNCOOK%20RIVER.PDF</a></p>
<p><code>WHAT THE ERROR IS:</code></p>
<p><code>WHAT THE FINAL RESULT SHOULD BE:</code></p>
<pre><code>from PyPDF2 import PdfWriter, PdfReader
import io
from reportlab.pdfgen import canvas
from reportlab.lib.pagesizes import letter
from reportlab.lib.colors import red

def main():
    packet = io.BytesIO()
    can = canvas.Canvas(packet, pagesize=letter)
    can.setFillColorRGB(1, 0, 0)
    can.circle(370,780,20,fill=1)
    can.setFillColor(red)
    can.setFont(&quot;Times-Roman&quot;, 14)
    can.drawString(352, 785, &quot;Customer Group #22&quot;)
    can.save()

    packet.seek(0)
    new_pdf = PdfReader(packet)

    existing_pdf = PdfReader(open(&quot;samplePDF.pdf&quot;, &quot;rb&quot;))
    output = PdfWriter()

    page = existing_pdf.pages[1]
    page.merge_page(new_pdf.pages[0])
    output.add_page(page)

    outputStream = open(&quot;finalPDF.pdf&quot;, &quot;wb&quot;)
    output.write(outputStream)
    outputStream.close()
if __name__ == &quot;__main__&quot;:
    main()
</code></pre>
"
"<p>From this github link: <a href=""https://github.com/pyxll/pyxll-examples/blob/master/bitmex/bitmex.py"" rel=""noreferrer"">https://github.com/pyxll/pyxll-examples/blob/master/bitmex/bitmex.py</a></p>
<p>When I run this code, I get the message saying the explicit passing of coroutine objects to asyncio.wait() is deprecated. I've pinpointed this to line 71: <code>await asyncio.wait(tasks)</code>, but can't figure out how to resolve the issue.</p>
<p>Code below for reference:</p>
<pre><code>from pyxll import xl_func, RTD, get_event_loop
import websockets
import asyncio
import json


class BitMex:
    &quot;&quot;&quot;Class to manage subscriptions to instrument prices.&quot;&quot;&quot;

    URI = &quot;wss://www.bitmex.com/realtime&quot;

    def __init__(self, loop=None):
        self.__websocket = None
        self.__running = False
        self.__running_task = None
        self.__subscriptions = {}
        self.__data = {}
        self.__lock = asyncio.Lock(loop=loop)

    async def __connect(self):
        # Connect to the websocket API and start the __run coroutine
        self.__running = True
        self.__websocket = await websockets.connect(self.URI)
        self.__connecting_task = None
        self.__running_task = asyncio.create_task(self.__run())

    async def __disconnect(self):
        # Close the websocket and wait for __run to complete
        self.__running = False
        await self.__websocket.close()
        self.__websocket = None
        await self.__running_task

    async def __run(self):
        # Read from the websocket until disconnected
        while self.__running:
            msg = await self.__websocket.recv()
            await self.__process_message(json.loads(msg))

    async def __process_message(self, msg):
        if msg.get(&quot;table&quot;, None) == &quot;instrument&quot;:
            # Extract the data from the message, update our data dictionary and notify subscribers
            for data in msg.get(&quot;data&quot;, []):
                symbol = data[&quot;symbol&quot;]
                timestamp = data[&quot;symbol&quot;]

                # Update the latest values in our data dictionary and notify any subscribers
                tasks = []
                subscribers = self.__subscriptions.get(symbol, {})
                latest = self.__data.setdefault(symbol, {})
                for field, value in data.items():
                    latest[field] = (value, timestamp)

                    # Notify the subscribers with the updated field
                    for subscriber in subscribers.get(field, []):
                        tasks.append(subscriber(symbol, field, value, timestamp))

                # await all the tasks from the subscribers
                if tasks:
                    await asyncio.wait(tasks)

    async def subscribe(self, symbol, field, callback):
        &quot;&quot;&quot;Subscribe to updates for a specific symbol and field.

        The callback will be called as 'await callback(symbol, field, value, timestamp)'
        whenever an update is received.
        &quot;&quot;&quot;
        async with self.__lock:
            # Connect the websocket if necessary
            if self.__websocket is None:
                await self.__connect()

            # Send the subscribe message if we're not already subscribed
            if symbol not in self.__subscriptions:
                msg = {&quot;op&quot;: &quot;subscribe&quot;, &quot;args&quot;: [f&quot;instrument:{symbol}&quot;]}
                await self.__websocket.send(json.dumps(msg))

            # Add the subscriber to the dict of subscriptions
            self.__subscriptions.setdefault(symbol, {}).setdefault(field, []).append(callback)

            # Call the callback with the latest data
            data = self.__data.get(symbol, {})
            if field in data:
                (value, timestamp) = data[field]
                await callback(symbol, field, value, timestamp)

    async def unsubscribe(self, symbol, field, callback):
        async with self.__lock:
            # Remove the subscriber from the list of subscriptions
            self.__subscriptions[symbol][field].remove(callback)
            if not self.__subscriptions[symbol][field]:
                del self.__subscriptions[symbol][field]

            # Unsubscribe if we no longer have any subscriptions for this instrument
            if not self.__subscriptions[symbol]:
                msg = {&quot;op&quot;: &quot;unsubscribe&quot;, &quot;args&quot;: [f&quot;instrument:{symbol}&quot;]}
                await self.__websocket.send(json.dumps(msg))
                del self.__subscriptions[symbol]
                self.__data.pop(symbol, None)

            # Disconnect if we no longer have any subscriptions
            if not self.__subscriptions:
                async with self.__lock:
                    await self.__disconnect()


class BitMexRTD(RTD):
    &quot;&quot;&quot;RTD class for subscribing to BitMEX prices using the
    BitMex class above.
    &quot;&quot;&quot;

    # Use a single BitMex object for all RTD functions
    _bitmex = BitMex(get_event_loop())

    def __init__(self, symbol, field):
        super().__init__(value=&quot;Waiting...&quot;)
        self.__symbol = symbol
        self.__field = field

    async def connect(self):
        # Subscribe to BitMix updates when Excel connects to the RTD object
        await self._bitmex.subscribe(self.__symbol, self.__field, self.__update)

    async def disconnect(self):
        # Unsubscribe to BitMix updates when Excel disconnects from the RTD object
        await self._bitmex.unsubscribe(self.__symbol, self.__field, self.__update)

    async def __update(self, symbol, field, value, timestamp):
        # Update the value in Excel
        self.value = value


@xl_func(&quot;string symbol, string field: rtd&quot;, recalc_on_open=True)
def bitmex_rtd(symbol, field=&quot;lastPrice&quot;):
    &quot;&quot;&quot;Subscribe to BitMEX prices for a given symbol.&quot;&quot;&quot;
    return BitMexRTD(symbol, field)


if __name__ == &quot;__main__&quot;:

    async def main():
        # This is the callback that will be called whenever there's an update
        async def callback(symbol, field, value, timestamp):
            print((symbol, field, value, timestamp))

        bm = BitMex()

        await bm.subscribe(&quot;XBTUSD&quot;, &quot;lastPrice&quot;, callback)

        await asyncio.sleep(60)

        await bm.unsubscribe(&quot;XBTUSD&quot;, &quot;lastPrice&quot;, callback)

        print(&quot;DONE!&quot;)

    # Run the 'main' function in an asyncio event loop
    loop = asyncio.get_event_loop()
    loop.create_task(main())
    loop.run_forever()
</code></pre>
"
"<p>I want to parse TOML files in python 3.9, and I am wondering if I can do so without installing another package.</p>
<p>Since <code>pip</code> knows how to work with <code>pyproject.toml</code> files, and I already have <code>pip</code> installed, does <code>pip</code> provide a parser that I can import and use in my own code?</p>
"
"<p>This is my environment:</p>
<pre><code>$ python
Python 3.8.10 (default, Nov 14 2022, 12:59:47) 
[GCC 9.4.0] on linux
Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.
&gt;&gt;&gt; quit()
$ lscpu | grep &quot;Model name&quot;
Model name:                      Intel(R) Core(TM) i5-4430 CPU @ 3.00GHz
$ sudo dmidecode --type memory | grep -E &quot;^\\s+(Speed|Type):&quot; -
    Type: DDR3
    Speed: 1600 MT/s
    Type: DDR3
    Speed: 1600 MT/s
</code></pre>
<p>I consistently get timing results like these on my machine:</p>
<pre><code>$ python -m timeit -s &quot;x = list(range(250)) * 4&quot; &quot;[*x]&quot;
200000 loops, best of 5: 1.54 usec per loop
$ python -m timeit -s &quot;x = [0, 1, 2, 3] * 250&quot; &quot;[*x]&quot;
200000 loops, best of 5: 1.48 usec per loop
$ python -m timeit -s &quot;x = [0, 1] * 500&quot; &quot;[*x]&quot;
100000 loops, best of 5: 2 usec per loop
$ python -m timeit -s &quot;x = [0] * 1000&quot; &quot;[*x]&quot;
100000 loops, best of 5: 3.84 usec per loop
</code></pre>
<p>(Here I have deliberately avoided using integers larger than 255, because I know about the <a href=""https://stackoverflow.com/questions/306313"">caching of small integers</a>.</p>
<p>Similarly, with single-character strings:</p>
<pre><code>$ python -m timeit -s &quot;x = [chr(i) for i in range(250)] * 4&quot; &quot;[*x]&quot;
200000 loops, best of 5: 1.81 usec per loop
$ python -m timeit -s &quot;x = [chr(0), chr(1), chr(2), chr(3)] * 250&quot; &quot;[*x]&quot;
200000 loops, best of 5: 1.5 usec per loop
$ python -m timeit -s &quot;x = [chr(0), chr(1)] * 500&quot; &quot;[*x]&quot;
100000 loops, best of 5: 2.03 usec per loop
$ python -m timeit -s &quot;x = [chr(0)] * 1000&quot; &quot;[*x]&quot;
100000 loops, best of 5: 3.83 usec per loop
</code></pre>
<p>(Again, I deliberately limit the character code points so that Python's <a href=""https://peps.python.org/pep-0393/"" rel=""noreferrer"">flexible string representation</a> will choose a consistent, single-byte-per-character representation.)</p>
<p><em>All of these input lists are length 1000, and the same method is being used to copy them, so I wouldn't expect dramatic differences in the timing</em>. However, I find that the lists that consist of the same simple element (a single-character string or a small integer) over and over, take nearly twice as long to copy as anything else; that using a few different values is even faster than using two alternating values; and that using a wide variety of values is slightly slower again.</p>
<p>With slower copying methods, the difference is still present, but less dramatic:</p>
<pre><code>$ python -m timeit -s &quot;x = [chr(i) for i in range(250)] * 4&quot; &quot;[i for i in x]&quot;
20000 loops, best of 5: 18.2 usec per loop
$ python -m timeit -s &quot;x = [chr(0), chr(1), chr(2), chr(3)] * 250&quot; &quot;[i for i in x]&quot;
20000 loops, best of 5: 17.3 usec per loop
$ python -m timeit -s &quot;x = [chr(0), chr(1)] * 500&quot; &quot;[i for i in x]&quot;
20000 loops, best of 5: 17.9 usec per loop
$ python -m timeit -s &quot;x = [chr(0)] * 1000&quot; &quot;[i for i in x]&quot;
20000 loops, best of 5: 19.1 usec per loop
</code></pre>
<p><strong>Why does this occur?</strong></p>
"
"<p>I'm trying to get logs from CloudWatch, and I'm interested in the first and the last log line, so I'm querying with both <code>startFromHead=True</code> (oldest to newest) and <code>startFromHead=False</code> (newest to oldest). However, when querying the same log, I get events when <code>startFromHead=True</code>, but not when <code>startFromHead=False</code>.</p>
<p>Here's some sample code, where I've defined the <code>logStreamName</code> and <code>logGroupName</code> elsewhere:</p>
<pre><code>cw = boto3.client('logs')
cw.get_log_events(
    logGroupName=logGroupName,
    logStreamName=logStreamName,
    startFromHead=True,
    limit=1
)
# Returns the oldest log entry in the log stream
</code></pre>
<pre><code>cw = boto3.client('logs')
cw.get_log_events(
    logGroupName=logGroupName,
    logStreamName=logStreamName,
    startFromHead=False,
    limit=1
)
# events key points to an empty list in the response, as though there are no events
</code></pre>
<p>I don't know why I'm not getting any results where <code>startFromHead=False</code>. What's more, it's not every log stream. The first stream in my group was fine, but the second showed this behavior. I tried iterating through the streams in the group, and all of them had this behavior except for the first.</p>
"
"<p>I open a monorepo folder within VS Code where the top folders are different services. One of the services is a python service using poetry to install dependencies.</p>
<p>I am using poetry's <code>in-project=true</code> <code>virtualenv</code> setting so that all dependencies are actually stored in <code>./python-service/.venv</code>.</p>
<p>The monorepo can be run locally using a docker-compose file. Consequently, any dependency installation is done on the container.</p>
<p>VS Code does not automatically detect this virtual environment, but when manually set, it errors immediately:</p>
<pre><code>Linter 'flake8' is not installed. Please install it or select another linter&quot;.
Error: spawn /Users/me/dev/project/python-service/.venv/bin/python EACCES
</code></pre>
<p>If I look at the <code>./python-service/.venv/bin/python</code> file, I can see it's actually a symlink pointing at <code>/usr/local/bin/python</code> which does not exist on my machine, only on the container.</p>
<p>Since VS Code is running on my machine, not the container, it makes sense that it cannot follow this symlink.</p>
<p>I have considered two options:</p>
<ol>
<li>Use poetry's <code>always-copy</code> <code>virtualenv</code> option. For some reason this does not actually copy the <code>python</code> binary (<a href=""https://github.com/python-poetry/poetry/discussions/7323"" rel=""noreferrer"">replicated by someone else</a>). So this doesn't actually resolve this problem.</li>
<li>Dev containers plugin for VS Code. While this works in theory, having to open a new window for each python service seems cumbersome. Ideally VS Code is able to use the appropriate venv as necessary.</li>
</ol>
<p>What is the correct way to configure poetry/vscode/docker to ensure an interpreter can be set in VS Code?</p>
"
"<h3>The statement</h3>
<ul>
<li>I'm reading data sets using <code>Polars.read_csv()</code> method via a Python file handler:</li>
</ul>
<pre><code> with gzip.open(os.path.join(getParameters()['rdir'], dataset)) as compressed_file:
    df = pl.read_csv(compressed_file, sep = '\t', ignore_errors=True)
</code></pre>
<ul>
<li>A performance warning keeps popping up:</li>
</ul>
<pre><code>Polars found a filename. Ensure you pass a path to the file instead of a python file object when possible for best performance.
</code></pre>
<h3>Possible solutions</h3>
<ul>
<li>I already tried Python warning suppression, but it seems Polars literally just prints out this statement without any default warning associated.</li>
<li>Another possibility would be to read using non-handler methods?</li>
</ul>
<p>Any ideas on how to get rid of this annoying message will be highly appreciated.</p>
"
"<p>Python 3.10 doesn't think so:</p>
<pre class=""lang-py prettyprint-override""><code>Python 3.10.6 | packaged by conda-forge | (main, Aug 22 2022, 20:38:29) [Clang 13.0.1 ] \
    on darwin
Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.
&gt;&gt;&gt; from typing import Iterable
&gt;&gt;&gt; isinstance(list[str], Iterable)
False
&gt;&gt;&gt; list(list[str])
Traceback (most recent call last):
  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;
TypeError: 'types.GenericAlias' object is not iterable
</code></pre>
<p>Python 3.11 considers it is:</p>
<pre class=""lang-py prettyprint-override""><code>Python 3.11.0 | packaged by conda-forge | (main, Jan 15 2023, 05:44:48) [Clang 14.0.6 ] \
    on darwin
Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.
&gt;&gt;&gt; from typing import Iterable
&gt;&gt;&gt; isinstance(list[str], Iterable)
True
&gt;&gt;&gt; list(list[str])
[*list[str]]
</code></pre>
<p>If it is an iterable, what should be the result of iterating over it? The <code>*list[str]</code> item appears to be the <code>unpacking</code> of itself or of a type variable tuple.<br />
What's going on here? I know that typing in python is in state of flux and evolving rapidly, but I really don't know how to interpret this.</p>
"
"<p>I am currently trying to clean my google cloud log explorer and want to add summary fields to show up <strong>by default</strong> in the explorer like this:</p>
<p><a href=""https://i.sstatic.net/PrzSy.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/PrzSy.png"" alt=""https://i.sstatic.net/PrzSy.png"" /></a></p>
<p>My desired effect can be achieved by setting custom summary fields to show label values, but I want this behaviour by default.</p>
<p>Current code being used to generate the log:</p>
<pre><code>from google.cloud import logging_v2

def write_entry(logger_name):
    &quot;&quot;&quot;Writes log entries to the given logger.&quot;&quot;&quot;
    logging_client = logging_v2.Client(project=&quot;test-project&quot;)

    # This log can be found in the Cloud Logging console under 'Custom Logs'.
    logger = logging_client.logger(&quot;TEST&quot;,labels={&quot;label1&quot;:&quot;I want this to show in default summary field&quot;})

    # Make a simple text log
    logger.log_text(&quot;Hello, world!&quot;)

    print(&quot;Wrote logs to {}.&quot;.format(logger.name))

write_entry(&quot;test-log-bucket&quot;)
</code></pre>
<p>I would like the log entry <code>label1</code> value in <code>labels</code> to be shown where the circle is in the provided image.</p>
<p>All help and guidance is appreciated.</p>
"
"<p>I want to calculate the centroid of a figure formed by the points: (0,0), (70,0), (70,25), (45, 45), (45, 180), (95, 188), (95, 200), (-25, 200), (-25,188), (25,180), (25,45), (0, 25), (0,0).</p>
<p>I know that the correct result for the centroid of this polygon is x = 35 and y = 100.4615 (<a href=""https://i.sstatic.net/qjakV.jpg"" rel=""nofollow noreferrer"">source</a>), but the code below does not return the correct values (figure of the polygon below).</p>
<pre><code>import numpy as np

points = np.array([(0,0), (70,0), (70,25), (45,45), (45,180), (95,188), (95,200), (-25,200), (-25, 188), (25,180), (25,45), (0,25), (0,0)])
centroid = np.mean(points, axis=0)
print(&quot;Centroid:&quot;, centroid)
</code></pre>
<p>Output: <code>Centroid: [32.30769231 98.15384615]</code></p>
<p>How can I correctly calculate the centroid of the polygon?</p>
<p><a href=""https://i.sstatic.net/tel6A.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/tel6Am.png"" alt=""enter image description here"" /></a></p>
"
"<p>Using pydantic setting management, how can I load env variables on nested setting objects on a main settings class? In the code below, the <code>sub_field</code> env variable field doesn't get loaded. <code>field_one</code> and <code>field_two</code> load fine. How can I load an environment file so the values are propagated down to the nested <code>sub_settings</code> object?</p>
<pre><code>from typing import Optional
from pydantic import BaseSettings, Field


class SubSettings(BaseSettings):
    sub_field: Optional[str] = Field(None, env='SUB_FIELD')


class Settings(BaseSettings):
    field_one: Optional[str] = Field(None, env='FIELD_ONE')
    field_two: Optional[int] = Field(None, env='FIELD_TWO')
    sub_settings: SubSettings = SubSettings()


settings = Settings(_env_file='local.env')
</code></pre>
"
"<p>A <code>GET</code> endpoint in FastAPI is returning correct result, but returns <code>405 method not allowed</code> when <code>curl -I</code> is used. This is happening with all the <code>GET</code> endpoints. As a result, the application is working, but health check on application from a load balancer is failing.</p>
<p>Any suggestions what could be wrong?</p>
<p><strong>code</strong></p>
<pre><code>@app.get('/health')
async def health():
    &quot;&quot;&quot;
    Returns health status
    &quot;&quot;&quot;
    return JSONResponse({'status': 'ok'})
</code></pre>
<p><strong>result</strong></p>
<pre><code>curl http://172.xx.xx.xx:8080
</code></pre>
<p><a href=""https://i.sstatic.net/YHbiN.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/YHbiN.png"" alt="""" /></a></p>
<p><strong>return header</strong></p>
<pre><code>curl -I http://172.xx.xx.xx:8080
</code></pre>
<p><a href=""https://i.sstatic.net/qFSo0.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/qFSo0.png"" alt="""" /></a></p>
"
"<p>I have previously arranged a Python repository without a <code>src</code> folder, and got it running with:</p>
<pre><code>pdm install --dev
pdm run mymodule
</code></pre>
<p>I am failing to replicate the process in a repository <em>with</em> a <code>src</code> folder. How do I do it?</p>
<p><strong>pyproject.toml</strong></p>
<pre><code>[project]
name = &quot;mymodule&quot;
version = &quot;0.1.0&quot;
description = &quot;Minimal Python repository with a src layout.&quot;
requires-python = &quot;&gt;=3.10&quot;

[build-system]
requires = [&quot;pdm-pep517&gt;=1.0.0&quot;]
build-backend = &quot;pdm.pep517.api&quot;

[project.scripts]
mymodule = &quot;cli:invoke&quot;
</code></pre>
<p><strong>src/mymodule/__init__.py</strong></p>
<p>Empty file.</p>
<p><strong>src/mymodule/cli.py</strong></p>
<pre><code>def invoke():
    print(&quot;Hello world!&quot;)


if __name__ == &quot;__main__&quot;:
    invoke()
</code></pre>
<p>With the configuration above, I can <code>pdm install --dev</code> but <code>pdm run mymodule</code> fails with:</p>
<pre><code>Traceback (most recent call last):
File &quot;/home/user/Documents/mymodule/.venv/bin/mymodule&quot;, line 5, in &lt;module&gt;
    from cli import invoke
ModuleNotFoundError: No module named 'cli'
</code></pre>
"
"<p>Could someone tell me how I can test an endpoint that uses the new <a href=""https://fastapi.tiangolo.com/advanced/events/"" rel=""noreferrer"">lifespan</a> feature from FastAPI?</p>
<p>I am trying to set up tests for my endpoints that use resources from the lifespan function, but the test failed since the dict I set up in the lifespan function is not passed to the TestClient as part of the FastAPI app.</p>
<p>My API looks as follows.</p>
<pre><code>from fastapi import FastAPI
from contextlib import asynccontextmanager

ml_model = {}

@asynccontextmanager
async def lifespan(app: FastAPI):
    predictor = Predictor(model_version)
    ml_model[&quot;predict&quot;] = predictor.predict_from_features
    yield
    # Clean up the ML models and release the resources
    ml_model.clear()


app = FastAPI(lifespan=lifespan)

@app.get(&quot;/prediction/&quot;)
async def get_prediction(model_input: str):
    prediction = ml_model[&quot;predict&quot;](model_input)
    return prediction
</code></pre>
<p>And the test code for the <code>/prediction</code> endpoint looks as follows:</p>
<pre><code>from fastapi.testclient import TestClient

from app.main import app

client = TestClient(app)

def test_read_prediction():
    model_input= &quot;test&quot;
    response = client.get(f&quot;/prediction/?model_input={model_input}&quot;)
    assert response.status_code == 200
</code></pre>
<p>The test failed with an error message saying
<code>KeyError: 'predict'</code>, which shows that the <code>ml_models</code> dict was not passed with the app object. I also tried using  <code>app.state.ml_models = {}</code>, but that didn't work either. I would appreciate any help!</p>
"
"<p>Apologies, am new to Python so a very basic question. Below is an example line of my method definition. Where would I be using capital Dict/List and lowercase Dict/List?</p>
<p>Thanks in advance!</p>
<p>Example scenarios below</p>
<pre><code>def execute_signals(self, parameter1: dict[list], parameter2: dict) -&gt; list[dict]:

def execute_signals(self, parameter1: Dict[List], parameter2: Dict) -&gt; List[Dict]:

def execute_signals(self, parameter1: dict[List], parameter2: dict) -&gt; List[dict]:
</code></pre>
"
"<p>I am trying to load a large Hugging face model with code like below:</p>
<pre><code>model_from_disc = AutoModelForCausalLM.from_pretrained(path_to_model)
tokenizer_from_disc = AutoTokenizer.from_pretrained(path_to_model)
generator = pipeline(&quot;text-generation&quot;, model=model_from_disc, tokenizer=tokenizer_from_disc)
</code></pre>
<p>The program is quickly crashing <strong>after the first line</strong> because it is running out of memory. Is there a way to chunk the model as I am loading it, so that the program doesn't crash?</p>
<hr>
<p><strong>EDIT</strong>
<br>
See cronoik's answer for accepted solution, but here are the relevant pages on Hugging Face's documentation:</p>
<p><strong>Sharded Checkpoints:</strong> <a href=""https://huggingface.co/docs/transformers/big_models#sharded-checkpoints:%7E:text=in%20the%20future.-,Sharded%20checkpoints,-Since%20version%204.18.0"" rel=""noreferrer"">https://huggingface.co/docs/transformers/big_models#sharded-checkpoints:~:text=in%20the%20future.-,Sharded%20checkpoints,-Since%20version%204.18.0</a>
<br>
<strong>Large Model Loading:</strong> <a href=""https://huggingface.co/docs/transformers/main_classes/model#:%7E:text=the%20weights%20instead.-,Large%20model%20loading,-In%20Transformers%204.20.0"" rel=""noreferrer"">https://huggingface.co/docs/transformers/main_classes/model#:~:text=the%20weights%20instead.-,Large%20model%20loading,-In%20Transformers%204.20.0</a></p>
"
"<p>I know it isn't a correct thing to do, but I would like to try to install package that requires Python 3.8, but my installed Python is 3.7.</p>
<p>Is it possible using pip? Or I must clone the repository and change the <code>setup.py</code>?</p>
"
"<p>I am trying to write a middleware in my FastAPI application, so that requests coming to endpoints matching a particular format will be rerouted to a different URL, but I am unable to find a way to do that since <code>request.url</code> is read-only.</p>
<p>I am also looking for a way to update request headers before rerouting.</p>
<p>Are these things even possible in FastAPI?</p>
<p>Redirection is the best I could do so far:</p>
<pre class=""lang-py prettyprint-override""><code>from fastapi import Request
from fastapi.responses import RedirectResponse

@app.middleware(&quot;http&quot;)
async def redirect_middleware(request: Request, call_next):
    if matches_certain_format(request.url.path):
        new_url = create_target_url(request.url.path)
        return RedirectResponse(url=new_url)
</code></pre>
"
"<p>Trying to use the multiprocessing module to run some code. I need to use the python debugger to pass <strong>'-Xfrozen_modules=off'</strong> but using <code>args</code> or <code>pythonArgs</code> in <code>launch.json</code> doesn't seem to work.</p>
<pre class=""lang-none prettyprint-override""><code>0.01s - Debugger warning: It seems that frozen modules are being used, which may
0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off   
0.00s - to python to disable frozen modules.
0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.
</code></pre>
<p>I passed <strong>'-Xfrozen_modules=off'</strong> using <code>args</code> and <code>pythonArgs</code> in <code>launch.json</code> but that didn't work.</p>
<p>Python 3.11.2</p>
"
"<p>I was installing Odoo 15 inside a Python virtual environment on Ubuntu 20.04. I've downloaded Odoo from the official GitHub repository and use Nginx as a reverse proxy.</p>
<p>after following the documentation to install and set up odoo in ubuntu 22.04, I did followed this how-to-do doc in this <a href=""https://linuxize.com/post/how-to-install-odoo-15-on-ubuntu-20-04/"" rel=""noreferrer"">link</a></p>
<p>I get this error the moment I do install pip packages using the command
<code>pip install -r requirement.txt</code>
any help please.</p>
<pre><code>Building wheels for collected packages: python-ldap
  Building wheel for python-ldap (pyproject.toml) ... error
  error: subprocess-exited-with-error
  
  × Building wheel for python-ldap (pyproject.toml) did not run successfully.
  │ exit code: 1
  ╰─&gt; [111 lines of output]
      /tmp/pip-build-env-rbfhnio_/overlay/lib/python3.10/site-packages/setuptools/config/setupcfg.py:516: SetuptoolsDeprecationWarning: The license_file parameter is deprecated, use license_files instead.
        warnings.warn(msg, warning_class)
      running bdist_wheel
      running build
      running build_py
      creating build
      creating build/lib.linux-x86_64-cpython-310
      copying Lib/ldapurl.py -&gt; build/lib.linux-x86_64-cpython-310
      copying Lib/ldif.py -&gt; build/lib.linux-x86_64-cpython-310
      creating build/lib.linux-x86_64-cpython-310/ldap
      copying Lib/ldap/resiter.py -&gt; build/lib.linux-x86_64-cpython-310/ldap
      copying Lib/ldap/syncrepl.py -&gt; build/lib.linux-x86_64-cpython-310/ldap
      copying Lib/ldap/__init__.py -&gt; build/lib.linux-x86_64-cpython-310/ldap
      copying Lib/ldap/dn.py -&gt; build/lib.linux-x86_64-cpython-310/ldap
      copying Lib/ldap/async.py -&gt; build/lib.linux-x86_64-cpython-310/ldap
      copying Lib/ldap/sasl.py -&gt; build/lib.linux-x86_64-cpython-310/ldap
      copying Lib/ldap/asyncsearch.py -&gt; build/lib.linux-x86_64-cpython-310/ldap
      copying Lib/ldap/logger.py -&gt; build/lib.linux-x86_64-cpython-310/ldap
      copying Lib/ldap/functions.py -&gt; build/lib.linux-x86_64-cpython-310/ldap
      copying Lib/ldap/modlist.py -&gt; build/lib.linux-x86_64-cpython-310/ldap
      copying Lib/ldap/constants.py -&gt; build/lib.linux-x86_64-cpython-310/ldap
      copying Lib/ldap/ldapobject.py -&gt; build/lib.linux-x86_64-cpython-310/ldap
      copying Lib/ldap/filter.py -&gt; build/lib.linux-x86_64-cpython-310/ldap
      copying Lib/ldap/pkginfo.py -&gt; build/lib.linux-x86_64-cpython-310/ldap
      copying Lib/ldap/cidict.py -&gt; build/lib.linux-x86_64-cpython-310/ldap
      copying Lib/ldap/compat.py -&gt; build/lib.linux-x86_64-cpython-310/ldap
      creating build/lib.linux-x86_64-cpython-310/ldap/controls
      copying Lib/ldap/controls/psearch.py -&gt; build/lib.linux-x86_64-cpython-310/ldap/controls
      copying Lib/ldap/controls/sessiontrack.py -&gt; build/lib.linux-x86_64-cpython-310/ldap/controls
      copying Lib/ldap/controls/simple.py -&gt; build/lib.linux-x86_64-cpython-310/ldap/controls
      copying Lib/ldap/controls/__init__.py -&gt; build/lib.linux-x86_64-cpython-310/ldap/controls
      copying Lib/ldap/controls/vlv.py -&gt; build/lib.linux-x86_64-cpython-310/ldap/controls
      copying Lib/ldap/controls/sss.py -&gt; build/lib.linux-x86_64-cpython-310/ldap/controls
      copying Lib/ldap/controls/libldap.py -&gt; build/lib.linux-x86_64-cpython-310/ldap/controls
      copying Lib/ldap/controls/pagedresults.py -&gt; build/lib.linux-x86_64-cpython-310/ldap/controls
      copying Lib/ldap/controls/pwdpolicy.py -&gt; build/lib.linux-x86_64-cpython-310/ldap/controls
      copying Lib/ldap/controls/readentry.py -&gt; build/lib.linux-x86_64-cpython-310/ldap/controls
      copying Lib/ldap/controls/deref.py -&gt; build/lib.linux-x86_64-cpython-310/ldap/controls
      copying Lib/ldap/controls/ppolicy.py -&gt; build/lib.linux-x86_64-cpython-310/ldap/controls
      copying Lib/ldap/controls/openldap.py -&gt; build/lib.linux-x86_64-cpython-310/ldap/controls
      creating build/lib.linux-x86_64-cpython-310/ldap/extop
      copying Lib/ldap/extop/passwd.py -&gt; build/lib.linux-x86_64-cpython-310/ldap/extop
      copying Lib/ldap/extop/__init__.py -&gt; build/lib.linux-x86_64-cpython-310/ldap/extop
      copying Lib/ldap/extop/dds.py -&gt; build/lib.linux-x86_64-cpython-310/ldap/extop
      creating build/lib.linux-x86_64-cpython-310/ldap/schema
      copying Lib/ldap/schema/__init__.py -&gt; build/lib.linux-x86_64-cpython-310/ldap/schema
      copying Lib/ldap/schema/subentry.py -&gt; build/lib.linux-x86_64-cpython-310/ldap/schema
      copying Lib/ldap/schema/tokenizer.py -&gt; build/lib.linux-x86_64-cpython-310/ldap/schema
      copying Lib/ldap/schema/models.py -&gt; build/lib.linux-x86_64-cpython-310/ldap/schema
      creating build/lib.linux-x86_64-cpython-310/slapdtest
      copying Lib/slapdtest/__init__.py -&gt; build/lib.linux-x86_64-cpython-310/slapdtest
      copying Lib/slapdtest/_slapdtest.py -&gt; build/lib.linux-x86_64-cpython-310/slapdtest
      running egg_info
      writing Lib/python_ldap.egg-info/PKG-INFO
      writing dependency_links to Lib/python_ldap.egg-info/dependency_links.txt
      writing requirements to Lib/python_ldap.egg-info/requires.txt
      writing top-level names to Lib/python_ldap.egg-info/top_level.txt
      reading manifest file 'Lib/python_ldap.egg-info/SOURCES.txt'
      reading manifest template 'MANIFEST.in'
      no previously-included directories found matching 'Doc/.build'
      adding license file 'LICENCE'
      writing manifest file 'Lib/python_ldap.egg-info/SOURCES.txt'
      /tmp/pip-build-env-rbfhnio_/overlay/lib/python3.10/site-packages/setuptools/command/build_py.py:202: SetuptoolsDeprecationWarning:     Installing 'slapdtest.certs' as data is deprecated, please list it in `packages`.
          !!
      
      
          ############################
          # Package would be ignored #
          ############################
          Python recognizes 'slapdtest.certs' as an importable package,
          but it is not listed in the `packages` configuration of setuptools.
      
          'slapdtest.certs' has been automatically added to the distribution only
          because it may contain data files, but this behavior is likely to change
          in future versions of setuptools (and therefore is considered deprecated).
      
          Please make sure that 'slapdtest.certs' is included as a package by using
          the `packages` configuration field or the proper discovery methods
          (for example by using `find_namespace_packages(...)`/`find_namespace:`
          instead of `find_packages(...)`/`find:`).
      
          You can read more about &quot;package discovery&quot; and &quot;data files&quot; on setuptools
          documentation page.
      
      
      !!
      
        check.warn(importable)
      creating build/lib.linux-x86_64-cpython-310/slapdtest/certs
      copying Lib/slapdtest/certs/README -&gt; build/lib.linux-x86_64-cpython-310/slapdtest/certs
      copying Lib/slapdtest/certs/ca.conf -&gt; build/lib.linux-x86_64-cpython-310/slapdtest/certs
      copying Lib/slapdtest/certs/ca.pem -&gt; build/lib.linux-x86_64-cpython-310/slapdtest/certs
      copying Lib/slapdtest/certs/client.conf -&gt; build/lib.linux-x86_64-cpython-310/slapdtest/certs
      copying Lib/slapdtest/certs/client.key -&gt; build/lib.linux-x86_64-cpython-310/slapdtest/certs
      copying Lib/slapdtest/certs/client.pem -&gt; build/lib.linux-x86_64-cpython-310/slapdtest/certs
      copying Lib/slapdtest/certs/gencerts.sh -&gt; build/lib.linux-x86_64-cpython-310/slapdtest/certs
      copying Lib/slapdtest/certs/gennssdb.sh -&gt; build/lib.linux-x86_64-cpython-310/slapdtest/certs
      copying Lib/slapdtest/certs/server.conf -&gt; build/lib.linux-x86_64-cpython-310/slapdtest/certs
      copying Lib/slapdtest/certs/server.key -&gt; build/lib.linux-x86_64-cpython-310/slapdtest/certs
      copying Lib/slapdtest/certs/server.pem -&gt; build/lib.linux-x86_64-cpython-310/slapdtest/certs
      running build_ext
      building '_ldap' extension
      creating build/temp.linux-x86_64-cpython-310
      creating build/temp.linux-x86_64-cpython-310/Modules
      x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -DHAVE_SASL -DHAVE_TLS -DHAVE_LIBLDAP_R -DHAVE_LIBLDAP_R -DLDAPMODULE_VERSION=3.4.0 &quot;-DLDAPMODULE_AUTHOR=python-ldap project&quot; &quot;-DLDAPMODULE_LICENSE=Python style&quot; -IModules -I/opt/odoo15/odoo-venv/include -I/usr/include/python3.10 -c Modules/LDAPObject.c -o build/temp.linux-x86_64-cpython-310/Modules/LDAPObject.o
      In file included from Modules/LDAPObject.c:3:
      Modules/common.h:15:10: fatal error: lber.h: No such file or directory
         15 | #include &lt;lber.h&gt;
            |          ^~~~~~~~
      compilation terminated.
      error: command '/usr/bin/x86_64-linux-gnu-gcc' failed with exit code 1
      [end of output]
  
  note: This error originates from a subprocess, and is likely not a problem with pip.
  ERROR: Failed building wheel for python-ldap
Failed to build python-ldap
ERROR: Could not build wheels for python-ldap, which is required to install pyproject.toml-based projects
</code></pre>
"
"<p>I have a relatively simple FastAPI app that accepts a query and streams back the response from ChatGPT's API. ChatGPT is streaming back the result and I can see this being printed to console as it comes in.</p>
<p>What's not working is the <code>StreamingResponse</code> back via FastAPI. The response gets sent all together instead. I'm really at a loss as to why this isn't working.</p>
<p>Here is the FastAPI app code:</p>
<pre class=""lang-py prettyprint-override""><code>import os
import time

import openai

import fastapi
from fastapi import Depends, HTTPException, status, Request
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
from fastapi.responses import StreamingResponse

auth_scheme = HTTPBearer()
app = fastapi.FastAPI()

openai.api_key = os.environ[&quot;OPENAI_API_KEY&quot;]


def ask_statesman(query: str):
    #prompt = router(query)
    
    completion_reason = None
    response = &quot;&quot;
    while not completion_reason or completion_reason == &quot;length&quot;:
        openai_stream = openai.ChatCompletion.create(
            model=&quot;gpt-3.5-turbo&quot;,
            messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: query}],
            temperature=0.0,
            stream=True,
        )
        for line in openai_stream:
            completion_reason = line[&quot;choices&quot;][0][&quot;finish_reason&quot;]
            if &quot;content&quot; in line[&quot;choices&quot;][0].delta:
                current_response = line[&quot;choices&quot;][0].delta.content
                print(current_response)
                yield current_response
                time.sleep(0.25)


@app.post(&quot;/&quot;)
async def request_handler(auth_key: str, query: str):
    if auth_key != &quot;123&quot;:
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail=&quot;Invalid authentication credentials&quot;,
            headers={&quot;WWW-Authenticate&quot;: auth_scheme.scheme_name},
        )
    else:
        stream_response = ask_statesman(query)
        return StreamingResponse(stream_response, media_type=&quot;text/plain&quot;)


if __name__ == &quot;__main__&quot;:
    import uvicorn
    uvicorn.run(app, host=&quot;0.0.0.0&quot;, port=8000, debug=True, log_level=&quot;debug&quot;)
</code></pre>
<p>And here is the very simple <code>test.py</code> file to test this:</p>
<pre class=""lang-py prettyprint-override""><code>import requests

query = &quot;How tall is the Eiffel tower?&quot;
url = &quot;http://localhost:8000&quot;
params = {&quot;auth_key&quot;: &quot;123&quot;, &quot;query&quot;: query}

response = requests.post(url, params=params, stream=True)

for chunk in response.iter_lines():
    if chunk:
        print(chunk.decode(&quot;utf-8&quot;))
</code></pre>
"
"<p>I have trained a YOLOv8 object detection model using a custom dataset, and I want to convert it to a Core ML model so that I can use it on iOS.</p>
<p>After exporting the model, I have a converted model to core ml, but I need the coordinates or boxes of the detected objects as output in order to draw rectangular boxes around the detected objects.</p>
<p>As a beginner in this area, I am unsure how to achieve this. Can anyone help me with this problem?</p>
<p>Training model:</p>
<pre><code>!yolo task=detect mode=train model=yolov8s.pt data= data.yaml epochs=25 imgsz=640 plots=True
</code></pre>
<p>Validation:</p>
<pre><code>!yolo task=detect mode=val model=runs/detect/train/weights/best.pt data=data.yaml
</code></pre>
<p>Export this model to coreML:</p>
<pre><code>!yolo mode=export model=runs/detect/train/weights/best.pt format=coreml
</code></pre>
<p>How can I get the co ordinate output?</p>
"
"<p>On <a href=""https://www.adobe.com/acrobat/hub/how-to/how-to-convert-pdf-to-html.html"" rel=""noreferrer"">this site Adobe write about conversion from pdf to html using pdfkit</a></p>
<p>They use <code>pdfkit.from_pdf(...)</code> method.</p>
<blockquote>
<p>This script uses the ‘pdfkit’ library to convert the PDF file to HTML. To use this script, you will need to install the ‘pdfkit’ library...</p>
</blockquote>
<p>When I want to use this method I have error</p>
<pre><code>Traceback (most recent call last):
  File &quot;C:\TestPdfToHtml\script.py&quot;, line 7, in &lt;module&gt;
    html_file = pdfkit.from_pdf(pdf_file, &quot;my_html_file.html&quot;)
                ^^^^^^^^^^^^^^^
AttributeError: module 'pdfkit' has no attribute 'from_pdf'. Did you mean: 'from_url'?
</code></pre>
<p>How can I resolve this problem?</p>
<p>Below is the full script</p>
<pre><code>import pdfkit
# Read the PDF file
pdf_file = open('test2.pdf', 'rb')
# Convert the PDF to HTML
html_file = pdfkit.from_pdf(pdf_file, &quot;my_html_file.html&quot;)
# Close the PDF file
pdf_file.close()
</code></pre>
"
"<p>Let's assume I have a function with many arguments, e.g.</p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd

def f(df: pd.DataFrame, a: int, b: int, c: int, d: int, inplace: bool = True) -&gt; Optional[pd.DataFrame]:
    raise NotImplementedError
</code></pre>
<p>The function will modify the dataframe if <code>inplace=True</code> and return a modified copy if <code>inplace=False</code>.</p>
<p>I know I can do</p>
<pre class=""lang-py prettyprint-override""><code>@overload
def f(df: pd.DataFrame, a: int, b: int, c: int, d: int, inplace: Literal[True] = True) -&gt; None:
    ...

@overload
def f(df: pd.DataFrame, a: int, b: int, c: int, d: int, inplace: Literal[False] = True) -&gt; pd.DataFrame:
    ...
</code></pre>
<p>to inform the typing system about this fact.</p>
<p>However, I'm wondering if there's a way to do this without repeating the entire function definition, which seems cumbersome if there are many arguments. I'm looking for something like</p>
<pre class=""lang-py prettyprint-override""><code>@overload
def f(..., inplace: Literal[True]) -&gt; None:
    ...

@overload
def f(..., inplace: Literal[False]) -&gt; pd.DataFrame:
    ...
</code></pre>
<p><strong>EDIT:</strong> Several persons have made the point that the API design itself may be flawed. While this may be true, in this particular case I am retrospectively adding type hints to an existing library and changing the interface is not an option.</p>
"
"<p>In CPython 3.11, the following code returns very large reference counts for some objects.  It seems to follow pre-cached objects like integers -5 to 256, but CPython 3.10 does not:</p>
<pre class=""lang-py prettyprint-override""><code>Python 3.11.2 (tags/v3.11.2:878ead1, Feb  7 2023, 16:38:35) [MSC v.1934 64 bit (AMD64)] on win32
Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.
&gt;&gt;&gt; import sys
&gt;&gt;&gt; for i in (-6, -5, 0, 255, 256, 257):
...    print(i, sys.getrefcount(i))
...
-6 5
-5 1000000004
0 1000000535
255 1000000010
256 1000000040
257 5
</code></pre>
<pre class=""lang-py prettyprint-override""><code>Python 3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)] on win32
Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.
&gt;&gt;&gt; import sys
&gt;&gt;&gt; for i in (-6, -5, 0, 255, 256, 257):
...    print(i, sys.getrefcount(i))
...
-6 5
-5 6
0 234
255 8
256 26
257 5
</code></pre>
<p><a href=""https://peps.python.org/pep-0683/"" rel=""noreferrer"">PEP 683 - Immortal Objects, Using a Fixed Refcount</a> may be related, but it isn't mentioned in <a href=""https://docs.python.org/3/whatsnew/3.11.html"" rel=""noreferrer"">What's New in Python 3.11</a>, nor is a change in <code>sys.getrefcount()</code> documented.</p>
<p>Anybody have knowledge about this change?</p>
"
"<p>I'm able to use the gpt-3.5-turbo-0301 model to access the ChatGPT API, but not any of the gpt-4 models. Here is the code I am using to test this (it excludes my openai API key). The code runs as written, but when I replace &quot;gpt-3.5-turbo-0301&quot; with &quot;gpt-4&quot;, &quot;gpt-4-0314&quot;, or &quot;gpt-4-32k-0314&quot;, it gives me an error</p>
<pre><code>openai.error.InvalidRequestError: The model: `gpt-4` does not exist
</code></pre>
<p>I have a ChatGPT+ subscription, am using my own API key, and can use gpt-4 successfully via OpenAI's own interface.</p>
<p>It's the same error if I use gpt-4-0314 or gpt-4-32k-0314. I've seen a couple articles claiming this or similar code works using 'gpt-4' works as the model specification, and the code I pasted below is from one of them.</p>
<p>Is it possible to access the gpt-4 model via Python + API, and if so, how?</p>
<pre><code>openai_key = &quot;sk...&quot;
openai.api_key = openai_key
system_intel = &quot;You are GPT-4, answer my questions as if you were an expert in the field.&quot;
prompt = &quot;Write a blog on how to use GPT-4 with python in a jupyter notebook&quot;
# Function that calls the GPT-4 API

def ask_GPT4(system_intel, prompt): 
    result = openai.ChatCompletion.create(model=&quot;gpt-3.5-turbo-0301&quot;,
                                 messages=[{&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: system_intel},
                                           {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: prompt}])
    print(result['choices'][0]['message']['content'])

# Call the function above
ask_GPT4(system_intel, prompt)
</code></pre>
"
"<pre><code>error: Incompatible types in assignment (expression has type &quot;Type[Any]&quot;, variable has type &quot;DefaultMeta&quot;)  [assignment]
error: Variable &quot;application.models.BaseModel&quot; is not valid as a type  [valid-type]
</code></pre>
<p>Previously it worked when it was declared like this</p>
<pre><code>from sqlalchemy.ext.declarative import DeclarativeMeta
BaseModel: DeclarativeMeta = db.Model
</code></pre>
<p>But after packages update I'm seeing that error again. I've read from <a href=""https://stackoverflow.com/questions/56774322/flask-sql-alchemy-vs-mypy-error-with-model-type"">here</a> that DeclarativeMeta was moved to <code>sqlalchemy.orm</code>, but when tried</p>
<pre><code>from sqlalchemy.orm import DeclarativeMeta
BaseModel: DeclarativeMeta = db.Model
</code></pre>
<p>I can still see the error. I've also noticed that sometimes <code>flask_sqlalchemy</code> is used</p>
<pre><code>from flask_sqlalchemy.model import DefaultMeta
BaseModel: DefaultMeta = db.Model
</code></pre>
<p>but that makes no difference I'm still seeing that mypy error.</p>
<p>Current version of packages is this:</p>
<pre><code>mypy                   │ 1.1.1
SQLAlchemy             │ 2.0.6
Flask-SQLAlchemy       │ 3.0.3
</code></pre>
<p>Does somebody know how write this so mypy is happy? I can always make mypy ignore it, but I somehow want to have this right.</p>
"
"<p>I have this code using python 3.11:</p>
<pre class=""lang-py prettyprint-override""><code>import timeit

code_1 = &quot;&quot;&quot;
initial_string = ''
for i in range(10000):
    initial_string = initial_string + 'x' + 'y'
&quot;&quot;&quot;

code_2 = &quot;&quot;&quot;
initial_string = ''
for i in range(10000):
    initial_string += 'x' + 'y'
&quot;&quot;&quot;

time_1 = timeit.timeit(code_1, number=100)
time_2 = timeit.timeit(code_2, number=100)

print(time_1)
# 0.5770808999950532
print(time_2)
# 0.08363639999879524
</code></pre>
<p>Why <code>+=</code> is more efficient <strong>in this case</strong>?
As far as I know, there is the same number of concatenation, and the order of execution doesn't change the result.</p>
<p>Since strings are immutable, it's not because of inplace shinanigans, and the only thing I found about string concat is about <code>.join</code> efficiency, but I don't want the most efficient, just understand why <code>+=</code> seems more efficient than <code>=</code>.</p>
<p>With this code, performances between forms almost equals:</p>
<pre class=""lang-py prettyprint-override""><code>import timeit

code_1 = &quot;&quot;&quot;
initial_string = ''
for i in range(10000):
    initial_string = initial_string + 'x'
&quot;&quot;&quot;

code_2 = &quot;&quot;&quot;
initial_string = ''
for i in range(10000):
    initial_string += 'x'
&quot;&quot;&quot;

time_1 = timeit.timeit(code_1, number=100)
time_2 = timeit.timeit(code_2, number=100)

print(time_1)
# 0.07953230000566691
print(time_2)
# 0.08027460001176223
</code></pre>
<hr />
<p>I noticed a difference using different Python version (<code>'x' + 'y'</code> form):</p>
<p>Python 3.7 to 3.9:</p>
<pre class=""lang-py prettyprint-override""><code>print(time_1)
# ~0.6
print(time_2)
# ~0.3
</code></pre>
<p>Python 3.10:</p>
<pre class=""lang-py prettyprint-override""><code>print(time_1)
# ~1.7
print(time_2)
# ~0.8
</code></pre>
<p>Python 3.11 for comparison:</p>
<pre class=""lang-py prettyprint-override""><code>print(time_1)
# ~0.6
print(time_2)
# ~0.1
</code></pre>
<hr />
<p>Similar but not answering the question: <a href=""https://stackoverflow.com/questions/69079181/how-is-the-s-sc-string-concat-optimization-decided"">How is the s=s+c string concat optimization decided?</a></p>
<blockquote>
<p>If s is a string, then s = s + 'c' might modify the string in place, while t = s + 'c' can't. But how does the operation s + 'c' know which scenario it's in?</p>
</blockquote>
<p>In a nutshell: Optimization occur when <code>s = s + 'c'</code>, not when <code>t = s + 'c'</code> because python need to keep a ref to the first string and can't concatenate in-place.</p>
<p>Here, we are always assigning using simple assignment or augmented assignment to the original string, so in-place concatenation should apply in both cases.</p>
"
"<pre><code>import discord
import openai
import os


openai.api_key = os.environ.get(&quot;OPENAI_API_KEY&quot;)

#Specify the intent
intents = discord.Intents.default()
intents.members = True

#Create Client
client = discord.Client(intents=intents)

async def generate_response(message):
    prompt = f&quot;{message.author.name}: {message.content}\nAI:&quot;
    response = openai.Completion.create(
        engine=&quot;gpt-3.5-turbo&quot;,
        prompt=prompt,
        max_tokens=1024,
        n=1,
        stop=None,
        temperature=0.5,
    )
    return response.choices[0].text.strip()

@client.event
async def on_ready():
    print(f&quot;We have logged in as {client.user}&quot;)
    
@client.event
async def on_message(message):
    if message.author == client.user:
        return

    response = await generate_response(message)
    await message.channel.send(response)

discord_token = 'DiscordToken'


client.start(discord_token)  
</code></pre>
<p>I try to use diferent way to access the API key, including adding to enviroment variables.</p>
<p>What else can I try or where I'm going wrong, pretty new to programming.
Error message:</p>
<blockquote>
<p>openai.error.AuthenticationError: No API key provided. You can set your API key in code using 'openai.api_key = ', or you can set the environment variable OPENAI_API_KEY=). If your API key is stored in a file, you can point the openai module at it with 'openai.api_key_path = '. You can generate API keys in the OpenAI web interface. See https://onboard.openai.com for details, or email support@openai.com if you have any questions.</p>
</blockquote>
<hr />
<p><strong>EDIT</strong></p>
<p>I solved &quot;No API key provided&quot; error. Now I get the following error message:</p>
<blockquote>
<p>openai.error.InvalidRequestError: This is a chat model and not
supported in the v1/completions endpoint. Did you mean to use
v1/chat/completions?</p>
</blockquote>
"
"<p>tldr : Am I right in assuming <code>torch.cuda.init()</code>, <code>device = &quot;cuda&quot;</code> and <code>result = model.transcribe(etc)</code> should be enough to enforce gpu usage ?</p>
<p>I have checked on several forum posts and could not find a solution. Sorry if it's silly. I also posted on the whisper git but maybe it's not whisper-specific.</p>
<p>Here is my python script in a nutshell :</p>
<pre><code>import whisper 
import soundfile as sf
import torch

# specify the path to the input audio file
input_file = &quot;H:\\path\\3minfile.WAV&quot;

# specify the path to the output transcript file
output_file = &quot;H:\\path\\transcript.txt&quot;

# Cuda allows for the GPU to be used which is more optimized than the cpu
torch.cuda.init()
device = &quot;cuda&quot; # if torch.cuda.is_available() else &quot;cpu&quot;

# Load audio file
audio_data, sample_rate = sf.read(input_file, always_2d=True)

#load whisper model
model_size = &quot;tiny&quot;
print(&quot;loading model :&quot;, model_size)
model = whisper.load_model(model_size).to(device)
print(model_size, &quot;model loaded&quot;)

# Initialize variables
results = []
language = &quot;fr&quot;

# Transcribe audio
with torch.cuda.device(device):
    result = model.transcribe(audio_data, language=language, fp16=False, word_timestamps=True)
</code></pre>
<p>However, it is returning the following error on the last line, hinting that it's trying to run it on cpu :</p>
<blockquote>
<p>RuntimeError: [enforce fail at C:\actions-runner_work\pytorch\pytorch\builder\windows\pytorch\c10\core\impl\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 30623038517864 bytes.</p>
</blockquote>
<p>I am using Jupyter, and i checked that the pytorch version it's using was the cuda/gpu one and not a cpu-locked version :
<code>print(torch.__version__)</code></p>
<blockquote>
<p>2.0.0+cu117</p>
</blockquote>
<p>So I really don't get it. Could there be a conflict of pythorch libraries ? Am I doing something wrong ? Is the transcribe() function indeed using cpu instead of gpu ?</p>
<p>I am using anaconda3, here is what <code>conda list</code> returns, in case it helps :</p>
conda list 
<p>
<pre><code># Name                    Version                   Build  Channel
_ipyw_jlab_nb_ext_conf    0.1.0            py39haa95532_0
alabaster                 0.7.12             pyhd3eb1b0_0
anaconda                  2022.10                  py39_0
anaconda-client           1.11.0           py39haa95532_0
anaconda-navigator        2.3.1            py39haa95532_0
anaconda-project          0.11.1           py39haa95532_0
anyio                     3.5.0            py39haa95532_0
appdirs                   1.4.4              pyhd3eb1b0_0
argon2-cffi               21.3.0             pyhd3eb1b0_0
argon2-cffi-bindings      21.2.0           py39h2bbff1b_0
arrow                     1.2.2              pyhd3eb1b0_0
astroid                   2.11.7           py39haa95532_0
astropy                   5.1              py39h080aedc_0
atomicwrites              1.4.0                      py_0
attrs                     21.4.0             pyhd3eb1b0_0
automat                   20.2.0                     py_0
autopep8                  1.6.0              pyhd3eb1b0_1
babel                     2.9.1              pyhd3eb1b0_0
backcall                  0.2.0              pyhd3eb1b0_0
backports                 1.1                pyhd3eb1b0_0
backports.functools_lru_cache 1.6.4              pyhd3eb1b0_0
backports.tempfile        1.0                pyhd3eb1b0_1
backports.weakref         1.0.post1                  py_1
bcrypt                    3.2.0            py39h2bbff1b_1
beautifulsoup4            4.11.1           py39haa95532_0
binaryornot               0.4.4              pyhd3eb1b0_1
bitarray                  2.5.1            py39h2bbff1b_0
bkcharts                  0.2              py39haa95532_1
black                     22.6.0           py39haa95532_0
blas                      1.0                         mkl
bleach                    4.1.0              pyhd3eb1b0_0
blosc                     1.21.0               h19a0ad4_1
bokeh                     2.4.3            py39haa95532_0
boto3                     1.24.28          py39haa95532_0
botocore                  1.27.28          py39haa95532_0
bottleneck                1.3.5            py39h080aedc_0
brotli                    1.0.9                h2bbff1b_7
brotli-bin                1.0.9                h2bbff1b_7
brotlipy                  0.7.0           py39h2bbff1b_1003
bzip2                     1.0.8                he774522_0
ca-certificates           2022.07.19           haa95532_0
certifi                   2022.9.14        py39haa95532_0
cffi                      1.15.1           py39h2bbff1b_0
cfitsio                   3.470                h2bbff1b_7
chardet                   4.0.0           py39haa95532_1003
charls                    2.2.0                h6c2663c_0
charset-normalizer        2.0.4              pyhd3eb1b0_0
click                     8.0.4            py39haa95532_0
cloudpickle               2.0.0              pyhd3eb1b0_0
clyent                    1.2.2            py39haa95532_1
colorama                  0.4.5            py39haa95532_0
colorcet                  3.0.0            py39haa95532_0
comtypes                  1.1.10          py39haa95532_1002
conda                     23.1.0           py39haa95532_0
conda-build               3.22.0           py39haa95532_0
conda-content-trust       0.1.3            py39haa95532_0
conda-env                 2.6.0                haa95532_1
conda-pack                0.6.0              pyhd3eb1b0_0
conda-package-handling    2.0.2            py39haa95532_0
conda-package-streaming   0.7.0            py39haa95532_0
conda-repo-cli            1.0.27           py39haa95532_0
conda-token               0.4.0              pyhd3eb1b0_0
conda-verify              3.4.2                      py_1
console_shortcut          0.1.1                         4
constantly                15.1.0             pyh2b92418_0
cookiecutter              1.7.3              pyhd3eb1b0_0
cryptography              37.0.1           py39h21b164f_0
cssselect                 1.1.0              pyhd3eb1b0_0
cuda-cccl                 12.1.55                       0    nvidia
cuda-cudart               11.8.89                       0    nvidia
cuda-cudart-dev           11.8.89                       0    nvidia
cuda-cupti                11.8.87                       0    nvidia
cuda-libraries            11.8.0                        0    nvidia
cuda-libraries-dev        11.8.0                        0    nvidia
cuda-nvrtc                11.8.89                       0    nvidia
cuda-nvrtc-dev            11.8.89                       0    nvidia
cuda-nvtx                 11.8.86                       0    nvidia
cuda-profiler-api         12.1.55                       0    nvidia
cuda-runtime              11.8.0                        0    nvidia
cudatoolkit               10.1.243             h74a9793_0
curl                      7.84.0               h2bbff1b_0
cycler                    0.11.0             pyhd3eb1b0_0
cython                    0.29.32          py39hd77b12b_0
cytoolz                   0.11.0           py39h2bbff1b_0
daal4py                   2021.6.0         py39h757b272_1
dal                       2021.6.0           h59b6b97_874
dask                      2022.7.0         py39haa95532_0
dask-core                 2022.7.0         py39haa95532_0
dataclasses               0.8                pyh6d0b6a4_7
datashader                0.14.1           py39haa95532_0
datashape                 0.5.4            py39haa95532_1
debugpy                   1.5.1            py39hd77b12b_0
decorator                 5.1.1              pyhd3eb1b0_0
defusedxml                0.7.1              pyhd3eb1b0_0
diff-match-patch          20200713           pyhd3eb1b0_0
dill                      0.3.4              pyhd3eb1b0_0
distributed               2022.7.0         py39haa95532_0
docutils                  0.18.1           py39haa95532_3
entrypoints               0.4              py39haa95532_0
et_xmlfile                1.1.0            py39haa95532_0
ffmpeg                    1.4                      pypi_0    pypi
ffmpeg-python             0.2.0                    pypi_0    pypi
fftw                      3.3.9                h2bbff1b_1
filelock                  3.6.0              pyhd3eb1b0_0
flake8                    4.0.1              pyhd3eb1b0_1
flask                     1.1.2              pyhd3eb1b0_0
fonttools                 4.25.0             pyhd3eb1b0_0
freetype                  2.10.4               hd328e21_0
fsspec                    2022.7.1         py39haa95532_0
future                    0.18.2           py39haa95532_1
gensim                    4.1.2            py39hd77b12b_0
giflib                    5.2.1                h62dcd97_0
glob2                     0.7                pyhd3eb1b0_0
greenlet                  1.1.1            py39hd77b12b_0
h5py                      3.7.0            py39h3de5c98_0
hdf5                      1.10.6               h1756f20_1
heapdict                  1.0.1              pyhd3eb1b0_0
holoviews                 1.15.0           py39haa95532_0
hvplot                    0.8.0            py39haa95532_0
hyperlink                 21.0.0             pyhd3eb1b0_0
icc_rt                    2022.1.0             h6049295_2
icu                       58.2                 ha925a31_3
idna                      3.3                pyhd3eb1b0_0
imagecodecs               2021.8.26        py39hc0a7faf_1
imageio                   2.19.3           py39haa95532_0
imagesize                 1.4.1            py39haa95532_0
importlib-metadata        4.11.3           py39haa95532_0
importlib_metadata        4.11.3               hd3eb1b0_0
incremental               21.3.0             pyhd3eb1b0_0
inflection                0.5.1            py39haa95532_0
iniconfig                 1.1.1              pyhd3eb1b0_0
intake                    0.6.5              pyhd3eb1b0_0
intel-openmp              2021.4.0          haa95532_3556
intervaltree              3.1.0              pyhd3eb1b0_0
ipykernel                 6.15.2           py39haa95532_0
ipython                   7.31.1           py39haa95532_1
ipython_genutils          0.2.0              pyhd3eb1b0_1
ipywidgets                7.6.5              pyhd3eb1b0_1
isort                     5.9.3              pyhd3eb1b0_0
itemadapter               0.3.0              pyhd3eb1b0_0
itemloaders               1.0.4              pyhd3eb1b0_1
itsdangerous              2.0.1              pyhd3eb1b0_0
jdcal                     1.4.1              pyhd3eb1b0_0
jedi                      0.18.1           py39haa95532_1
jellyfish                 0.9.0            py39h2bbff1b_0
jinja2                    2.11.3             pyhd3eb1b0_0
jinja2-time               0.2.0              pyhd3eb1b0_3
jmespath                  0.10.0             pyhd3eb1b0_0
joblib                    1.1.0              pyhd3eb1b0_0
jpeg                      9e                   h2bbff1b_0
jq                        1.6                  haa95532_1
json5                     0.9.6              pyhd3eb1b0_0
jsonschema                4.16.0           py39haa95532_0
jupyter                   1.0.0            py39haa95532_8
jupyter_client            7.3.4            py39haa95532_0
jupyter_console           6.4.3              pyhd3eb1b0_0
jupyter_core              4.11.1           py39haa95532_0
jupyter_server            1.18.1           py39haa95532_0
jupyterlab                3.4.4            py39haa95532_0
jupyterlab_pygments       0.1.2                      py_0
jupyterlab_server         2.10.3             pyhd3eb1b0_1
jupyterlab_widgets        1.0.0              pyhd3eb1b0_1
keyring                   23.4.0           py39haa95532_0
kiwisolver                1.4.2            py39hd77b12b_0
lazy-object-proxy         1.6.0            py39h2bbff1b_0
lcms2                     2.12                 h83e58a3_0
lerc                      3.0                  hd77b12b_0
libaec                    1.0.4                h33f27b4_1
libarchive                3.6.1                hebabd0d_0
libbrotlicommon           1.0.9                h2bbff1b_7
libbrotlidec              1.0.9                h2bbff1b_7
libbrotlienc              1.0.9                h2bbff1b_7
libcublas                 11.11.3.6                     0    nvidia
libcublas-dev             11.11.3.6                     0    nvidia
libcufft                  10.9.0.58                     0    nvidia
libcufft-dev              10.9.0.58                     0    nvidia
libcurand                 10.3.2.56                     0    nvidia
libcurand-dev             10.3.2.56                     0    nvidia
libcurl                   7.84.0               h86230a5_0
libcusolver               11.4.1.48                     0    nvidia
libcusolver-dev           11.4.1.48                     0    nvidia
libcusparse               11.7.5.86                     0    nvidia
libcusparse-dev           11.7.5.86                     0    nvidia
libdeflate                1.8                  h2bbff1b_5
libiconv                  1.16                 h2bbff1b_2
liblief                   0.11.5               hd77b12b_1
libnpp                    11.8.0.86                     0    nvidia
libnpp-dev                11.8.0.86                     0    nvidia
libnvjpeg                 11.9.0.86                     0    nvidia
libnvjpeg-dev             11.9.0.86                     0    nvidia
libpng                    1.6.37               h2a8f88b_0
libsodium                 1.0.18               h62dcd97_0
libspatialindex           1.9.3                h6c2663c_0
libssh2                   1.10.0               hcd4344a_0
libtiff                   4.4.0                h8a3f274_0
libuv                     1.44.2               h2bbff1b_0
libwebp                   1.2.2                h2bbff1b_0
libxml2                   2.9.14               h0ad7f3c_0
libxslt                   1.1.35               h2bbff1b_0
libzopfli                 1.0.3                ha925a31_0
llvmlite                  0.38.0           py39h23ce68f_0
locket                    1.0.0            py39haa95532_0
lxml                      4.9.1            py39h1985fb9_0
lz4                       3.1.3            py39h2bbff1b_0
lz4-c                     1.9.3                h2bbff1b_1
lzo                       2.10                 he774522_2
m2-msys2-runtime          2.5.0.17080.65c939c               3
m2-patch                  2.7.5                         2
m2w64-libwinpthread-git   5.0.0.4634.697f757               2
markdown                  3.3.4            py39haa95532_0
markupsafe                2.0.1            py39h2bbff1b_0
matplotlib                3.5.2            py39haa95532_0
matplotlib-base           3.5.2            py39hd77b12b_0
matplotlib-inline         0.1.6            py39haa95532_0
mccabe                    0.6.1            py39haa95532_2
menuinst                  1.4.19           py39h59b6b97_0
mistune                   0.8.4           py39h2bbff1b_1000
mkl                       2021.4.0           haa95532_640
mkl-service               2.4.0            py39h2bbff1b_0
mkl_fft                   1.3.1            py39h277e83a_0
mkl_random                1.2.2            py39hf11a4ad_0
mock                      4.0.3              pyhd3eb1b0_0
more-itertools            9.1.0                    pypi_0    pypi
mpmath                    1.2.1            py39haa95532_0
msgpack-python            1.0.3            py39h59b6b97_0
msys2-conda-epoch         20160418                      1
multipledispatch          0.6.0            py39haa95532_0
munkres                   1.1.4                      py_0
mypy_extensions           0.4.3            py39haa95532_1
navigator-updater         0.3.0            py39haa95532_0
nbclassic                 0.3.5              pyhd3eb1b0_0
nbclient                  0.5.13           py39haa95532_0
nbconvert                 6.4.4            py39haa95532_0
nbformat                  5.5.0            py39haa95532_0
nest-asyncio              1.5.5            py39haa95532_0
networkx                  2.8.4            py39haa95532_0
nltk                      3.7                pyhd3eb1b0_0
nose                      1.3.7           pyhd3eb1b0_1008
notebook                  6.4.12           py39haa95532_0
numba                     0.55.1           py39hf11a4ad_0
numexpr                   2.8.3            py39hb80d3ca_0
numpy                     1.21.5           py39h7a0a035_3
numpy-base                1.21.5           py39hca35cd5_3
numpydoc                  1.4.0            py39haa95532_0
olefile                   0.46               pyhd3eb1b0_0
openai-whisper            20230314                 pypi_0    pypi
openjpeg                  2.4.0                h4fc8c34_0
openpyxl                  3.0.10           py39h2bbff1b_0
openssl                   1.1.1q               h2bbff1b_0
packaging                 21.3               pyhd3eb1b0_0
pandas                    1.4.4            py39hd77b12b_0
pandocfilters             1.5.0              pyhd3eb1b0_0
panel                     0.13.1           py39haa95532_0
param                     1.12.0             pyhd3eb1b0_0
paramiko                  2.8.1              pyhd3eb1b0_0
parsel                    1.6.0            py39haa95532_0
parso                     0.8.3              pyhd3eb1b0_0
partd                     1.2.0              pyhd3eb1b0_1
pathlib                   1.0.1              pyhd3eb1b0_1
pathspec                  0.9.0            py39haa95532_0
patsy                     0.5.2            py39haa95532_1
pep8                      1.7.1            py39haa95532_1
pexpect                   4.8.0              pyhd3eb1b0_3
pickleshare               0.7.5           pyhd3eb1b0_1003
pillow                    9.2.0            py39hdc2b20a_1
pip                       22.2.2           py39haa95532_0
pkginfo                   1.8.2              pyhd3eb1b0_0
platformdirs              2.5.2            py39haa95532_0
plotly                    5.9.0            py39haa95532_0
pluggy                    1.0.0            py39haa95532_1
powershell_shortcut       0.0.1                         3
poyo                      0.5.0              pyhd3eb1b0_0
prometheus_client         0.14.1           py39haa95532_0
prompt-toolkit            3.0.20             pyhd3eb1b0_0
prompt_toolkit            3.0.20               hd3eb1b0_0
protego                   0.1.16                     py_0
psutil                    5.9.0            py39h2bbff1b_0
ptyprocess                0.7.0              pyhd3eb1b0_2
py                        1.11.0             pyhd3eb1b0_0
py-lief                   0.11.5           py39hd77b12b_1
pyasn1                    0.4.8              pyhd3eb1b0_0
pyasn1-modules            0.2.8                      py_0
pycodestyle               2.8.0              pyhd3eb1b0_0
pycosat                   0.6.3            py39h2bbff1b_0
pycparser                 2.21               pyhd3eb1b0_0
pyct                      0.4.8            py39haa95532_1
pycurl                    7.45.1           py39hcd4344a_0
pydispatcher              2.0.5            py39haa95532_2
pydocstyle                6.1.1              pyhd3eb1b0_0
pyerfa                    2.0.0            py39h2bbff1b_0
pyflakes                  2.4.0              pyhd3eb1b0_0
pygments                  2.11.2             pyhd3eb1b0_0
pyhamcrest                2.0.2              pyhd3eb1b0_2
pyjwt                     2.4.0            py39haa95532_0
pylint                    2.14.5           py39haa95532_0
pyls-spyder               0.4.0              pyhd3eb1b0_0
pynacl                    1.5.0            py39h8cc25b3_0
pyodbc                    4.0.34           py39hd77b12b_0
pyopenssl                 22.0.0             pyhd3eb1b0_0
pyparsing                 3.0.9            py39haa95532_0
pyqt                      5.9.2            py39hd77b12b_6
pyrsistent                0.18.0           py39h196d8e1_0
pysocks                   1.7.1            py39haa95532_0
pytables                  3.6.1            py39h56d22b6_1
pytest                    7.1.2            py39haa95532_0
python                    3.9.13               h6244533_1
python-dateutil           2.8.2              pyhd3eb1b0_0
python-fastjsonschema     2.16.2           py39haa95532_0
python-libarchive-c       2.9                pyhd3eb1b0_1
python-lsp-black          1.0.0              pyhd3eb1b0_0
python-lsp-jsonrpc        1.0.0              pyhd3eb1b0_0
python-lsp-server         1.3.3              pyhd3eb1b0_0
python-slugify            5.0.2              pyhd3eb1b0_0
python-snappy             0.6.0            py39hd77b12b_3
pytorch                   2.0.0           py3.9_cuda11.8_cudnn8_0    pytorch
pytorch-cuda              11.8                 h24eeafa_3    pytorch
pytorch-mutex             1.0                        cuda    pytorch
pytz                      2022.1           py39haa95532_0
pyviz_comms               2.0.2              pyhd3eb1b0_0
pywavelets                1.3.0            py39h2bbff1b_0
pywin32                   302              py39h2bbff1b_2
pywin32-ctypes            0.2.0           py39haa95532_1000
pywinpty                  2.0.2            py39h5da7b33_0
pyyaml                    6.0              py39h2bbff1b_1
pyzmq                     23.2.0           py39hd77b12b_0
qdarkstyle                3.0.2              pyhd3eb1b0_0
qstylizer                 0.1.10             pyhd3eb1b0_0
qt                        5.9.7            vc14h73c81de_0
qtawesome                 1.0.3              pyhd3eb1b0_0
qtconsole                 5.2.2              pyhd3eb1b0_0
qtpy                      2.2.0            py39haa95532_0
queuelib                  1.5.0            py39haa95532_0
regex                     2022.7.9         py39h2bbff1b_0
requests                  2.28.1           py39haa95532_0
requests-file             1.5.1              pyhd3eb1b0_0
rope                      0.22.0             pyhd3eb1b0_0
rtree                     0.9.7            py39h2eaa2aa_1
ruamel.yaml               0.17.21          py39h2bbff1b_0
ruamel.yaml.clib          0.2.6            py39h2bbff1b_1
ruamel_yaml               0.15.100         py39h2bbff1b_0
s3transfer                0.6.0            py39haa95532_0
scikit-image              0.19.2           py39hf11a4ad_0
scikit-learn              1.0.2            py39hf11a4ad_1
scikit-learn-intelex      2021.6.0         py39haa95532_0
scipy                     1.9.1            py39he11b74f_0
scrapy                    2.6.2            py39haa95532_0
seaborn                   0.11.2             pyhd3eb1b0_0
send2trash                1.8.0              pyhd3eb1b0_1
service_identity          18.1.0             pyhd3eb1b0_1
setuptools                63.4.1           py39haa95532_0
sip                       4.19.13          py39hd77b12b_0
six                       1.16.0             pyhd3eb1b0_1
smart_open                5.2.1            py39haa95532_0
snappy                    1.1.9                h6c2663c_0
sniffio                   1.2.0            py39haa95532_1
snowballstemmer           2.2.0              pyhd3eb1b0_0
sortedcollections         2.1.0              pyhd3eb1b0_0
sortedcontainers          2.4.0              pyhd3eb1b0_0
soundfile                 0.12.1                   pypi_0    pypi
soupsieve                 2.3.1              pyhd3eb1b0_0
sphinx                    5.0.2            py39haa95532_0
sphinxcontrib-applehelp   1.0.2              pyhd3eb1b0_0
sphinxcontrib-devhelp     1.0.2              pyhd3eb1b0_0
sphinxcontrib-htmlhelp    2.0.0              pyhd3eb1b0_0
sphinxcontrib-jsmath      1.0.1              pyhd3eb1b0_0
sphinxcontrib-qthelp      1.0.3              pyhd3eb1b0_0
sphinxcontrib-serializinghtml 1.1.5              pyhd3eb1b0_0
spyder                    5.2.2            py39haa95532_1
spyder-kernels            2.2.1            py39haa95532_0
sqlalchemy                1.4.39           py39h2bbff1b_0
sqlite                    3.39.3               h2bbff1b_0
statsmodels               0.13.2           py39h2bbff1b_0
sympy                     1.10.1           py39haa95532_0
tabulate                  0.8.10           py39haa95532_0
tbb                       2021.6.0             h59b6b97_0
tbb4py                    2021.6.0         py39h59b6b97_0
tblib                     1.7.0              pyhd3eb1b0_0
tenacity                  8.0.1            py39haa95532_1
terminado                 0.13.1           py39haa95532_0
testpath                  0.6.0            py39haa95532_0
text-unidecode            1.3                pyhd3eb1b0_0
textdistance              4.2.1              pyhd3eb1b0_0
threadpoolctl             2.2.0              pyh0d69192_0
three-merge               0.1.1              pyhd3eb1b0_0
tifffile                  2021.7.2           pyhd3eb1b0_2
tiktoken                  0.3.1                    pypi_0    pypi
tinycss                   0.4             pyhd3eb1b0_1002
tk                        8.6.12               h2bbff1b_0
tldextract                3.2.0              pyhd3eb1b0_0
toml                      0.10.2             pyhd3eb1b0_0
tomli                     2.0.1            py39haa95532_0
tomlkit                   0.11.1           py39haa95532_0
toolz                     0.11.2             pyhd3eb1b0_0
torch                     2.0.0+cu117              pypi_0    pypi
torchaudio                2.0.1+cu117              pypi_0    pypi
torchvision               0.15.1+cu117             pypi_0    pypi
tornado                   6.1              py39h2bbff1b_0
tqdm                      4.64.1           py39haa95532_0
traitlets                 5.1.1              pyhd3eb1b0_0
twisted                   22.2.0           py39h2bbff1b_1
twisted-iocpsupport       1.0.2            py39h2bbff1b_0
typing-extensions         4.3.0            py39haa95532_0
typing_extensions         4.3.0            py39haa95532_0
tzdata                    2022c                h04d1e81_0
ujson                     5.4.0            py39hd77b12b_0
unidecode                 1.2.0              pyhd3eb1b0_0
urllib3                   1.26.11          py39haa95532_0
vc                        14.2                 h21ff451_1
vs2015_runtime            14.27.29016          h5e58377_2
w3lib                     1.21.0             pyhd3eb1b0_0
watchdog                  2.1.6            py39haa95532_0
wcwidth                   0.2.5              pyhd3eb1b0_0
webencodings              0.5.1            py39haa95532_1
websocket-client          0.58.0           py39haa95532_4
werkzeug                  2.0.3              pyhd3eb1b0_0
wheel                     0.37.1             pyhd3eb1b0_0
widgetsnbextension        3.5.2            py39haa95532_0
win_inet_pton             1.1.0            py39haa95532_0
win_unicode_console       0.5              py39haa95532_0
wincertstore              0.2              py39haa95532_2
winpty                    0.4.3                         4
wrapt                     1.14.1           py39h2bbff1b_0
xarray                    0.20.1             pyhd3eb1b0_1
xlrd                      2.0.1              pyhd3eb1b0_0
xlsxwriter                3.0.3              pyhd3eb1b0_0
xlwings                   0.27.15          py39haa95532_0
xz                        5.2.6                h8cc25b3_0
yaml                      0.2.5                he774522_0
yapf                      0.31.0             pyhd3eb1b0_0
zeromq                    4.3.4                hd77b12b_0
zfp                       0.5.5                hd77b12b_6
zict                      2.1.0            py39haa95532_0
zipp                      3.8.0            py39haa95532_0
zlib                      1.2.12               h8cc25b3_3
zope                      1.0              py39haa95532_1
zope.interface            5.4.0            py39h2bbff1b_0
zstandard                 0.19.0           py39h2bbff1b_0
zstd                      1.5.2                h19a0ad4_0
</code></pre>
</p>

"
"<p>I keep get an error as below</p>
<p><code>Request timed out: HTTPSConnectionPool(host='api.openai.com', port=443): Read timed out. (read timeout=600)</code></p>
<p>when I run the code below</p>
<pre><code>def generate_gpt3_response(user_text, print_output=False):
    &quot;&quot;&quot;
    Query OpenAI GPT-3 for the specific key and get back a response
    :type user_text: str the user's text to query for
    :type print_output: boolean whether or not to print the raw output JSON
    &quot;&quot;&quot;
    time.sleep(5)
    completions = ai.Completion.create(
        engine='text-davinci-003',  # Determines the quality, speed, and cost.
        temperature=0.5,            # Level of creativity in the response
        prompt=user_text,           # What the user typed in
        max_tokens=150,             # Maximum tokens in the prompt AND response
        n=1,                        # The number of completions to generate
        stop=None,                  # An optional setting to control response generation
    )

    # Displaying the output can be helpful if things go wrong
    if print_output:
        print(completions)

    # Return the first choice's text
    return completions.choices[0].text
</code></pre>
<pre><code>df_test['GPT'] = df_test['Q20'].apply(lambda x: \
              generate_gpt3_response\
              (&quot;I am giving you the answer of respondents \
                in the format [Q20], \
                give me the Broader topics like customer service, technology, satisfaction\
                or the related high level topics in one word in the \
                format[Topic: your primary topic] for the text '{}' &quot;.format(x)))

# result
df_test['GPT'] = df_test['GPT'].apply(lambda x: (x.split(':')[1]).replace(']',''))
</code></pre>
<p>I tried modifiying the parameters, but the error still occurs.</p>
<p>Anyone experienced the same process?</p>
<p>Thanks in advance.</p>
"
"<p>In my python code, I import func...</p>
<pre><code>from sqlalchemy.sql.expression import func
</code></pre>
<p>Then, during my code I select data from a database table...</p>
<pre><code>select(func.max(MyTable.my_datetime))
</code></pre>
<p>...where my_datetime is a DateTime data type...</p>
<pre><code>from sqlalchemy.types import DateTime

my_datetime = Column('my_datetime', DateTime)
</code></pre>
<p>The code runs OK, but in the vscode editor I am getting the following error...</p>
<p><strong>func.max is not callable Pylint(E1102:not-callable)</strong></p>
<p>I don't want to ignore this if there is a genuine concern behind this Pylint error.</p>
<p>Should I be concerned by this error or can I safely ignore it?</p>
"
"<p>I want to segment a video transcript into chapters based on the content of each line of speech. The transcript would be used to generate a series of start and end timestamps for each chapter. This is similar to how YouTube now &quot;auto-chapters&quot; videos.</p>
<p>Example .srt transcript:</p>
<pre><code>...

70
00:02:53,640 --&gt; 00:02:54,760
All right, coming in at number five,

71
00:02:54,760 --&gt; 00:02:57,640
we have another habit that saves me around 15 minutes a day
...
</code></pre>
<p>I have had minimal luck doing this with ChatGPT as it finds it difficult to both segment by topic and recollect start and end timestamps accurately. I am now exploring whether there are other options for doing this.</p>
<p>I know topic modeling based on time series is possible with some python libraries. I have also read about text tiling as another option. <strong>What options are there for achieving an outcome like this?</strong></p>
<p>Note: The format above (.srt) is not necessary. It's just the idea that the input is a list of text-content with start and end timestamps.</p>
"
"<p>I came across one interactive problem in Codeforces. I want to know how the grader or interactor (as per Codeforces' terms) might be designed.</p>
<p>Let's say I want to create a grader for this problem: <a href=""https://codeforces.com/gym/101021/problem/1"" rel=""noreferrer"">1. Guess the Number</a>.</p>
<p>My solution to the above problem is stored in <code>1_Guess_the_Number.py</code> file. It is a correct solution and is accepted by the CF grader.</p>
<pre class=""lang-py prettyprint-override""><code>#!/usr/bin/env python3

l, r = 1, 1000000
while l != r:
    mid = (l + r + 1) // 2
    print(mid, flush=True)
    response = input()
    if response == &quot;&lt;&quot;:
        r = mid - 1
    else:
        l = mid

print(&quot;!&quot;, l)
</code></pre>
<p>I created the following <code>grader.py</code> file:</p>
<pre class=""lang-py prettyprint-override""><code>#!/usr/bin/env python3

import sys

INP = 12


def interactor(n):
    if n &gt; INP:
        return &quot;&lt;&quot;
    return &quot;&gt;=&quot;


while True:
    guess = input()
    if guess.startswith(&quot;!&quot;):
        print(int(guess.split()[1]) == INP, flush=True)
        sys.exit()
    print(interactor(int(guess)), flush=True)
</code></pre>
<p>So, when I run <code>./1_Guess_the_Number.py | ./grader_1.py</code>, I expect it to work correctly. But in the terminal, the above command runs for an infinite time with only the following output:</p>
<pre class=""lang-bash prettyprint-override""><code>&lt;
</code></pre>
<p>I don't know what is going wrong. Also, it will be very helpful if someone can provide any other way.</p>
"
"<p>You can install the python package <a href=""https://github.com/google/jax/blob/main/setup.py"" rel=""noreferrer"">Jax</a> with some extra packages depending on your environment.</p>
<p>For GPU:</p>
<pre><code>pip install jax[cuda] --find-links https://storage.googleapis.com/jax-releases/jax_cuda_releases.html
</code></pre>
<p>For TPU:</p>
<pre><code>pip install jax[tpu]  --find-links https://storage.googleapis.com/jax-releases/libtpu_releases.html
</code></pre>
<p>How do I add those <code>--find-links</code> URLs to the following <code>pyproject.toml</code>?</p>
<pre class=""lang-ini prettyprint-override""><code>[build-system]
requires = [&quot;setuptools&gt;=67.6.0&quot;]
build-backend = &quot;setuptools.build_meta&quot;

[project]
name = &quot;minimal_example&quot;
version = '0.0.1'
requires-python = &quot;&gt;=3.9&quot;

dependencies = [
    &quot;seqio-nightly[gcp,cache-tasks]&quot;,
    &quot;t5[gcp]&quot;,
    &quot;t5x @ git+https://github.com/google-research/t5x.git&quot;
]

[project.optional-dependencies]
cpu = [&quot;jax[cpu]&quot;]
gpu = [&quot;jax[cuda]&quot; , &quot;t5x[gpu] @ git+https://github.com/google-research/t5x.git&quot;]
tpu = [&quot;jax[tpu]&quot;, &quot;t5x[tpu] @ git+https://github.com/google-research/t5x.git&quot;]
dev = [&quot;pytest&quot;, &quot;mkdocs&quot;]
</code></pre>
<p>If I do <code>pip install -e .</code> then I get a working install.
But doing a <code>pip install -e &quot;.[gpu]</code> gives me a <code>ResolutionImpossible</code> error.
And doing <code>pip install -e &quot;.[tpu]</code> gives me:</p>
<pre><code>Packages installed from PyPI cannot depend on packages which are not also hosted on PyPI.
jax depends on libtpu-nightly@ https://storage.googleapis.com/cloud-tpu-tpuvm-artifacts/wheels/libtpu-nightly/libtpu_nightly-0.1.dev20210615-py3-none-any.whl 
</code></pre>
"
"<p>OpenAI's text models have a context length, e.g.: Curie has a context length of 2049 tokens.</p>
<p>They provide <code>max_tokens</code> and <code>stop</code> parameters to control the length of the generated sequence. Therefore the generation stops either when stop token is obtained, or <code>max_tokens</code> is reached.</p>
<p>The issue is: when generating a text, I don't know how many tokens my prompt contains. Since I do not know that, I cannot set <code>max_tokens = 2049 - number_tokens_in_prompt</code>.</p>
<p>This prevents me from generating text dynamically for a wide range of text in terms of their length. What I need is to continue generating until the stop token.</p>
<p>My questions are:</p>
<ul>
<li>How can I count the number of tokens in Python API so that I will set <code>max_tokens</code> parameter accordingly?</li>
<li>Is there a way to set <code>max_tokens</code> to the max cap so that I won't need to count the number of prompt tokens?</li>
</ul>
"
"<p>I am trying to <code>pip install pybluez</code> and I get this error:</p>
<pre><code>Collecting PyBluez
  Using cached PyBluez-0.23.tar.gz (97 kB)
  Preparing metadata (setup.py) ... error
  error: subprocess-exited-with-error

  × python setup.py egg_info did not run successfully.
  │ exit code: 1
  ╰─&gt; [1 lines of output]
      error in PyBluez setup command: use_2to3 is invalid.
      [end of output]

  note: This error originates from a subprocess, and is likely not a problem with pip.
error: metadata-generation-failed

× Encountered error while generating package metadata.
╰─&gt; See above for output.

note: This is an issue with the package mentioned above, not pip.
hint: See above for details.
</code></pre>
<p>How do I solve this?</p>
<p>Edit:
Turns out its not being updated for windows so I just switched to a linux vb</p>
"
"<p>The question is simple, but I have googled a lot of methods, and there no such solution as:</p>
<pre><code>import svg-render-library
figure = svg-render-library.open('test.svg')
figure.render()
</code></pre>
<p>Is there any simple methods to display an SVG image using only python libraries?
I am asking about rendering the SVG image without any conversion to another formats and to render using pure python, without any 3rd party software. As I have tried, this seems impossible for now.</p>
<p>As built-in python I mean - only python packages available through pip, so it is not necessary to install/compile anything else. And to render I mean to show inside window which part of the python, not the browser or any external software.</p>
"
"<p><a href=""https://i.sstatic.net/vVoYR.png"" rel=""noreferrer"">Matplotlib issue</a></p>
<p>How to get rid of this error when importing matplotlib (actually <code>matplotlib.pyplot</code>)?</p>
<pre><code>ImportError: DLL load failed while importing _cext: The specified module could not be found
</code></pre>
<p>These are fresh Python and Pycharm installs after reinstalling Windows 10. I am using Python 3.11.2 and Pycharm Community Edition 2022.3.3</p>
<p>I did not experience this issue before reinstalling Windows. Most likely I've been using older versions of Python and Pycharm but not much older (don't remember which exactly), something like six to eight months old.</p>
"
"<p>Consider the following model:</p>
<pre class=""lang-py prettyprint-override""><code>from pydantic import BaseModel

class Cirle(BaseModel):
    radius: int
    pi = 3.14
</code></pre>
<p>If I run the following code, I can see the fields of this model:</p>
<pre class=""lang-py prettyprint-override""><code>print(Circle.__fields__)

# Output:

{
  'radius': ModelField(name='radius', type=int, required=True), 
  'pi': ModelField(name='pi', type=float, required=False, default=3.14)
}
</code></pre>
<p>The question is: how can I get the type (or inferred type hint) from the <code>ModelField</code> type? These all give errors:</p>
<pre class=""lang-py prettyprint-override""><code>Circle.__fields__['pi'].type
# AttributeError: 'ModelField' object has no attribute 'type'

Circle.__fields__['pi'].__dict__
# AttributeError: 'ModelField' object has no attribute '__dict__'

type(Circle.__fields__['pi'])
# &lt;class 'pydantic.fields.ModelField'&gt;

import typing
typing.get_type_hints(Circle.__fields__['pi'])
# TypeError: ModelField(name='pi', type=float, required=False, default=3.14)
# is not a module, class, method, or function.

typing.get_type_hints(Circle)
# This does not include the &quot;pi&quot; field because it has no type hints
# {'radius': &lt;class 'int'&gt;}
</code></pre>
<p>I don't even see where the <code>ModelField</code> type is defined in <a href=""https://github.com/pydantic/pydantic/search?q=ModelField"" rel=""noreferrer"">github</a>.</p>
<p>How can I iterate over the fields of a pydantic model and find the types?</p>
"
"<p>I'm trying to remove the vertical space between two specific axes; however <code>plt.figure(layout='constrained')</code> prevents <code>matplotlib.pyplot.subplots_adjust(hspace=0)</code>.</p>
<h2>With <code>layout='constrained'</code></h2>
<pre class=""lang-py prettyprint-override""><code>import matplotlib.pyplot as plt
fig = plt.figure(figsize=(10, 8),layout='constrained')
# fig = plt.figure(figsize=(10, 8))
subfigs = fig.subfigures(2, 1)
axsUp = subfigs[0].subplots(1, 3)
subfigsnest = subfigs[1].subfigures(1, 2, width_ratios=[1, 2])
ax = subfigsnest[0].subplots(1)
axsnest = subfigsnest[1].subplots(2, 1, sharex=True)
subfigsnest[1].subplots_adjust(hspace=0)
plt.show()
</code></pre>
<ul>
<li>Generally subplots are in place; but there is a gap between ax5 &amp; ax6</li>
</ul>
<p><img src=""https://i.sstatic.net/FNoLw.png"" alt=""Generally subplots are in place; but there is a gap between ax5 &amp; ax6"" /></p>
<h2>Without <code>layout='constrained'</code></h2>
<pre class=""lang-py prettyprint-override""><code>import matplotlib.pyplot as plt
# fig = plt.figure(figsize=(10, 8),layout='constrained')
fig = plt.figure(figsize=(10, 8))
subfigs = fig.subfigures(2, 1)
axsUp = subfigs[0].subplots(1, 3)
subfigsnest = subfigs[1].subfigures(1, 2, width_ratios=[1, 2])
ax = subfigsnest[0].subplots(1)
axsnest = subfigsnest[1].subplots(2, 1, sharex=True)
subfigsnest[1].subplots_adjust(hspace=0)
plt.show()
</code></pre>
<ul>
<li>Desired ax5 &amp; ax6</li>
</ul>
<p><img src=""https://i.sstatic.net/O2EeJ.png"" alt=""Desired ax5 &amp; ax6"" /></p>
<p>Using <code>subplots_adjust(hspace=0)</code> with <code>layout='constrained'</code> doesn't work and it produces the warning <strong>UserWarning: This figure was using a layout engine that is incompatible with subplots_adjust and/or tight_layout; not calling subplots_adjust.</strong>.</p>
"
"<p>I am just starting to learn selenium but when I run the program it opens the chrome window for few seconds and then closes giving the error:</p>
<pre><code>DevTools listening on ws://127.0.0.1:52176/devtools/browser/b5500769-96fb-4f99-85df-b05a11157eea
[11368:11308:0324/102029.198:ERROR:page_load_metrics_update_dispatcher.cc(194)] Invalid first_paint 5.369 s for first_meaningful_paint 5.287 s
[11368:11308:0324/102029.486:ERROR:page_load_metrics_update_dispatcher.cc(194)] Invalid first_paint 5.369 s for first_meaningful_paint 5.287 s
[11368:11308:0324/102029.757:ERROR:page_load_metrics_update_dispatcher.cc(194)] Invalid first_paint 5.369 s for first_meaningful_paint 5.287 s
[11368:11308:0324/102030.045:ERROR:page_load_metrics_update_dispatcher.cc(194)] Invalid first_paint 5.369 s for first_meaningful_paint 5.287 s
</code></pre>
<p>As I am new to selenium I dont have any idea why this error is showing up.</p>
"
"<p>Basically I want to achieve this with Flask and LangChain: <a href=""https://www.youtube.com/watch?v=x8uwwLNxqis"" rel=""noreferrer"">https://www.youtube.com/watch?v=x8uwwLNxqis</a>.</p>
<p>I'm building a Q&amp;A Flask app that uses LangChain in the backend, but I'm having trouble to stream the response from ChatGPT. My chain looks like this:</p>
<pre><code>chain = VectorDBQA.from_chain_type(llm=ChatOpenAI(model_name=&quot;gpt-3.5-turbo&quot;, streaming=True, chain_type=&quot;stuff&quot;, vectorstore=docsearch)
...
result = chain({&quot;query&quot;: query})
output = result['result']
</code></pre>
<p>Jinja simply prints the <code>{{ output }}</code>, and it works fine, but the result doesn't appear in the website until the entire response is finished. I want to stream the result as it's being generated by ChatGPT.</p>
<p>I've tried using <code>stream_template</code>, but it doesn't work (it doesn't stream the result, it just prints the full response at once, although I could be doing something wrong).</p>
<hr />
<p>I finally solved it:</p>
<p><a href=""https://github.com/DanteNoguez/FlaskGPT"" rel=""noreferrer"">https://github.com/DanteNoguez/FlaskGPT</a></p>
"
"<p>I am working on Windows 11, using Python 3.11; I'm working on the following code snippet, which comes from from the Python docs on <a href=""https://docs.python.org/3/library/enum.html#enum.StrEnum"" rel=""noreferrer"">enum.StrEnum</a></p>
<pre class=""lang-py prettyprint-override""><code>import enum
from enum import StrEnum


class Build(StrEnum):
    DEBUG = enum.auto()
    OPTIMIZED = enum.auto()

    @classmethod
    def _missing_(cls, value):
        value = value.lower()
        for member in cls:
            if member.value == value:
                return member
        return None

print(Build.DEBUG.value)
</code></pre>
<p>When I run the code, I get the following error: <code>ImportError: cannot import name 'StrEnum' from 'enum'</code>. I made the following changes:</p>
<pre class=""lang-py prettyprint-override""><code>import enum

class Build(enum.StrEnum):  # &lt;--change made here
    DEBUG = enum.auto()
    OPTIMIZED = enum.auto()

</code></pre>
<p>Now when I run the code I get an <code>AttributeError: module 'enum' has no attribute 'StrEnum'</code>. I am working with PyCharm, I can see <code>StrEnum</code> class is apart of the <code>enum</code> library. So, can someone please explain what I am doing wrong, or whats going on here. Thanks for any help!</p>
"
"<p>I'm trying to use <a href=""https://mypy.readthedocs.io/en/stable/more_types.html"" rel=""nofollow noreferrer"">overloading</a> to make the return type of a variadic function depend on the type of its arguments in a certain way. Specifically, I want the return type to be X if and only if <em>any</em> of its arguments is of type X.</p>
<p>Consider the following minimal example:</p>
<pre class=""lang-py prettyprint-override""><code>from typing import overload

class Safe:
    pass

class Dangerous:
    pass

@overload
def combine(*args: Safe) -&gt; Safe: ...

@overload
def combine(*args: Safe | Dangerous) -&gt; Safe | Dangerous: ...

def combine(*args: Safe | Dangerous) -&gt; Safe | Dangerous:
    if all(isinstance(arg, Safe) for arg in args):
        return Safe()
    else:
        return Dangerous()

reveal_type(combine())
reveal_type(combine(Safe()))
reveal_type(combine(Dangerous()))
reveal_type(combine(Safe(), Safe()))
reveal_type(combine(Safe(), Dangerous()))
</code></pre>
<p>This outputs</p>
<pre class=""lang-none prettyprint-override""><code>example.py:21: note: Revealed type is &quot;example.Safe&quot;
example.py:22: note: Revealed type is &quot;example.Safe&quot;
example.py:23: note: Revealed type is &quot;Union[example.Safe, example.Dangerous]&quot;
example.py:24: note: Revealed type is &quot;example.Safe&quot;
example.py:25: note: Revealed type is &quot;Union[example.Safe, example.Dangerous]&quot;
Success: no issues found in 1 source file
</code></pre>
<p>I want to set things up so that the inferred types of <code>combine(Dangerous())</code> and <code>combine(Safe(), Dangerous())</code>, for example, are <code>Dangerous</code> rather than <code>Safe | Dangerous</code>. Changing the return type of the second overload to just <code>Dangerous</code> yields an error:</p>
<pre class=""lang-none prettyprint-override""><code>example.py:10: error: Overloaded function signatures 1 and 2 overlap with incompatible return types  [misc]
example.py:21: note: Revealed type is &quot;example.Safe&quot;
example.py:22: note: Revealed type is &quot;example.Safe&quot;
example.py:23: note: Revealed type is &quot;example.Dangerous&quot;
example.py:24: note: Revealed type is &quot;example.Safe&quot;
example.py:25: note: Revealed type is &quot;example.Dangerous&quot;
Found 1 error in 1 file (checked 1 source file)
</code></pre>
<p>Thus it seems that I need a way to annotate the second overload to explicitly state that <em>at least one of</em> its arguments is <code>Dangerous</code>. Is there a way to do this?</p>
<p>It occurs to me that the desired type for the argument sequence is <code>Sequence[Safe | Dangerous] - Sequence[Safe]</code>, but I don't think type subtraction is supported yet.</p>
"
"<p><strong>If your question was closed as a duplicate of this, it is because</strong> you have some code of the <em>general form</em></p>
<pre><code>x = X()
# later...
x = x.y()
# or:
x.y().z()
</code></pre>
<p>where <code>X</code> is some type that provides <code>y</code> and <code>z</code> methods intended to <em>mutate</em> (modify) the object (instance of the <code>X</code> type). This can apply to:</p>
<ul>
<li><em>mutable</em> built-in types, such as <code>list</code>, <code>dict</code>, <code>set</code> and <code>bytearray</code></li>
<li><em>classes</em> provided by the standard library (especially Tkinter widgets) or by a third-party library.</li>
</ul>
<p>Code of this form is <strong>commonly, but not always</strong> wrong. The telltale signs of a problem are:</p>
<ul>
<li><p>With <code>x.y().z()</code>, an exception is raised like <code>AttributeError: 'NoneType' object has no attribute 'z'</code>.</p>
</li>
<li><p>With <code>x = x.y()</code>, <code>x</code> becomes <code>None</code>, instead of being the modified object. This might be discovered by later wrong results, or by an exception like the above (when <code>x.z()</code> is tried later).</p>
</li>
</ul>
<p>There are a huge number of existing questions on Stack Overflow about this issue, all of which are really the same question. There are even multiple previous attempts at canonicals covering the same question in a specific context. However, the context is <em>not needed to understand the problem</em>, so here is an attempt to answer generally:</p>
<p><em>What is wrong with the code? Why do the methods behave this way, and how can we work around that?</em></p>
<hr />
<p><sub>Also note that analogous problems occur when <a href=""/q/51310263"">trying to use a <code>lambda</code></a> (<a href=""/q/5753597"">or a list comprehension</a>) for side effects.</sub></p>
<p><sub>The same <em>apparent</em> problem can be caused by methods that return <code>None</code> for other reasons - for example, <a href=""/q/75845973"">BeautifulSoup uses <code>None</code> return values to indicate that a tag was not found in the HTML</a>. However, once the current problem - of <em>expecting</em> a method to update an object and also return the same object - has been identified, it is the same problem in all contexts.</sub></p>
<p><sub>Please do not use use this question to close other questions that are about using <code>.append</code> <em>in a loop</em> to append to a list repeatedly. Simply understanding what went wrong with the <code>.append</code> usage will not be very helpful in these cases, and people asking those questions should also see other techniques for building lists. Please use <a href=""https://stackoverflow.com/questions/75666408/"">How can I collect the results of a repeated calculation in a list, dictionary etc. (or make a copy of a list with each element modified)?</a> instead.</sub></p>
<p><sub>More specific versions of the Q&amp;A:</sub></p>

<ul>
<li><sub> <a href=""https://stackoverflow.com/questions/11205254/"">Why do these list methods (append, sort, extend, remove, clear, reverse) return None rather than the resulting list?</a> </sub></li>
</ul>
"
"<p>I'm getting a syntax error for the following statement, the problem section being <code>(Player.name == data[&quot;name&quot;]) | (Player.account == data[&quot;account&quot;]))</code>:</p>
<pre class=""lang-py prettyprint-override""><code>player_from_db = db.session.execute(
    select(Player).where(
             (Player.name == data[&quot;name&quot;]) | 
             (Player.account == data[&quot;account&quot;])
    )).scalar()
</code></pre>
<p>This is the error:</p>
<blockquote>
<p>Expected type 'Union[ColumnElement[bool], _HasClauseElement, SQLCoreOperations[bool], ExpressionElementRole[bool], () -&gt; ColumnElement[bool], LambdaElement]', got 'Union[bool, int]' instead</p>
</blockquote>
<p>But the clause clearly has two bool expressions. Help appreciated!</p>
"
"<p>How does Cloudflare even know that this request came from a script even if I provided all the data, cookies and parameters when making a normal request? What does it check for? Am I doing something wrong? For example (I have redacted some of the values):</p>
<pre class=""lang-py prettyprint-override""><code>import requests

cookies = {
    '__Host-next-auth.csrf-token': '...',
    'cf_clearance': '...',
    'oai-asdf-ugss': '...',
    'oai-asdf-gsspc': '...',
    'intercom-id-dgkjq2bp': '...',
    'intercom-session-dgkjq2bp': '',
    'intercom-device-id-dgkjq2bp': '...',
    '_cfuvid': '...',
    '__Secure-next-auth.callback-url': '...',
    'cf_clearance': '...',
    '__cf_bm': '...',
    '__Secure-next-auth.session-token': '...',
}

headers = {
    'authority': 'chat.openai.com',
    'accept': 'text/event-stream',
    'accept-language': 'en-IN,en-US;q=0.9,en;q=0.8',
    'authorization': 'Bearer ...',
    'content-type': 'application/json',
    'cookie': '__Host-next-auth.csrf-token=...',
    'origin': 'https://chat.openai.com',
    'referer': 'https://chat.openai.com/chat',
    'sec-ch-ua': '&quot;Brave&quot;;v=&quot;111&quot;, &quot;Not(A:Brand&quot;;v=&quot;8&quot;, &quot;Chromium&quot;;v=&quot;111&quot;',
    'sec-ch-ua-mobile': '?0',
    'sec-ch-ua-platform': '&quot;Linux&quot;',
    'sec-fetch-dest': 'empty',
    'sec-fetch-mode': 'cors',
    'sec-fetch-site': 'same-origin',
    'sec-gpc': '1',
    'user-agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/111.0.0.0 Safari/537.36',
}

json_data = {
 ...
}

response = requests.post('https://chat.openai.com/backend-api/conversation', cookies=cookies, headers=headers, json=json_data)
</code></pre>
<p>I have tried different useragents to no avail, but I can't seem to figure out whats causing the problem in the first place.</p>
<p>The response comes back with error code <code>403</code> and HTML something like:</p>
<pre class=""lang-html prettyprint-override""><code>&lt;html&gt;
...
...
&lt;h1&gt;Access denied&lt;/h1&gt;
  &lt;p&gt;You do not have access to chat.openai.com.&lt;/p&gt;&lt;p&gt;The site owner may have set restrictions that prevent you from accessing the site.&lt;/p&gt;
  &lt;ul class=&quot;cferror_details&quot;&gt;
    &lt;li&gt;Ray ID: ...&lt;/li&gt;
    &lt;li&gt;Timestamp: ...&lt;/li&gt;
    &lt;li&gt;Your IP address: ...&lt;/li&gt;
    &lt;li class=&quot;XXX_no_wrap_overflow_hidden&quot;&gt;Requested URL: chat.openai.com/backend-api/conversation &lt;/li&gt;
    &lt;li&gt;Error reference number: ...&lt;/li&gt;
    &lt;li&gt;Server ID: ...&lt;/li&gt;
    &lt;li&gt;User-Agent: Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/111.0.0.0 Safari/537.36&lt;/li&gt;
  &lt;/ul&gt;
...
...
&lt;/html&gt;
</code></pre>
"
"<p>I have a dataframe:</p>
<pre><code>df = pd.DataFrame({
    'col1': [0.1, 0.8, 0.2, 0.12],
    'col2': [0.2, 0.9, 0.1, 0.1],
    'col3': [0.3, 0.2, 0.1, 0.2],
    'col4': [0.6, 0.7, 0.8, 0.9]
})

   col1  col2  col3  col4
0  0.10   0.2   0.3   0.6
1  0.80   0.9   0.2   0.7
2  0.20   0.1   0.1   0.8
3  0.12   0.1   0.2   0.9
</code></pre>
<p>I want to find those column values where at least one row value is greater than 0.7 and all the remaining are less than 0.3. In the above snippet, the result should be:</p>
<pre><code>['col1', 'col2']
</code></pre>
<p>I have tried the following code:</p>
<pre><code>selected_cols = []
for col in df.columns:
    values = df[col]
    if any(values &gt; 0.7) and all(values &lt; 0.3):
        selected_cols.append(col)

print(selected_cols)
</code></pre>
"
"<p>I'm trying to build a discord bot that uses the GPT-4 API to function as a chatbot on discord. I have the most recent version of the OpenAI library but when I run my code it tells me &quot;An error occurred: module 'openai' has no attribute 'ChatCompletion'&quot;</p>
<p>I tried uninstalling and reinstalling the OpenAI library, I tried using the completions endpoint and got the error &quot;This is a chat model and not supported in the v1/completions endpoint. Did you mean to use v1/chat/completions?&quot;</p>
<p>This is the snippet of code that's giving me issues:</p>
<pre><code>async def get_gpt_response(prompt, history):
    history_strings = [f&quot;{message['role']}: {message['content']}&quot; for message in history] # update history format
    chat_prompt = '\n'.join(history_strings + [f&quot;user: {prompt}&quot;])
    
    completions = openai.ChatCompletion.create(
        engine=config[&quot;model&quot;],
        prompt=chat_prompt,
        max_tokens=config[&quot;max_tokens&quot;],
        n=1,
        temperature=config[&quot;temperature&quot;],
    )
    return completions.choices[0].text.strip().split('assistant:', 1)[-1].strip()
</code></pre>
"
"<p>I just have a newly created Environment in Anaconda (conda 22.9.0 and Python 3.10.10). Then I proceed to install langchain (<code>pip install langchain</code> if I try conda install langchain it does not work). According to the quickstart guide <a href=""https://python.langchain.com/en/latest/getting_started/getting_started.html"" rel=""noreferrer"">I have to install one model provider</a> so I install openai (<code>pip install openai</code>).</p>
<p>Then I enter to the python console and try to load a PDF using the class <a href=""https://python.langchain.com/en/latest/modules/indexes/document_loaders/examples/pdf.html#using-unstructured"" rel=""noreferrer"">UnstructuredPDFLoader</a> and I get the following error. What the problem could be?</p>
<pre><code>(langchain) C:\Users\user&gt;python
Python 3.10.10 | packaged by Anaconda, Inc. | (main, Mar 21 2023, 18:39:17) [MSC v.1916 64 bit (AMD64)] on win32
&gt;&gt;&gt; from langchain.document_loaders import UnstructuredPDFLoader
&gt;&gt;&gt; loader = UnstructuredPDFLoader(&quot;C:\\&lt;path-to-data&gt;\\data\\name-of-file.pdf&quot;)
Traceback (most recent call last):
  File &quot;C:\&lt;path-to-anaconda&gt;\envs\langchain\lib\site-packages\langchain\document_loaders\unstructured.py&quot;, line 32, in __init__
    import unstructured  # noqa:F401
ModuleNotFoundError: No module named 'unstructured'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;
  File &quot;C:\&lt;path-to-anaconda&gt;\envs\langchain\lib\site-packages\langchain\document_loaders\unstructured.py&quot;, line 90, in __init__
    super().__init__(mode=mode, **unstructured_kwargs)
  File &quot;C:\&lt;path-to-anaconda&gt;\envs\langchain\lib\site-packages\langchain\document_loaders\unstructured.py&quot;, line 34, in __init__
    raise ValueError(
ValueError: unstructured package not found, please install it with `pip install unstructured`
</code></pre>
"
"<p>After a long search, I could not find any thread/discussion helping me to make <strong>autoscaling</strong> work with plotly.</p>
<p>The idea would be that when I use the x-range slider to go through the data, the y-axis would be dynamically rescaled each time I move the slider.</p>
<p>Currently, this is how it looks like (pretty unuseful):
<a href=""https://i.sstatic.net/305MC.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/305MC.png"" alt=""enter image description here"" /></a></p>
<p>Here is a snippet of my code:</p>
<pre><code>fig = plt.figure()
    
    # Layout format and buttons
    layout = dict(
        xaxis=dict(
            rangeselector=dict(
                buttons=list([
                    dict(count=1,
                         label='1m',
                         step='month',
                         stepmode='backward'),
                    dict(count=6,
                         label='6m',
                         step='month',
                         stepmode='backward'),
                    dict(count=1,
                        label='YTD',
                        step='year',
                        stepmode='todate'),
                    dict(count=1,
                        label='1y',
                        step='year',
                        stepmode='backward'),
                    dict(step='all')
                ])
            ),
            rangeslider=dict(
                visible = True
            ),
            type='date'
        )
    )
    
    # Plot open, high, low, close for each coins
    for c in dic.keys():
        ohlc = dic[c][['open','high','low','close']].copy()
        
        candlestick = go.Candlestick(x = ohlc.index,
                                     open = ohlc['open'],
                                     high = ohlc['high'],
                                     low = ohlc['low'],
                                     close = ohlc['close'],
                                     name = 'OHLC')
        
        fig = go.Figure(data = [candlestick],
                        layout = layout)
        
        fig.update_yaxes(showgrid=True, 
                          zeroline=False, 
                          showticklabels=True,
                          showspikes=True, 
                          spikemode='across', 
                          spikesnap='cursor', 
                          spikethickness=1, 
                          showline=True, 
                          spikedash='dot',
                          spikecolor=&quot;#707070&quot;,
                          autorange = True,     # DOESN'T WORK....
                          fixedrange= False     # DOESN'T WORK....
                          )
            
        fig.update_xaxes(showgrid=True, 
                          zeroline=False, 
                          showticklabels=True,
                          showspikes=True, 
                          spikemode='across', 
                          spikesnap='cursor', 
                          spikethickness=1, 
                          showline=True, 
                          spikedash='dot',
                          spikecolor=&quot;#707070&quot;,
                          type='date',
                          )
        
        fig.update_layout(title = 'OHLC for: '+c,
                          height = 750,
                          plot_bgcolor=&quot;#FFFFFF&quot;,
                          hovermode=&quot;x&quot;,
                          hoverdistance=100,
                          spikedistance=1000,
                          xaxis_rangeslider_visible=True,
                          dragmode='pan',
                          )
        
        fig.show()
</code></pre>
<p>I tried to use <code>autorange = True</code> and <code>fixedrange = False</code> but apparently that does nothing for reasons I don't understand.</p>
"
"<p>I have a polars DataFrame with multiple numeric (float dtype) columns. I want to write some of them to a csv with a certain number of decimal places. The number of decimal places I want is column-specific.</p>
<p><code>polars</code> offers <a href=""https://pola-rs.github.io/polars/py-polars/html/reference/expressions/api/polars.format.html#polars-format"" rel=""noreferrer"">format</a>:</p>
<pre class=""lang-py prettyprint-override""><code>import polars as pl

df = pl.DataFrame({&quot;a&quot;: [1/3, 1/4, 1/7]})

df.select(
    [
        pl.format(&quot;as string {}&quot;, pl.col(&quot;a&quot;)),
        ]
    )

shape: (3, 1)
┌───────────────────────────────┐
│ literal                       │
│ ---                           │
│ str                           │
╞═══════════════════════════════╡
│ as string 0.3333333333333333  │
│ as string 0.25                │
│ as string 0.14285714285714285 │
└───────────────────────────────┘
</code></pre>
<p>However, if I try to set a directive to specify number of decimal places, it fails:</p>
<pre class=""lang-py prettyprint-override""><code>df.select(
    [
        pl.format(&quot;{:.3f}&quot;, pl.col(&quot;a&quot;)),
        ]
)
</code></pre>
<blockquote>
<p>ValueError: number of placeholders should equal the number of arguments</p>
</blockquote>
<p>Is there an option to have &quot;real&quot; f-string functionality without using an <code>apply</code>?</p>
<ul>
<li><code>pl.__version__: '0.16.16'</code></li>
<li>related: <a href=""https://stackoverflow.com/q/71790235/10197418"">Polars: switching between dtypes within a DataFrame</a></li>
<li>to set the decimal places of <em>all</em> output columns, <a href=""https://pola-rs.github.io/polars/py-polars/html/reference/api/polars.DataFrame.write_csv.html#polars-dataframe-write-csv"" rel=""noreferrer"">pl.DataFrame.write_csv</a> offers the <code>float_precision</code> keyword</li>
</ul>
"
"<p>I'm currently trying to use pytorch 2.0 for boosting training performance with my project. And I've heard that torch.compile might be boost some models.</p>
<p>So my question (for now) is simple; how should I use torch.compile with large model?</p>
<p>Such as, should I use torch.model like this?</p>
<pre class=""lang-py prettyprint-override""><code>class BigModel(nn.Module):
    def __init__(self, ...):
        super(BigModel, self).__init__()
        self.model = nn.Sequential(
            SmallBlock(), 
            SmallBlock(), 
            SmallBlock(), 
            ...
        )
        ...

class SmallBlock(nn.Module):
    def __init__(self, ...):
        super(SmallBlock, self).__init__()
        self.model = nn.Sequential(
            ...some small model...
        )

model = BigModel()
model_opt = torch.compile(model)
</code></pre>
<p>,or like this?</p>
<pre class=""lang-py prettyprint-override""><code>class BigModel(nn.Module):
    def __init__(self, ...):
        super(BigModel, self).__init__()
        self.model = nn.Sequential(
            SmallBlock(), 
            SmallBlock(), 
            SmallBlock(), 
            ...
        )
        ...

class SmallBlock(nn.Module):
    def __init__(self, ...):
        super(SmallBlock, self).__init__()
        self.model = nn.Sequential(
            ...some small model...
        )
        self.model = torch.compile(self.model)

model = BigModel()
model_opt = torch.compile(model)
</code></pre>
<p>For summary,</p>
<ol>
<li>Should compile each layer? or torch.compile do this automatically?</li>
<li>Is there any tips for using torch.compile properly?</li>
</ol>
<p>To be honest, I tried both, but there are no differences..</p>
<p>And also, it didn't speed up dramatically, I just checked speed up rate for my model just about 5 ~ 10%.</p>
"
"<p>i have a pydantic class:</p>
<pre><code>class SomeData(BaseModel):
    id: int
    x: str
    y: str
    z: str
</code></pre>
<p>and lets say i have two object of this class, obj1, obj2.</p>
<p>is there any simple way, i can copy obj2, into obj1, while ignoring a subset of fields? for example copy all SomeData fields, except [id,z]</p>
"
"<p>I have a large collection of documents each consisting of ~ 10 sentences. For each document, I wish to find the sentence that maximises perplexity, or equivalently the loss from a fine-tuned causal LM. I have decided to use Hugging Face and the <code>distilgpt2</code> model for this purpose. I have 2 problems when trying to do in an efficient (vectorized) fashion:</p>
<ol>
<li><p>The tokenizer required padding to work in batch mode, but when computing the loss on padded <code>input_ids</code> those pad tokens are contributing to the loss. So the loss of a given sentence depends on the length of the longest sentence in the batch which is clearly wrong.</p>
</li>
<li><p>When I pass a batch of input IDs to the model and compute the loss, I get a scalar as it (mean?) pools across the batch. I instead need the loss per item, not the pooled one.</p>
</li>
</ol>
<p>I made a version that operates on a sentence by sentence basis and while correct, it is extremely slow (I want to process ~ 25m sentences total). Any advice?</p>
<p>Minimal example below:</p>
<pre><code># Init
tokenizer = AutoTokenizer.from_pretrained(&quot;distilgpt2&quot;)
tokenizer.pad_token = tokenizer.eos_token
model = AutoModelForCausalLM.from_pretrained(&quot;clm-gpu/checkpoint-138000&quot;)
segmenter = spacy.load('en_core_web_sm')

# That's the part I need to vectorise, surely within a document (bsize ~ 10)
# and ideally across documents (bsize as big as my GPU can handle)
def select_sentence(sentences):
    &quot;&quot;&quot;We pick the sentence that maximizes perplexity&quot;&quot;&quot;
    max_loss, best_index = 0, 0
    for i, sentence in enumerate(sentences):
        encodings = tokenizer(sentence, return_tensors=&quot;pt&quot;)
        input_ids = encodings.input_ids
        loss = lm(input_ids, labels=input_ids).loss.item()
        if loss &gt; max_loss:
            max_loss = loss
            best_index = i

    return sentences[best_index]

for document in documents:
    sentences = [sentence.text.strip() for sentence in segmenter(document).sents]
    best_sentence = select_sentence(sentences)
    write(best_sentence)

</code></pre>
"
"<p>I have to move data from old account to new one. (Both are free account)</p>
<p>And there is 12,000 vectors at the index.</p>
<p>Is there any API or UI for pinecone to export &quot;all&quot; data?</p>
<h2>#1</h2>
<p>I tried to get all data by using <code>top_k</code> as all number of my index...</p>
<pre><code>query_embedding = embedder.encode('hi', convert_to_numpy=True)
response = index.query(query_embedding, top_k=12000, include_metadata=True)
</code></pre>
<p>But fail like below. the maximum return value is 10,000</p>
<pre><code>ApiValueError: Invalid value for `top_k`, must be a value less than or equal to `10000`
</code></pre>
<h2>#2</h2>
<p><a href=""https://docs.pinecone.io/reference/create_collection"" rel=""noreferrer"">https://docs.pinecone.io/reference/create_collection</a></p>
<p>I know there is the method that create new collection, but it seems that I can't access the other account's collection.</p>
"
"<p>Can someone please assist how to fix this issue, why i am getting this error while running the Ansible playbook</p>
<blockquote>
<p>2023-03-31 11:47:39,902 p=57332 u=NI40153964 n=ansible | An exception occurred during task execution.
To see the full traceback, use -vvv. The error was: re.error: global flags not at the start of the expression at position 1
2023-03-31 11:47:39,903 p=57332</p>
</blockquote>
<p>Python &amp; Ansible versions:</p>
<ul>
<li>Python 3.11.2</li>
<li>Ansible [core 2.14.3]</li>
</ul>
<p>The YML file which I am using as</p>
<pre class=""lang-yaml prettyprint-override""><code>    - name: rename log file to user specific
      delegate_to: localhost
      command: mv ~/.ansible/wmDeployment.log ~/.ansible/wmDeployment_{{_user}}.log
      register: logfile_rename_output
      run_once: true
      ignore_errors: yes

    - name: set the fact for email
      set_fact:
        package_list: &quot;{{hostvars['localhost']['package_list']}}&quot;
        log_output: &quot;{{log_data| regex_search('((?s)(?&lt;=This task is to install package in target servers).*?(?=This task is to enable kafka consumers and send mail of playbook log))')}}&quot;
        emailkey: code
        cacheable: true
</code></pre>
"
"<p>I'm making a Python script to use OpenAI via its API. However, I'm getting this error:</p>
<blockquote>
<p>openai.error.RateLimitError: You exceeded your current quota, please check your plan and billing details</p>
</blockquote>
<p>My script is the following:</p>
<pre class=""lang-py prettyprint-override""><code>#!/usr/bin/env python3.8
# -*- coding: utf-8 -*-

import openai
openai.api_key = &quot;&lt;My PAI Key&gt;&quot;

completion = openai.ChatCompletion.create(
  model=&quot;gpt-3.5-turbo&quot;,
  messages=[
    {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Tell the world about the ChatGPT API in the style of a pirate.&quot;}
  ]
)

print(completion.choices[0].message.content)
</code></pre>
<p>I'm declaring the shebang <code>python3.8</code>, because I'm using <a href=""https://github.com/pyenv/pyenv"" rel=""noreferrer"">pyenv</a>. I think it should work, since I did 0 API requests, so I'm assuming there's an error in my code.</p>
"
"<p>I am trying to save a figure using tikzplotlib. However, I am encountering an AttributeError: 'Legend' object has no attribute '_ncol'. I am currently using tikzplotlib version 0.10.1 and matplotlib version 3.7.0. Without using &quot;plt.legend()&quot; everything works.</p>
<p>Below is an example that is not working:</p>
<pre><code>import numpy as np
import matplotlib.pyplot as plt
import tikzplotlib

# Data
x = np.linspace(0, 10, 100)
y1 = np.sin(x)
y2 = np.cos(x)
y3 = np.tan(x)

# Plotting
plt.figure()
plt.plot(x, y1, label='sin(x)')
plt.plot(x, y2, label='cos(x)')
plt.plot(x, y3, label='tan(x)')
plt.legend()

# Save as TikZ file
tikzplotlib.save(&quot;plot.tikz&quot;)

</code></pre>
"
"<p>i have problem while building docker image on python :
below execution process takes long time around 20 minutes:</p>
<pre><code>Building wheel for pandas (pyproject.toml): still running...
Building wheel for pandas (pyproject.toml): still running...
Building wheel for pandas (pyproject.toml): still running...
Building wheel for pandas (pyproject.toml): still running...
Building wheel for pandas (pyproject.toml): still running...
</code></pre>
<p><code>Dockerfile</code>:</p>
<pre><code>FROM python:3.11.2-buster
WORKDIR /app
COPY . .
RUN pip install -r /app/requirements.txt
CMD [&quot;uvicorn&quot;, &quot;main:app&quot;, &quot;--host=0.0.0.0&quot;, &quot;--port=80&quot;]
</code></pre>
<p><code>requirement.txt</code>:</p>
<pre><code>fastapi
uvicorn
pydantic[dotenv]
requests
python-dotenv==0.19.2
pandas==1.4.3
numpy==1.24.2
scikit-learn==1.2.2
</code></pre>
"
"<p>On <a href=""https://superfastpython.com/asyncio-vs-threading/"" rel=""noreferrer"">this</a> page I read this:</p>
<blockquote>
<p>Coroutines in the asyncio module are not limited by the Global Interpreter Lock or GIL.</p>
</blockquote>
<p>But how is this possible if both the <code>asyncio</code> event loop and the <code>threading</code> threads are running in a single Python process with GIL?</p>
<p>As far as I understand, the impact of the GIL on <code>asyncio</code> will not be as strong as on <code>threading</code>, because in the case of <code>threading</code>, the interpreter will switch to useless operations, such as <code>time.sleep()</code>.  Therefore, when working with <code>asyncio</code>, it is recommended to use <code>asyncio.sleep()</code>.</p>
<p>I understand that these tools are designed for slightly different things, <code>threading</code> is more often used to execute &quot;legacy&quot; blocking code for IO-Bound operations, and <code>asyncio</code> for non-blocking code.</p>
"
"<p>I have issues with jupyter extensions on ArchLinux. In particular, I get the following error:</p>
<pre><code>[W 2023-04-01 18:34:36.504 ServerApp] A `_jupyter_server_extension_points` function was not found in jupyter_nbextensions_configurator. Instead, a `_jupyter_server_extension_paths` function was found and will be used for now. This function name will be deprecated in future releases of Jupyter Server.
[W 2023-04-01 18:34:36.493 ServerApp] A `_jupyter_server_extension_points` function was not found in notebook_shim. Instead, a `_jupyter_server_extension_paths` function was found and will be used for now. This function name will be deprecated in future releases of Jupyter Server.
</code></pre>
<p>How can I get rid of this error/warning? I tried removing the functions with Pip, but it did not work.
Any ideas?</p>
"
"<p>I'm trying to use whisper AI on my computer. I have a NVIDIA GPU RTX 2060, installed CUDA and FFMPEG.</p>
<p>I'm running this code :</p>
<pre><code>import whisper

model = whisper.load_model(&quot;medium&quot;)
result = model.transcribe(&quot;venv/files/test1.mp3&quot;)
print(result[&quot;text&quot;])
</code></pre>
<p>and having issue :</p>
<pre><code>whisper\transcribe.py:114: UserWarning: FP16 is not supported on CPU; using FP32 instead
  warnings.warn(&quot;FP16 is not supported on CPU; using FP32 instead&quot;)
</code></pre>
<p>I don't understand why FP16 is not support since I have a good GPU and everything installed. Any help would be appreciated. Thanks.</p>
<p>I installed all the requirement and I was expecting that whisper AI would use the GPU</p>
"
"<p>im very new to sqlite, im using macbook m1, im try to use jupyter notebook to do sql
here is my code</p>
<pre><code>%load_ext sql

import csv, sqlite3

con = sqlite3.connect(&quot;socioeconomic.db&quot;)
cur = con.cursor()
</code></pre>
<p>everything is okay until this
but when i connect to the sqlite</p>
<pre><code>%sql sqlite:///socioeconomic.db
</code></pre>
<p>there is error something like this</p>
<pre><code>MetaData.__init__() got an unexpected keyword argument 'bind'
Connection info needed in SQLAlchemy format, example:
               postgresql://username:password@hostname/dbname
               or an existing connection: dict_keys([])
</code></pre>
<p>do you have any suggestion what should i do?</p>
<p>i tried to use other laptop but windows, and the same code works, but in mac isnt</p>
"
"<p>I installed rasa on a virtual environment on windows. But while I am trying to check either rasa is installed or not, it is showing an error that says-</p>
<p><code>ImportError: cannot import name 'CLOSED' from 'websockets.connection'</code></p>
<p>I have reinstalled rasa, and installed websockets. But still getting the error.</p>
<p>Python version is 3.10.2</p>
<p>Can anyone help me to solve this problem?</p>
"
"<p>i wanted to know how to use python script with tauri app, i tried a few things but failed
i try to take an input from the user using html tag then want to pass it to python and then after the python code does the adding wanted to display the result back in the html page, i got confused how to communicate the two of them(python and javascript)
i saved my python script in the same directory as the html but when i click the button there is not response,</p>
<p>this is my python script</p>
<pre><code>    num1 = int(sys.argv[1])
    num2 = int(sys.argv[2])
    result = num1 + num2
    print(str(result))
</code></pre>
<p>and this is the html part</p>
<pre><code>&lt;html&gt;
&lt;head&gt;
  &lt;meta charset=&quot;UTF-8&quot;&gt;
  &lt;title&gt;My Tauri App&lt;/title&gt;
&lt;/head&gt;
&lt;body&gt;
  &lt;label for=&quot;num1&quot;&gt;Enter number 1:&lt;/label&gt;
  &lt;input type=&quot;number&quot; id=&quot;num1&quot;&gt;

  &lt;label for=&quot;num2&quot;&gt;Enter number 2:&lt;/label&gt;
  &lt;input type=&quot;number&quot; id=&quot;num2&quot;&gt;

  &lt;button id=&quot;addBtn&quot;&gt;Add Numbers&lt;/button&gt;

  &lt;div id=&quot;result&quot;&gt;&lt;/div&gt;

  &lt;script&gt;
    const { spawn } = require('child_process');

    const addBtn = document.getElementById('addBtn');
    const num1Input = document.getElementById('num1');
    const num2Input = document.getElementById('num2');
    const resultDiv = document.getElementById('result');

    addBtn.addEventListener('click', () =&gt; {
      const num1 = parseInt(num1Input.value);
      const num2 = parseInt(num2Input.value);

      const python = spawn('python', ['add_numbers.py', num1.toString(), num2.toString()]);

      python.stdout.on('data', data =&gt; {
        const result = data.toString().trim();
        resultDiv.textContent = `Result: ${result}`;
      });

      python.stderr.on('data', error =&gt; {
        console.error(error.toString());
      });
    });
  &lt;/script&gt;
&lt;/body&gt;
&lt;/html&gt;
</code></pre>
<p>i saved my python script in the same directory as the html but when i click the button there is not response,</p>
"
"<p>I have a simple python package, let's call it <code>my_package</code>.</p>
<p>Its files are located in <code>src/python/my_package</code>.
In addition, there is a <code>data</code> folder in the repository root, which should be included in the resulting python wheel within the <code>my_package</code>.</p>
<pre><code>.
├── src
    └── python
        └── my_package
├── data
    └── stuff.json
├── pyproject.toml
</code></pre>
<p>I did not find any way to configure poetry that it includes the additional <code>data</code> folder in the correct way.</p>
<p>Here is my <code>pyproject.toml</code></p>
<pre><code>[tool.poetry]
name = &quot;my-package&quot;
version = &quot;2.10.0&quot; 

packages = [
    { include = &quot;my_package&quot;, from = &quot;src/python&quot; }
]

# This does not work! It puts the `data` folder into site-packages/data instead of site-packages/my_package/data
include = [
    { path = &quot;data&quot;, format = [&quot;sdist&quot;, &quot;wheel&quot;] }
]

[build-system]
requires = [&quot;poetry-core&gt;=1.0.0&quot;]
build-backend = &quot;poetry.core.masonry.api&quot;
</code></pre>
<p>I also found the following solution, using a pre-build script:
<a href=""https://github.com/python-poetry/poetry/issues/5539#issuecomment-1126818974"" rel=""noreferrer"">https://github.com/python-poetry/poetry/issues/5539#issuecomment-1126818974</a></p>
<p>Problem: it changes the wheel that it is not pure anymore, but depends on CPython.</p>
<p>Also tried with symlink, but that does not work: <a href=""https://stackoverflow.com/questions/65670606/how-to-include-symlinks-and-the-linked-file-in-a-python-wheel-using-poetry"">How to include symlinks and the linked file in a python wheel using Poetry?</a></p>
<p><strong>Question:
What is the correct way to include additional resource files in a python wheel using poetry?</strong></p>
"
"<p>I'm having an issue with circular imports in SQLAlchemy.</p>
<p>I have two files foo.py and bar.py. foo.py defines a SQLAlchemy class Foo, and bar.py defines a class Bar.</p>
<p>Both Foo and Bar are each other's foreign keys, so I map them to each other with <code>Mapped[&quot;...&quot;]</code> to get type safety, however that means I need to import the actual classes aswell.</p>
<p>This is causing a circular import error.</p>
<p>What's the best way to handle this issue? What are some general best practices for dealing with circular imports in SQLAlchemy? Can't you have type safety in this case if you use a relationship bidirectionally?</p>
<pre class=""lang-py prettyprint-override""><code># foo.py

from sqlalchemy import Column, Integer, ForeignKey
from sqlalchemy.orm import relationship
from .bar import Bar

class Foo(Base):
    __tablename__ = 'foo'
    id = Column(Integer, primary_key=True)
    bar_id = Column(Integer, ForeignKey('bar.id'))
    bar: Mapped[&quot;Bar&quot;] = relationship('Bar')

# bar.py

from sqlalchemy import Column, Integer, ForeignKey
from sqlalchemy.orm import relationship
from .foo import Foo

class Bar(Base):
    __tablename__ = 'bar'
    id = Column(Integer, primary_key=True)
    foo_id = Column(Integer, ForeignKey('foo.id'))
    foo: Mapped[&quot;Foo&quot;] = relationship('Foo')
</code></pre>
<p>Edit:
Note that I can't remove the Bar and Foo imports because then <code>Mapped[&quot;...&quot;]</code> will raise an undefined error for &quot;...&quot;</p>
"
"<p>when my project run this code it will return
<code>openai.error.APIConnectionError: Error communicating with OpenAI</code></p>
<pre><code>async def embeddings_acreate(input: list[str]):
    
    return await openai.Embedding.acreate(
        api_key=await get_openai_api_key(),
        model='text-embedding-ada-002',
        input=input,
        timeout=60,
    )
</code></pre>
<p>but if I tried:</p>
<pre><code>import openai
import logging


openai.api_key = 'secret'

input_list = [
    &quot;tell me your name&quot;
]

response = openai.Embedding.create(
    model=&quot;text-embedding-ada-002&quot;,
    input=input_list
)

embeddings = response[&quot;data&quot;]
print(embeddings)
</code></pre>
<p>it worked......</p>
<p>I hope to use async and make it</p>
"
"<p>I want the outline of a rectangle in a canvas to get a bigger width, when the rectangle is in state &quot;disabled&quot;. Therefore I use the parameter &quot;disabledwidth=4&quot;. But when the rectangle is in state &quot;disabled&quot;, the outline has still a width of 1 instead of 4.</p>
<p>This is my code, which shows the problem: When I move the mouse over the rectangle, the state of the rectangle changes to &quot;active&quot;, everything works as expected, especially the outlinewidth changes to 4. But when I change the state to &quot;disabled&quot; by clicking on the button the outline stays at width 1. What am I doing wrong?</p>
<pre><code>import tkinter as tk
def disabled():
    canvas.itemconfig(rect, state=&quot;disabled&quot;)
def normal():
    canvas.itemconfig(rect, state=&quot;normal&quot;)
root    = tk.Tk()
canvas  = tk.Canvas(root, height=250, width=250)
button1 = tk.Button(root, text=&quot;change rectangle to state disabled&quot;, command=disabled)
button2 = tk.Button(root, text=&quot;change rectangle to state normal&quot;  , command=normal  )
rect = canvas.create_rectangle(40, 40, 180, 180,
                fill           = &quot;red&quot;,
                activefill     = &quot;green2&quot;,
                activeoutline  = &quot;green3&quot;,
                activewidth    = 4,
                disabledfill   = &quot;grey&quot;,
                disabledoutline= &quot;grey2&quot;,
                disabledwidth  = 4
                )
canvas.grid()
button1.grid()
button2.grid()
root.mainloop()

</code></pre>
"
"<p>I'm testing a couple of the widely published GPT models just trying to get my feet wet and I am running into an error that I cannot solve.</p>
<p>I am running this code:</p>
<pre><code>from llama_index import SimpleDirectoryReader, GPTListIndex, GPTSimpleVectorIndex, LLMPredictor, PromptHelper
from langchain import OpenAI
import gradio as gr
import sys
import os

os.environ[&quot;OPENAI_API_KEY&quot;] = 'MYKEY'

def construct_index(directory_path):
    max_input_size = 4096
    num_outputs = 512
    max_chunk_overlap = 20
    chunk_size_limit = 600

    prompt_helper = PromptHelper(max_input_size, num_outputs, max_chunk_overlap, chunk_size_limit=chunk_size_limit)

    llm_predictor_gpt = LLMPredictor(llm=OpenAI(temperature=0.7, model_name=&quot;text-davinci-003&quot;, max_tokens=num_outputs))

    documents = SimpleDirectoryReader(directory_path).load_data()

    index = GPTSimpleVectorIndex(documents, llm_predictor=llm_predictor_gpt, prompt_helper=prompt_helper)

    index.save_to_disk('index.json')

    return index

def chatbot(input_text):
    index = GPTSimpleVectorIndex.load_from_disk('index.json')
    response = index.query(input_text, response_mode=&quot;compact&quot;)
    return response.response




iface = gr.Interface(fn=chatbot,
                     inputs=gr.inputs.Textbox(lines=7, label=&quot;Enter your text&quot;),
                     outputs=&quot;text&quot;,
                     title=&quot;Custom-trained AI Chatbot&quot;)

index = construct_index(&quot;salesdocs&quot;)
iface.launch(share=False)
</code></pre>
<p>And I keep getting this error</p>
<pre><code>  File &quot;C:\Users\Anonymous\anaconda3\lib\site-packages\llama_index\indices\vector_store\base.py&quot;, line 58, in __init__
    super().__init__(
TypeError: __init__() got an unexpected keyword argument 'llm_predictor'
</code></pre>
<p>Having a hard time finding much documentation on llamma index errors, hoping someone can point me in the right direction.</p>
"
"<p>I have a pandas data frame which I want to convert into spark data frame. Usually, I use the below code to create spark data frame from pandas but all of sudden I started to get the below error, I am aware that pandas has removed iteritems() but my current pandas version is 2.0.0 and also I tried to install lesser version and tried to created spark df but I still get the same error. The error invokes inside the spark function. What is the solution for this? which pandas version should I install in order to create spark df. I also tried to change the runtime of cluster databricks and tried re running but I still get the same error.</p>
<pre><code>import pandas as pd
spark.createDataFrame(pd.DataFrame({'i':[1,2,3],'j':[1,2,3]}))

error:-
UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  'DataFrame' object has no attribute 'iteritems'
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
AttributeError: 'DataFrame' object has no attribute 'iteritems'
</code></pre>
"
"<p>I would like to hash some <code>pickle</code> files for verification but I wonder if Python's <code>pickle</code> always produces the same output for the same input, at least within a protocol version? I wonder if the OS makes a difference? Do you have any references?</p>
"